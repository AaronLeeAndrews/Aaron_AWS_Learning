#!/bin/bash
# Use this for your user data (script top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html


//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 4
  __                 
 /  \ |  |  |  | 
 |    |__|  |__| 
 |    |  |     |   
 \__/ |  |.    |

IAM and AWS CLI
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

IAM Intro: User, Groups, Policies
//////////////////////////////////////////////////////////////////////////
IAM = Identity and Access Management
Root Account created by default, rarely used, NEVER shared
Users: people within org, can be grouped
Groups: contain users, NOT other groups
Users don't have to belong to a group, and can be in several groups


Group: Devs____                      Group: Ops______
|              |                     |              |        Fred
|   Alice      |                     |              |
|              |                     |  Edward      |
|   Bob        |                     |              |
|   ___________|____Group: Audits____|__________    |
|  |           |                     |  David   |   |
|  |Charles    |                     |__________|___|
|  |___________|________________________________|
|______________|

IAM Permissions:
//////////////////////
Users or Groups can be assigned JSON docs called policies
These policies define the permissions of users
In AWS always apply the LEAST PRIVILEGE PRINCIPLE (give least amount of privileges required for the user to complete their tasks)

Policy Ex.
//////////
{
  "Version": "2012-10-17",
  "Statement": 
  [
    {
      "Effect": "Allow",
      "Action": "ec2:Describe*",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": "elasticloadbalancing:Describe*",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
                  "cloudwatch:ListMetrics",
                  "cloudwatch:GetMetricStatistics",
                  "cloudwatch:Describe*",
                ]
      "Resource": "*"
    }
  ]
}
//////////
//////////////////////
//////////////////////////////////////////////////////////////////////////

IAM Users and Groups Hands On
//////////////////////////////////////////////////////////////////////////

Creating a User
//////////////////////
Go to IAM > Users > Create User
	Set a name
	Click Next
	Set a password
		Can be custom or auto-generated
		Can be required for User to change on first login
	Click Next
	Add the User to a Group
		Create Group
		Set a name
		Give permissions (Ex. Admin)
	Set Key-Value Tags (Ex. Department-Engineering)
	Click Create User

Now in IAM > User Groups > admin
	The new user can be seen
//////////////////////

Creating an Account Alias
//////////////////////
In IAM > Dashboard
	Users and Groups can be seen
	An alias can be created for the AWS account
		This account is THE account for all users/groups and AWS resources
		Users can enter this alias when logging in instead of a 12-digit id
//////////////////////
//////////////////////////////////////////////////////////////////////////

IAM Policies Inheritance
//////////////////////////////////////////////////////////////////////////

  Devs Policy                          Ops Policy
      |                                    |             Inline Policy (Specific to Fred)
Group: Devs____                      Group: Ops______         |
|              |                     |              |        Fred
|   Alice      |                     |              |
|              |     Audits Policy   |  Edward      |
|   Bob        |          |          |              |
|   ___________|____Group: Audits____|__________    |
|  |           |                     |  David   |   |
|  |Charles    |                     |__________|___|
|  |___________|________________________________|
|______________|

In the example of groups from before, each Group has a policy attached to it
	These policies give the Users in these Groups permissions
	Charles and David are getting their permissions from Devs and Ops respectively
		They are also both getting the permissions from the Audit Group
	Fred is using a User Policy specific to him (this is called an Inline Policy)

Policy Structure Ex.
//////////
{
  "Version": "2012-10-17",                          // Policy language version
  "Id": "S3-Account-Permissions",                   // Policy Id (optional)
  "Statement":                                      // One or more statements
  [
    {
      "Sid": "1",                                   // Statement id
      "Effect": "Allow",                            // If it is Allow or Deny
      "Principle": {                                // Account/User/Role this applies to
                  "AWS": ["arn:aws:iam:123456789012:root"]
      },
      "Action": [                                   // List of actions this applies to
                  "s3:GetObject",
                  "s3:PutObject"
                ]
      "Resource": ["arn:aws:s3:::mybucket/*"]       // List of resources this applies to
                             // optional conditions for when this policy is in effect
      "Condition" : { "StringEquals" : { "aws:username" : "aquin" }}
    }
  ]
}
//////////
//////////////////////////////////////////////////////////////////////////

IAM Policies Hands On
//////////////////////////////////////////////////////////////////////////

Applying Policies Directly
//////////
Have the User stephane logging into a separate private window
	In the stephane window, go to IAM Users, and pull the Users information
As the root User, go to IAM > ... > admin
	Under Users, remove the user stephane from the Admin Group
	Refreshing the stephane window, the Users information is now Access Denied

	Under: Add Permissions
		Select: Attach Policies Directly
		Add: IAMReadOnlyAccess
		Next -> Add Permissions
	Now, stephane will be able to see the Users information, and not make any changes
		In the stpahane window, clicking on Create User Group gives an error
//////////

Applying Policies to Users through Groups
//////////
Create a Group called admins, and give it AdministratorAccess permissions
Create a Group called devs, and give it AlexaForBusinessDevelopment permissions
For both Groups, add the stephane User
Selecting the stephane User, under Permissions
	The Permissions from all sources are listed
		AdministratorAccess from the admin Group
		AlexaForBusinessDevelopment from the devs Group
		IAMReadOnlyAccess from the Direct (Inline) Policy
//////////

Policies Details
//////////
IAM > Policies

AdministratorAccess
//////////
	Select: AdministratorAccess
		Under: Permissions, is a detailed list of each policy that this gives
			Which is all of them (384 of 384 Services)

JSON:
////
{
  "Version": "2012-10-17",
  "Id": "S3-Account-Permissions",                   
  "Statement":                 
  [
    {                                
      "Effect": "Allow",
      "Action": "*",
      "Resource": "*"
    }
  ]
}
////
//////////

IAMReadAccessOnly
//////////
	Select: IAMReadOnlyAccess
		Under: Permissions, is a detailed list of each policy that this gives
			Which is all reads for Users info (1 of 384 Services)
				Select: IAM
					This breaks down the Permissions even further
					Shows each specific action the Permission allows
JSON:
////
{
  "Version": "2012-10-17",
  "Id": "S3-Account-Permissions",                   
  "Statement":                 
  [
    {                                
      "Effect": "Allow",
      "Action": [
		  "iam:GenerateCredentialReport",
		  "iam:GenerateServceLastAccssDetails",
		  "iam:Get*",                         // This means for all Get actions
		  "iam:List*",                        // This means for all List actions
		  "iam:SimulateCustomPolicy",
		  "iam:SimulatePrincipalPolicy"
		]
      "Resource": "*"
    }
  ]
}
////
//////////

Custom
//////////
IAM > Policies > Create Policy

Custom policies can also be created
	This can be done with either a Visual builder or pure JSON
	In either case there are helper windows with the names of the parameters
	

//////////
//////////////////////////////////////////////////////////////////////////

IAM - Password Policy
//////////////////////////////////////////////////////////////////////////

Require Strong Passwords
//////////
Strong passwords == higher security for account
In AWS, a password policy can be created
	Set min length
	Require specific chars
		lowercase letters
		uppercase letters
		numbers
		non-alphanumeric chars
	Allow all IAM Users to change their own passwords
	Expire passwords after a certain time
	Prevent password re-use
//////////

Multi Factor Authentication - MFA
//////////
Any Users with Access to an Account can possibly make bad changes
The Root Account MUST be protected at the very least
	Protections on all User accounts is recommended
MFA requires a password and a device owned by the account User
	This means that a stolen password is not enough to log in

MFA has multiple device options
	Virtual MFA device
		Google Authenticator, Authy // both are phone only
	Universal 2nd Factor Security Key
		YubiKey (3rd party)
			USB device, can be used for multiple root and IAM Users
	Hardware Key Fob MFA Device
		Gemalto (3rd party)
	Hardware Key Fob MFA Device for AWS GovCloud
		SurePassID (3rd party)
//////////
//////////////////////////////////////////////////////////////////////////

IAM - MFA Hands On
//////////////////////////////////////////////////////////////////////////

IAM > Account Settings > Edit Password Policy
//////////
IAM Default or Custom
	Custom allows for any combination of the password requirements
	Custom also allows for setting expiration, Users changing their own password, etc
//////////

IAM > Security Credentials > Assign MFA Device
//////////

Enter device name
Select what type of device it is (phone, USB, etc)
For phone:
	Select app
	Scan the QR code
	Enter the MFA codes
//////////
//////////////////////////////////////////////////////////////////////////

AWS Access Keys, CLI, and SDK
//////////////////////////////////////////////////////////////////////////

AWS Access
//////////
How Users can access AWS, Three Options
	AWS Management Console: (password + MFA)
	AWS Command Line Interface (CLI): access keys
	AWS Software Developer Kit (SDK) - for code: access keys
Access Keys are generated through the AWS Console
Users manage their own access keys
Access Keys are secret, just like passwords
Access Key ID ~= username
Secret Access Key ~= password
These keys are generated by a User, and downloaded into the CLI
	Gives access to AWS API
	DO NOT SHARE THESE
//////////

AWS CLI
//////////
Tool that enables interacting with AWS services through command-line shell
	Ex. aws s3 cp file.txt s3://ccp-bucket/file.txt
	    /*Response:*/ upload: ./file.txt to s3://ccp-bucket/file.txt
            aws s3 ls s3://ccp-bucket
	    /*Response:*/ 2021-05-14 03:22:52                0 file.txt
Direct access to public APIs of AWS services
Can develop scripts to manage resources
Open-source: https://github.com/aws/aws-cli
Alternative to using the management console
//////////

AWS SDK
//////////
AWS Software Development Kit (AWS SDK)
Language-specific APIs (Set of libraries)
Enables accessing and managing AWS services programmatically
Embedded within application
Supports:
	SDKs (JavaScript, Python, PHP, .NET, Ruby, Java, Go, Node.js, C++)
	Mobile SDKs: Android, iOS
	IoT Device SDKs: Embedded C, Arduino, ...
Ex. AWS CLI is built on AWS SDK for Python
//////////
//////////////////////////////////////////////////////////////////////////

AWS CLI Setup on Windows
//////////////////////////////////////////////////////////////////////////
Google "AWS CLI install Windows"
	Select the AWS link to the docs.aws.amazon.com
	Go to: Installing on Windows
	Select the Step 1 link to download the AWS CLI Installer for Windows
	Install the download
	Confirm the installation by checking the version
		aws --version
//////////////////////////////////////////////////////////////////////////

AWS CLI Hands On
//////////////////////////////////////////////////////////////////////////
Go to IAM > Users > *User name* > Create Access Key
	Choose CLI
	-> Next
	-> Create Access Key
	After creating the key, open the command line window
In the command-line window
	Here, begin the configuration and copy over the access key info
	aws configure
	AWS Access Key ID [None]: *Access Key ID*
	AWS Secret Access Key [None]: *Secret Access Key*
	Default region name [None]: *us-west-2*
	Default output format [None]: // Just hit enter

	Now using the aws keyword, we can use the AWS API
		aws iam list-users
		// Response is the list of Users from AWS

	Removing the User from the Group that gives them Permissions to access the Users will result in the CLI not being able to access the User list from the CLI as well
	
//////////////////////////////////////////////////////////////////////////

AWS Cloudshell
//////////////////////////////////////////////////////////////////////////

From the AWS Management Console, we can use the Cloudshell to access AWS resources
	Cloudshell is only available in certain AWS Regions

AWS > Documentation > AWS Cloudshell > User Guide
	This is the docs for Cloudshell

To get into CLoudshell, just type Cloudshell into the AWS search bar

In Cloudshell:
	Execute AWS API commands
	Create files
	Download files
	Open multiple windows like tabs
//////////////////////////////////////////////////////////////////////////

IAM Roles for AWS Services
//////////////////////////////////////////////////////////////////////////

Some AWS services will need to perform actions on other AWS resources
To do this, these AWS services will need to be assigned Permissions with IAM Roles

Ex. (Giving an EC2 Instance an IAM Role)
	The IAM Role is given Permissions for Access AWS

Common Roles
	EC2 Instance Roles
	Lambda Function Roles
	Roles for CloudFormation
//////////////////////////////////////////////////////////////////////////

IAM Roles Hands On
//////////////////////////////////////////////////////////////////////////
Go to IAM > Roles > Create Role
	Select AWS Service
	Choose a Service: EC2
	Choose a Use Case: EC2
	-> Next
	Add Permissions: IAMReadOnlyAccess
	-> Next
	Set a name: "DemoRoleForEC2"
		Below, the JSON translation of the Permissions is shown
	-> Create Role
This role is now ready to be assigned to an EC2 Instance
	That EC2 Instance will now be able to read the IAM User information
//////////////////////////////////////////////////////////////////////////

IAM Security Tools
//////////////////////////////////////////////////////////////////////////

IAM Credentials Report (account-level)
	A report that lists all of the account's Users and status of their credentials
IAM Access Advisor (user-level)
	Shows service permissions of a User and when the services were last accessed
	Use this to revise policies when needed

//////////////////////////////////////////////////////////////////////////

IAM Security Tools Hands On
//////////////////////////////////////////////////////////////////////////

Credentials Report
//////////
IAM > Credentials Report
	-> Download Credentials Report
	This downloads the Permissions of each User, and when they were last used
//////////

Access Advisor
//////////
IAM > Users > *User name*
	-> Access Advisor Tab
	Shows an easier to read list of each Service and when it was last accessed
//////////

//////////////////////////////////////////////////////////////////////////

IAM Guidelines and Best Practices
//////////////////////////////////////////////////////////////////////////
- Don't use the Root account except for AWS account setup
- One physical User == One AWS User
- Assign Users to Groups and assign Permissions to Groups
- Create a strong password policy
- Use and enforce use of MFA
- Create and use Roles for giving Permissions to AWS Services
- Use Access Keys for the CLI and SDK
- Audit Permissions of accounts using Credentials Report and IAM Access Advisor
- NEVER share IAM Users and Access Keys
//////////////////////////////////////////////////////////////////////////

Shared Responsibility Model for IAM
//////////////////////////////////////////////////////////////////////////

AWS and Developers have separate and defined responsibilities

AWS
//////////
- Infrastructure (global network security)
- Configuration and vulnerability analysis
- Compliance validation
//////////

Developers
//////////
- Users, Groups, Roles, Policies management and monitoring
- Enable MFA on all accounts
- Rotate all keys often
- Use IAM tools to apply appropriate permissions
- Analyze access patterns and review permissions
//////////

//////////////////////////////////////////////////////////////////////////

IAM Section - Summary
//////////////////////////////////////////////////////////////////////////

- Users: mapped to a physical User, with a password for AWS Console
- Groups: contains Users only
- Policies: JSON document that outlines Permissions for Users
- Roles: for EC2 Instances or AWS Services
- Security: MFA and Password Policy
- AWS CLI: Manage AWS Services using command-line
- AWS SDK: Manage AWS Services using a programming language
- Access Keys: Access AWS using the CLI or SDK
- Audit: IAM Credential Reports and IAM Access Advisor

//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 5
  __         __        
 /  \ |  |  |   
 |    |__|  |__ 
 |    |  |     \   
 \__/ |  |. \__/

EC2 Fundamentals
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

AWS Budget Setup
//////////////////////////////////////////////////////////////////////////

> AWS Billing
As a default, even Admin Permissions don't allow Users to have access to billing

> AWS Billing > Account
	Scroll down to IAM User and role access to Billing Information
	-> Edit
	Check the Activate IAM Access
	-> Update
	Now the User can see the Billing Information, if they are Admins

> AWS Billing > Bills
	Under Charges by Service, lists the costs being charged to the account and what service is making that charge

> Billing and Cost Management > Free Tier
	This lists each service and if it will go over the Free Tier limits

> Billing and Cost Management > Budgets > Create Budget
	// Allows for setting up different budgets
	Ex. Zero Cost Budget
		-> Use a Template
		-> Zero Spend Budget
		Set the email to be alerted when going over budget
		-> Create Budget
	Ex. $10 Cost Budget
		-> Use a Template
		-> Monthly Cost Budget
		Set a name
		Set the budget amount
		Set the email to be alerted when going over budget
		// An email will be sent when the spend reaches 85% of the budget, it reaches 100%, and if it is forecasted to reach 100%
		-> Create Budget

// Not super important (seemingly), but the path starts with Bills until selecting an item lower in the Service explorer list, at which point the path starts with Billing and Cost Management
//////////////////////////////////////////////////////////////////////////

EC2 Basics
//////////////////////////////////////////////////////////////////////////

Basics
//////////
EC2 is one of the most popular Services
EC2: Elastic Compute Cloud == Infrastructure as a Service
Features:
 - Rent virtual machines (EC2)
 - Storing data on virtual drives (EBS)
 - Distributing load across machines (ELB)
 - Scaling services using an auto-scaling group (ASG)

Knowing EC2 is fundamental to understand how the Cloud works
//////////

EC2 Sizing and Configuring Options
//////////
- EC2 Instances can be Mac, Linux, or Windows
- Choose compute power and cores (CPU)
- Choose RAM
- Choose storage space
	Network-attached (EBS and EFS)
	hardware (EC2 Instance store) // This only lasts as long as the Instance does, and goes away when the Instance does
- Network card: speed of card, Public IP address
- Firewall rules with Security Groups
- Bootstrap script (configures the Instance on first launch): EC2 User Data
//////////

EC2 User Data
//////////
On the first launch of an EC2 Instance, it can be given a bootstrap script.
This can install software, create files, or whatever other setup that's desired
Any commands executed with the script will have root user permissions
//////////

EC2 Instance Types
//////////
There are many combinations of EC2 Instances that can be created by choosing from the different parameters:
	Instance type: (t2.micro, c5d.4xlarge, m5.8xlarge, ...) // t2 micro is what we use here b/c it is always part of the Free Tier
	vCPU: (1, 4, ...)
	Mem(GiB): (1, 16, ...) 
	Storage: (EBS-Only, 1 x 400 NVMe SSD, ...)
	Network Performance: (Moderate, Low to Moderate, ...)
	EBS Bandwidth (Mbps): (-, 4750, 6800, ...)
//////////
//////////////////////////////////////////////////////////////////////////

Launching an EC2 Instance Running Linux
//////////////////////////////////////////////////////////////////////////

Goals:
	Launch an EC2 Instance using the AWS Management Console
	Get first high-level approach to parameters
	See that web server is launched using EC2 user data // From our Bootstrap Script
	How to Start, Stop, and Terminate an Instance

> EC2 > Instances
	-> Launch Instances
	Set the name
	Choose a base image // We just choose Linux, Amazon Linux 2 AMI
	Choose Instance Type: t2.micro
	-> Create a new Key-Pair
		Set name
		Set type: RSA
		Private file format: just use .pem // The other one is for Win 7, 8
		-> Create Key Pair
	Under Network Settings: Check Allow SSH and Allow HTTP
	Under Advanced Details
		Enter the text of the bash script to set up the web server and index.html
//////////////////////////////
#!/bin/bash
# Use this for your user data (script top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html
//////////////////////////////
	-> Launch Instance

Once the Instance is running, under Details is the link that can be pasted to a browser
	B/c it has the webserver and the Allow traffic, it can be accessed from HTTP, but NOT HTTPS
	It does take a few minutes to be fully up

> EC2 > Instances
	The Actions dropdown has commands to Start, Stop, and Terminate EC2 Instances
	Public IPv4 may change

//////////////////////////////////////////////////////////////////////////

EC2 Instance Types - Overview
//////////////////////////////////////////////////////////////////////////

The EC2 Type names indicate their details
	Ex: m5.2xlarge
		m: Instance Class
		5: Version number // Will eventually be replaced with 6
		2xlarge: size
There are different types of EC2 Instances for different workloads
	General Purpose: Wide diversity of workloads, Balanced stats (Compute, Memory, Networking)
	Compute Optimized: Compute-Intensive tasks that require high performance processors
		Batch processing, Media transcoding, Dedicated game servers, ...
	Memory Optimized: Fast performance to process large data sets in memory
		In-Memory DBs for Business Intelligence, Real-Time data processing, ...
	Storage Optimized: Intensive read-write on large data sets
		High frequency online transactions, Relational and NoSQL DBs, ...

ec2instances.info is a website that compares all of the EC2 Instance Types
//////////////////////////////////////////////////////////////////////////

Security Groups and Classic Ports Overview
//////////////////////////////////////////////////////////////////////////

Security Groups are a fundamental part of network security in AWS
These control traffic in and out of EC2 Instances
Security Groups only contain "allow" rules
Security Group Rules can reference by IP or Security Group
 _________________
| Security Group  |
|                 |
|                 |
| EC2 Instance    |
|      __         |
|     |__|        |
|_________________|


Deeper Dive
//////////

Security Groups act as a "firewall" on EC2 Instances
They regulate:
	Access to Ports
	Authorized IP ranges - IPv4 and IPv6
	Control Inbound Network (from other to the Instance)
	Control Outbound Network (from the Instance to other)

//////////

Security Groups Diagram
//////////
 ____________    _________________________                     ___________________
|           <---------------------------------- Port 22 ------|Authorized Computer|             
|            |  |   Security Group 1      |                   |___________________|
|            |  |      Inbound            |                    ________________
|            |  |Filter IP / Port w/ Rules|*Blocked*<-Port 22-| Other Computer |
|EC2 Instance|  |_________________________|                   |________________|
|            |   _________________________                     ____________________
|            |  |   Security Group 1      |                   |         WWW        |
|            |  |     Outbound            |                   |  Any IP - Any Port |
|            |  |Filter IP / Port w/ Rules|                   |                    |
|           -----------------------------------Any Port------>|____________________|
|____________|  |_________________________|
//////////

Security Groups Good to Know
//////////
- Can be attached to multiple Instances
- Locked to one region / VPC combination
- These are outside of the EC2 Instances, blocking/allowing traffic
- Good to maintain one Security Group specifically for SSH access
- If app is not accessible and times out, it is a Security Group issue
- If app has connection refused, then it is an app error
- By default:
	All inbound traffic is blocked
	All outbound traffic is allowed
//////////

Referencing other Security Groups
//////////
We can reference other Security Groups directly.
This makes it easy for other EC2 Instances to safely talk to each other.
Only those Security Groups with authorization will be able to send data with this rule.

 ____________    _________________________                     ________________
|           <----------------------------------- Port 122 ----|Security Group 1|             
|            |  |   Security Group 1      |                   |________________|
|            |  |      Inbound            |               
|            |  |Security Group 1 is auth |
|EC2 Instance|  |Security Group 2 is auth |
|            |  |                         |                    ________________
|           <----------------------------------- Port 122 ----|Security Group 2| 
|            |  |                         |                   |________________| 
|            |  |                         |                    ________________
|            |  |                     *Blocked*<-Port 122 ----|Security Group 3| 
|            |  |                         |                   |________________|                    
|            |  |                         |          
|            |  |_________________________|         
|____________|  
//////////

Classic Ports to Know
//////////
There are ports usually used for specific things

22   == SSH   (Secure Shell) log into a Linux instance
21   == FTP   (File Transfer Protocol) upload files
22   == SFTP  (Secure File Transfer Protocol) upload files w/ SSH
80   == HTTP   access unsecured sites
443  == HTTPS  access secured sites
3389 == RDP   (Remote Desktop Protocol) Log into a Windows instance
//////////
//////////////////////////////////////////////////////////////////////////

Security Groups Hands On
//////////////////////////////////////////////////////////////////////////
EC2 > Network & Security > Security Groups
	Default Security Group launch-wizard-1 has two rules
		SSH,  port 22, all sources		
		HTTP, port 80, all sources

		If the HTTP port 80 rule is removed, the EC2 Instance can't be accessed anymore
		
		Outbound is to everywhere by default, but we can change this
//////////////////////////////////////////////////////////////////////////

SSH Overview
//////////////////////////////////////////////////////////////////////////

SSH is for Mac, Linux, and <= Windows 10
Putty is for >= Windows 10
EC2 Instance Connect works for all

//////////////////////////////////////////////////////////////////////////

SSH Hands On Linux/Mac
//////////////////////////////////////////////////////////////////////////

The Security Group allows port 22 by default, for SSH
SSH allows us to control the machine using command line

The first step is get the .pem file for the EC2 Instance, created at its launch
	Place the .pem in a safe location on the personal computer 
	Get into ssh through the Console
		ssh ec2-user@<IPv4 Address> // The IPv4 Address is what we got from the EC2 Instance Details page
		If the command isn't being made in the folder with the .pem file, it will fail
		ssh -i EC2Tutorial.pem ec2-user@<IPv4 Address>
			This gives an error about an unprotected Private Key File
			We fix this by changing the read-write permissions on the file itself
				chmod 0400 EC2Tutorial.pem
		Now running the same command runs correctly
		Commands now run on the EC2 Instance as the ec2-user

If the EC2 Instance doesn't have a keypair from when it was created, a new one must be made
	This can be done through EC2 > Network & Security > Key Pairs
//////////////////////////////////////////////////////////////////////////

SSH Troubleshooting
//////////////////////////////////////////////////////////////////////////

These are just a few possible scenarios and how to fix them

Connection Timeout
//////////
This is a Security Group issue, likely due to the Inbound firewall rules
It may be that the EC2 Instance was not assigned the Security Group
//////////

Connection Timeout Persists
//////////
The Security Group is correctly configured and assigned, but the timeout still happens
This would be an issue with the network firewall
Just use the EC2 Connection through the Management Console
//////////

SSH Doesn't Work
//////////
ssh command not found:
	Have to use Putty
Make sure everything is installed and configured correctly
//////////

Connection Refused
//////////
This is an application issue
	Try restarting the EC2 Instance
	Try terminating the EC2 Instance and creating a new one
	Make sure that the EC2 Instance has the Amazon Linux OS
//////////

Permission Denied
//////////
This is either from:
	Using the wrong security key or no security key for the EC2 Instance
	Make sure that the EC2 Instance has been started
		Double-check that the command uses the ec2-user@<IPv4>
//////////

Nothing is working
//////////
Use the EC2 Instance Connect through the AWS Management Console
//////////

Able to connect yesterday, but not today
//////////
A restart to the EC2 Instance may have changed the public IPv4
//////////
//////////////////////////////////////////////////////////////////////////

EC2 Instance Connect
//////////////////////////////////////////////////////////////////////////
> EC2 > Instances
	Select an EC2 Instance and then Connect (button in the upper menu)
	Leave the defaults as is
		-> Connect
	This brings up an interface into the EC2 Instance as user ec2-user
	Make sure that it works
		whoami // returns the username: ec2-user
		ping google.com
If the Connect does not work, check the Security Group:
	Does it have the Port 22 SSH rule
	Is it assigned to the EC2 Instance
//////////////////////////////////////////////////////////////////////////

EC2 Instance Roles Demo
//////////////////////////////////////////////////////////////////////////

Make sure calls to the aws API are working
	aws --version
	If the Access keys have not been set, this will not work
	These can be set directly inside of the EC2 Instance, however we shouldn't
		Doing this would be bad

	The better way is use EC2 Instance Roles
> EC2 > Instances
	Select the EC2 Instance
	In the Actions dropdown, select Security > Modify IAM Role
		Choose an IAM Role with Users Permissions
		The Roles are listed at IAM > Roles
	Run a command to read the Users list
		aws iam list-users

//////////////////////////////////////////////////////////////////////////

EC2 Instance Purchasing Options
//////////////////////////////////////////////////////////////////////////

Overview
//////////
Overview of the different EC2 Instance types that are available.
These generally range from reserved for long workloads to flexible for short workloads.

On-Demand - short workloads, immediate results, pay per second of use, flexible, predictable
Reserved - (1 and 3 years)
	Reserved for long workloads
	Convertible Reserve Instances - Long workloads with flexible instances
Savings Plans - (1 and 3 years) - long workload, commitment to amount of usage
Spot Instances - short workloads, cheap, can lose instances (less reliable)
Dedicated Hosts - reserve entire physical server, control instance placement
Dedicated Instances - no other customers share your hardware
Capacity Reservations - reserve capacity in specific AZ for any duration
//////////

EC2 On-Demand
//////////
Pay for what is used
	Linux/Windows: billing per second for first minute
	All others: billing per hour
Highest Cost, but nothing upfront
No long-term commitments

Recommended for short-term un-interrupted workloads, where app behavior is unpredictable
//////////

EC2 Reserved Instances
//////////
Up to 72% discount compare to On-Demand
	Here, + is amount of discount
Reserve specific attributes (Instance Type, Region, Tenancy, OS)
Reservation Period: 1 year (+) or 3 yeas (+++)
Payment Options: None Upfront (), Partial (++), All (+++)
Reserved Instance's Scope: Regional or by Zone
Buy or sell reservations in the marketplace
Convertible Reserved Instance
	Can change EC2 Instance type, family, OS, scope, and tenancy
	Up to 66% discount

Recommended for steady-state usage apps (like a db)
//////////

EC2 Savings Plan
//////////
Up to 72% discount, depending on usage
Commits to a certain type of usage ($10/hour for 1 to 3 years)
Usage beyond plan limits is billed at the On-Demand pricing

Locked to a specific instance family and AWS region
Flexible:
	Instance Size
	OS
	Tenancy
//////////

EC2 Spot Instances
//////////
Most cost-effective at 90% discount compared to On-Demand
Instances that can be lost at any time, if the set max price of the spot is exceeded
Useful for resilient workloads
	Batch jobs
	Data analysis
	Image processing
	Any distributed workloads
	Workloads w/ a flexible start and end time

NOT useful for critical jobs or DBs // Exam Q!!!!
//////////

EC2 Dedicated Hosts
//////////
Physical Server w/ EC2 Instance capacity
Allows address compliance requirements and owner's existing server-bound licenses
	(per-socket, per-core, pe--VM software licenses)
Purchasing options:
	On-Demand - pay per second for active dedicated host
	Reserved - 1 or 3 years (No upfront, partial, full)
This is the MOST expensive option

Useful for software w/ compliance licensing model (Bring Your Own License)
	Companies that have strong regulatory or compliance needs
//////////

EC2 Dedicated Instances
//////////
Instances run on owner's hardware
May share hardware with other instances in same account
No control over Instance placement
	So, Instances can be moved after stopped/started
//////////

EC2 Capacity Reservations
//////////
Reserve On-Demand Instances capacity in an AZ for any duration
Capacity is always available when requested
No time commitment needed, no billing discounts
Can be combined w/ Regional Reserved Instances and Savings Plans for billing discounts
Will be billed at On-Demand rate whether Instances run or not

Useful for short-term, uninterruptable workloads in a dedicated AZ
//////////
                                            
Summary (What should I Use?ðŸ¤”, Resort Analogy)
//////////
On-Demand - Full price when we stay, but we can stay whenever we want
Reserved - Long-term reservation, can get good discount compared w/ same time On-Demand pricing
Savings Plan - Reserve amount per hour for long-term (1 to 3 years), can change room type
Spot Instances - Resort takes bids on empty rooms, high bidder wins, can be kicked at any time
Dedicated Hosts - Book entire building of resort
Capacity Reservations - Book a dedicated room, pay whether stay there or not
//////////
//////////////////////////////////////////////////////////////////////////

IP Address Charges in AWS
//////////////////////////////////////////////////////////////////////////
AWS is trying to migrate everyone to IPv6, so IPv4 public addresses will incur fees
	Cost per IPv4 is $0.005 per hour
	The Free Tier for this is 750 hours per month
	All active Public IPv4 Addresses drain this
	Once drained, all active Public IPv4 Addresses start to incur costs
Load Balancers and RDS DBs are not in the Free Tier, so they incur charges from the start
	Load Balancers have a Public IPv4 Addresses in each of their AZs
		Each of these Public IPv4 Addresses incur charges
		Make sure to delete these if not in use
IPv6 does not get charged, but is more complicated to set up

Tracking IP Charges:
	IP charges can be tracked at: Billing and Cost Management > Bills
	> Amazon VPC IP Address Manager > Public IP Insights
		Here, we an create an IPAM to track our IPv4 Addresses
			IPAM: IP Address Manager

//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 6
  __                 
 /  \ |  |    /  
 |    |__|   /_ 
 |    |  |  /  \   
 \__/ |  |. \__/

EC2 Instance Storage
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

EBS Overview
//////////////////////////////////////////////////////////////////////////
An EBS Volume (Elastic Block Store Volume) is a network drive for an EC2 Instance
	If non-root, default is to persist after attached EC2 Instance is  terminated
	Very helpful to think of these as Network USB Sticks
	An EBS can only be mounted to one EC2 Instance at a time
		An EC2 Instance can have multiple attached EBSs
		They can be completely unmounted (no attached EC2 Instances)
	They are bound to one specific AZ
		Snapshots of EBSs can be move across AZs
	Free Tier gives 30GB of EBS per month
	Can be detached from one EC2 Instance and attached to a different one quickly
		This makes them good for fail-overs
	Must be provisioned, specifying GB size and IOPS (In-Out operations per second)
		Size and IOPS can be changed after creation
Delete on Termination
	This is an exam Q!!!!!
	Default behavior for Root EBS is to be deleted when the attached EC2 Instance is deleted
		This can be changed through the AWS Console and AWS CLI
	By default, all other EBSs are not deleted
		This can also be changed through the AWS Console and AWS CLI

//////////////////////////////////////////////////////////////////////////

EBS Hands On
//////////////////////////////////////////////////////////////////////////
Create a new EBS Volume:
	> EC2 > Volumes
		The Root Volumes of EC2 Instances are also listed here
	-> Create Volume
	Set the GB size and IOPS
	Set the AZ
		This has to exactly match the AZ of the desired EC2 Instance(s), not just Region
	Wait for the Volume to be created and ready
Mount the Volume to the EC2 Instance
	-> Actions dropdown
	Attach Volume
	Choose the EC2 Instance from the dropdown
	Choose a Drive Name for the Volume
		This will be the Volume's name/location in the EC2 Instance's file structure

//////////////////////////////////////////////////////////////////////////

EBS Snapshots
//////////////////////////////////////////////////////////////////////////
Snapshots are backups of EBS Volumes at a point in time
An EBS Volume can be attached to an EC2 Instance when the Snapshot is created
	However, this is bad practice
Snapshots can be copied across AZs and Regions

EBS Snapshots Features
//////////
Moving a Snapshot to an EBS Snapshot Archive makes them cheaper
	But, they take 24 to 72 hours to restore from the archive

Snapshots should have a Recycle Bin
	These protect against accidental deletions
	These can store Snapshots up to 1 year

Fast Snapshot Restore
	Force full initialization of Snapshot to have no latency on first use
	Useful when the Snapshot is very big
	Doing this is expensive
//////////
//////////////////////////////////////////////////////////////////////////

EBS Snapshots Hands On
//////////////////////////////////////////////////////////////////////////

Create a Snapshot
//////////
> EC2 > Volumes
-> Actions dropdown
	-> Create Snapshot
-> Create Snapshot
//////////

Copy Snapshot to another Region
//////////
> EC2 > Snapshots
-> Actions dropdown
	-> Copy Snapshot
Set the Destination Region
-> Copy Snapshot
//////////

Create EBS Volume from Snapshot
//////////
> EC2 > Snapshots
-> Actions dropdown
	-> Create Volume from Snapshot
Set the size
Set the AZ
	Note that the AZ choices are only within the Snapshot's Region
Set it to be encrypted/not-encrypted
-> Create Snapshot
The resulting Volume will have a Detail that says the Snapshot it was created from
//////////

Set Up Recycle Bin
//////////
> EC2 > Snapshots
-> Recycle Bin // Upper Menu button
-> Create Retention Rule
Set Resource Type to EBS Snapshots
Set the number of days to keep items
Make sure to leave Rule Lock Settings to Unlock
	This lets us delete the Rule easily
-> Create Retention Rule
//////////

Recover Snapshot from Recycle Bin
//////////
> EC2 > Snapshots
-> Recycle Bin // Upper Menu button
-> Resources // left-hand side menu
Select the EBS Snapshot(s)
-> Recover
The Snapshots will now be at: EC2 > Snapshots
//////////

Archive Snapshot
//////////
> EC2 > Snapshots
The Snapshot will have a Detail called: Storage Tier
	If not archived, it will be: Standard
-> Actions dropdown
	-> Archiving
		->Archive Snapshot
//////////
//////////////////////////////////////////////////////////////////////////

AMIs (Amazon Machine Image):
//////////
We can create an AMI based on an EC2 Instance
 - The created AMI is a Region-specific customization of an EC2 Instance
 - From an AMI, identical EC2 Instances can be launched (in that Region)
 -- To use an AMI to put a new EC2 instance into a different Region, the AMI must be copied, and the copy is set to the new Region
 --- The EC2 Instance can then be created in the new Region
 - These can be made by ourselves, or others, and there are businesses that just create AMIs to sell
//////////

EC2 Instance Stores:
//////////
These are high-performance hardware disks with advantages over the EBS Volumes.
They act like cache in a computer, storing fast-access memory while the computer is running, but doesn't store anything on a shutdown.
 - Better I/O performance
 - Good for buffer, cache, scratch data, and temporary content
 - A downside is that they go away when the EC2 instance stops (they're ephemeral)
 -- Risk of data loss on a hardware failure
If all we care about is IOPS. then choosing this as a solution is just fine
Can handle a very high IOPS, depending on i3 chosen (see slide) (100,000 to 2 million Reads per Second)
//////////

EBS Volumes:
//////////
gp2/gp3 (SSD), General purpose, low cost
 - gp2, Size of volume and IOPS are linked together
 - gp3, volume and IOPS can be scaled separately
io1/io2 Block Express (SSD), High performance for mission-critical, low-latency, or high-throughput
st1 (HDD), Low cost for frequent access, throughput-intensive
sc1 (HDD), Lowest cost for less frequent access

Only gp and io boot volumes can have an OS running on them.
	Seriously, what the hell does this mean???
	Does the EBS have an installed OS???
	Does the EBS give access to the OS???

Defined by Size, Throughput, IOPS (I/O Ops per Second)

Use Cases Provisioned IOPS (PIOPS) SSD
 - Critical business apps w/ sustained IOPS performance
 - Apps that need more than 16,000 IOPS
 - Great for DB workloads (sensitive to storage perfs and consistency)
 - io1 (4GiB - 16 GiB)
 -- Max PIOPS, 64,000 for Nitro EC2 instances & 32,000 otherwise
 -- Can scale PIOPS and storage independently
 - io2 Block Express (4GiB - 64TiB)
 -- Sub-millisecond latency
 -- Max PIOPS, 256,000 w/ IOPS:GiB ratio of 1000:1
 - PIOPS supports EBS Multi-Attach // Meaning we can attach a single io1 or io2 to multiple EC2 instances (in the same Availability zone)

 - st1 & sc1 are HDDs
 -- Can't be boot volumes
 -- 125GiB to 16TiB
 -- (st1) Throughput optimized: Big Data, Data Warehouses, Log Processing
 --- Max throughput is 500MiBs, max IOPS 500
 -- (sc1) Cold HDD, lowest cost and least IO
 --- Max throughput is 250MiBs, max IOPS 250

EBS Multi-Attach (only io1 and io2)
 - The max of 16 EC2 instances // Exam Q!!!!!!!!!!!
 - Attach the same EBS volume to multiple (max 16) EC2 instances (in same Availability Zone (AZ))
 - Each EC2 instance has full read/write permissions
 - Use Case: Linux clustered apps (like Teradata), and it is on the app to handle concurrency
 - Must use file system that is cluster-aware (XFS, EXT4, etc...)
//////////


Amazon EFS (Elastic File System)

Overview: a file system that can be accessed from multiple AZs, can automatically move b/t storage/IO optimizing for cost
This has several tiers optimizing for IO or storage
This is an NFS (Network File System)
Can be attached to multiple EC2 instances across AZs, and is expensive (3x gp2), but is pay per use
Can move b/t the tiers based on a lifecycle policy
Basics:
 - Use cases: content management, web serving, data sharing, Wordpress // Sounds like a website that serves multiple places around the globe with one data store
 - Uses NFSv4 protocol (allows a client to access this like a local file)
 - Uses Security Group permissions for access
 - Only compatible w/ Linux AMI (Amazon's Linux image)
 - Encryption at rest w/ KMS (encrypts stored data, w/ "At Rest" referring to infrequently accessed long-term storage data)

Accessing the EFS from two different EC2 instances is pretty cool.

EBS vs EFS vs Instance Store:
EBS Volumes:
 - Attached to one EC2 at a time, except for io1/io2 Multi-Attach
 - Locked at the AZ level
 - gp2 IO increases w/ Disk size
 - gp3 and io1 can increase IO independently
 - To Migrate to new AZ:
 -- Take Snapshot
 -- Restore instance from snapshot inside of a new AZ
 - To Migrate to new Region:
 -- Take Snapshot
 -- Copy Snapshot to new Region
 -- Restore instance from snapshot inside of a new AZ in that Region
EFS Volumes:
 - Made to be attached to multiple EC2 instances across AZs
 - Only available on Linux OSs
 - Higher price point
 -- Leverage storage tiers for cost savings
Instance Store:
 - Acts as the RAM of an EC2 Instance OS
 - Is wiped every time it shuts down
 - Very fast IOPS


Scaling In/Out vs Up/Down
	In/Out refers to adding more of the same infrastructure rather than exchanging what's already there for a more powerful version (Which would be scaling up)

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 7
  __         __       
 /  \ |  |  |  |  
 |    |__|     |  
 |    |  |     |  
 \__/ |  |.    |

AWS Fundamentals: ELB + ASG (Elastic Load Balancer + Automatic Scaling Group)
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

High Availability and Scalability
//////////////////////////////////////////////////////////////////////////

Scalability and High Availability
//////////
Scalability: An app or system can handle greater loads by adapting to demand
	Vertical: Replace/increase current instance
	horizontal: Add more instances of the current resource
//////////

Vertical Scalability
//////////
This means increasing the size of the instance
	Ex. (changing out a t2.micro instance for a t2.large)
Vertical scaling is very common for non-distribute systems, like a DB
RDS and ElastiCache are two services that can scale vertically
There are usually hardware limits to how much an instance can be scaled vertically
//////////

Horizontal Scalability
//////////
This means adding more instances of the resource.
Horizontal scaling implies distributed systems
	Ex. (Web apps and modern apps)
Amazon EC2 makes horizontal scaling very easy
//////////

High Availability
//////////
This goes w/ horizontal scaling
High availability means running an app/system in at least 2 AZs
The goal is to survive data center loss
This can be passive
	Ex. (RDS Multi AZ)
This can also be active
	Ex. Horizontal scaling
//////////

High Availability and Scalability for EC2
//////////
Vertical Scaling: Increase EC2 Instance size (up or down)
	Low:  t2.nano - 9.5GB RAM, 1 vCPU
	High: u-12tb1.metal - 12.3TB RAM, 448 vCPUs

Horizontal Scaling: Increase number of EC2 Instances (in or out)
	Auto Scaling Groups
	Load Balancers
	
High Availability: Running Instances in at least 2 AZs
	Auto Scaling Group multi AZ
	Load Balancer multi AZ
//////////
//////////////////////////////////////////////////////////////////////////

Elastic Load Balancing (ELB) Overview
//////////////////////////////////////////////////////////////////////////

Load Balancers are servers that take traffic and distribute it to resources
	Ex. (App w/ instances running on multiple EC2 Instances)

Why Use a Load Balancer
//////////
Spread load across multiple Instances
Expose single point of access (Static DNS) for app // This is the static URL name provided by AWS that we can connect to with a browser or Service
Seamlessly hide resource Instance failures
Do regular health checks on resource Instances
Provide SSL termination (HTTPS) for websites
Enforce stickiness w/ cookies
Have high availability across zones
Have a layer between public and private traffic
//////////

Why Use an Elastic Load Balancer
//////////
All of the benefits of a Load Balancer, and we can scale w/ demand
An Elastic Load Balancer is a managed Load Balancer
	AWS guarantees that it will be working
	AWS takes care of the infrastructure (upgrades, maintenance, high availability)
	AWS provides only a few configuration parameters
Costs less to setup a personal Elastic Load Balancer, but is much harder

ELBs are integrated w/ many AWS offerings/services
	EC2, EC2 Auto Scaling Groups, Amazon ECS
	AWS Certificate Manager (ACM), CloudWatch
	Route 53, AWS WAF, AWS Global Accelerator
//////////

Health Checks
//////////
These are crucial for Load Balancers
These enable the Load Balancer to know that the Instances are available for traffic
The Health Check is performed on a port and a route (commonly, /health)
Response of 200 is healthy, and anything else is unhealthy
//////////

Types of Load Balancers on AWS
//////////
Four Kinds:
	Classic Load Balancer (Deprecated)
	Application Load Balancer
		v2 - new generation - 2016 - ALB
		HTTP, HTTPS, WebSocket
	Network Load Balancer
		v2 - new generation - 2016 - ALB
		TCP, TLS (secure TCP), UDP
	Gateway Load Balancer
		2020 - GWLB
		Operates at layer 3 (Network Layer) - IP Protocol

Overall, recommended to NOT use the Classic Load Balancer
Some Load Balancers can be setup as internal (private) or external (public) ELBs
//////////

Load Balancer Security Groups
//////////

        HTTPS/HTTP from anywhere                  HTTP Restricted to Load Balancer
Users <-------------------------> Load Balancer <----------------------------------> EC2

Users connect from anywhere on port 80 (HTTP) or 443 (HTTPS)
The EC2 Instances allow port 80 traffic, ONLY from the Load Balancer's Security Group
//////////
//////////////////////////////////////////////////////////////////////////

Application Load Balancer (ALB)
//////////////////////////////////////////////////////////////////////////

Application Load Balancers (v2)
//////////
Application Load Balancers is Layer 7 (HTTP)
Load balancing to multiple HTTP applications across machines (target groups)
Load balancing to multiple apps on same machine (Ex. Containers)
Support for HTTP/2 and WebSocket
Support redirects (Ex. from HTTP to HTTPS)

Routing tables to different target groups:
	Routing based on path in URL, Ex. (ex.com/users, ex.com/posts)
	Routing based on hostname in URL, Ex. (one.ex.com, two.ex.com)
	Routing based on Query String, Headers, Ex. (ex.com/users?id=123&oder=false)
ALBs are a great fit for micro services & container-based apps, Ex. (Docker, Amazon ECS)
ALBs have a port mapping feature to redirect to a dynamic port in ECS
					(ECS: Elastic Container System for Docker)
//////////

HTTP Based Traffic
//////////
Diagram shows how one ALB can use URL path to route traffic to different Target Groups
It also shows that the Health Checks are at the Target Group level
                                                          ____________________________
                        _____________        HTTP        |Target Group   EC2 Instances| 
      Route/user       |             |<----------------->|for Users app               |
   |<----------------->|  External   |                   |               Health Check |
WWW|  Route/search     | Application |                   |____________________________|
   |<----------------->|Load Balancer|       HTTP         ____________________________
                       |    (v2)     |<----------------->|Target Group   EC2 Instances|
                       |_____________|                   |for Search app              |
                                                         |               Health Check |
                                                         |____________________________|
//////////

Target Groups
//////////

These are the resources that the ALB distributes traffic across
These can be:
	EC2 Instances on HTTP (can be managed by an Auto Scaling Group)
	ECS tasks on HTTP (Managed by ECS)
	Lambda Functions on HTTP (request is translated into a JSON event)
	IP Addresses (private IPs)

ALB can route to multiple Target Groups
Health Checks are at the Target Group level
//////////

Query Strings/Parameters Routing
//////////
Diagram shows how an ALB can use URL parameters to route traffic
                                                          ____________________________
                        _____________ ?platform=Mobile   |Target Group 1              | 
      Requests         |             |<----------------->| (EC2 Instances)     Health |
   |<----------------->|  External   |                   |                     Check  |
WWW|                   | Application |                   |____________________________|
   |                   |Load Balancer|?platform=Desktop   ____________________________
                       |    (v2)     |<----------------->|Target Group 2              |
                       |_____________|                   |  (On-Premises,       Health|
                                                         | Private IP Routing)  Check |
                                                         |____________________________|
//////////

Good to Know
//////////
ALBs have a fixed hostname
The app servers don't see the IP of the client directly
	The IP of the client is inserted in the header X-Forwarded-For
	It is also possible to get the Port (X-Forwarded-Port)
	And the proto (X-Forwarded-Proto)
This diagram shows how the ALB does a Connection Termination to protect the EC2 Instance
	This also means that the client's IP is hidden from the app on the EC2 Instance
	We can still get this through the Header

                                       Load Balancer IP   ____________________________
                        _____________     (Private IP)   |                            | 
      Client IP        |    ALB      |<----------------->|  EC2 Instance              |
     12.34.56.78<----->| Connection  |                   |                            |
                       | Termination |                   |____________________________|
                       |             |
                       |             |
                       |_____________|
//////////
//////////////////////////////////////////////////////////////////////////

Application Load Balancer - Hands On 1
//////////////////////////////////////////////////////////////////////////

Launch EC2 Instance:
	> EC2 > Instances
	-> Launch Instances
	Set the number of Instances to 2
	Select Linux
	Select t2.micro
	Proceed w/o a Key Pair
	Under: Network Settings
		Select Existing Security Group (launch-wizard-1)
	Under: Advanced, copy over the standard site creation script
	-> Launch Instance
	When those are ready, check their public IP addresses to make sure they're good

Launch ALB:
	> EC2 > Load Balancers
	-> Create Load Balancer
	-> ALB's Create
	Set a unique name (account level)
	Keep IPv4 as the Address Type
	Select all of the AZs
	-> Create New Security Group
		Set a name and description
		-> Add Inbound Rule
			Type: HTTP
			Port: 80
			Source: Anywhere
		-> Create Security Group
	Remove the default Security Group
	Set the Security Group as the one we just created
	Under Listeners and Routing:
		Type: HTTP
		Port: 80
		-> Create Target Group
			Select Instances
			Set a name
			Protocol: HTTP
			Port: 80
			-> Next
			Select the two EC2 Instances as targets
			-> Include as Pending Below
			-> Create Target Group
		Default Action: *Target Group that was just created*
			// Might need to refresh that section
	-> Create Load Balancer
Now, going to the public IP of the Load Balancer brings up one of the EC2 Instances
Repeated refreshes of the page switches between the two EC2 Instances

//////////////////////////////////////////////////////////////////////////

Application Load Balancer - Hands On 2
//////////////////////////////////////////////////////////////////////////

Send traffic to the EC2 Instances from the Load Balancer only
This will make the EC2 Instances private, only accessed by the ALB
//////////
> EC2 > Instances > Security Groups
	Select launch-wizard-1
	Select Inbound Rules tab
	-> Edit Rules
		Delete the rule for Port 80 traffic from anywhere
		Add a new Rule
			Type: HTTP
			Port: 80
			Source: *scroll down and select the ALB Security Group*

Now the EC2 Instances can't be accessed directly.
They can still be accessed through the ALB's public IP
//////////

Setting Up an ALB Error Page
//////////
> EC2 > Load Balancers
	Select the DemoALB
	Scroll down to Listeners
	-> HHTP:80
	Scroll down to Listener's Rules
	-> Add Rule
		Set name
		-> Next
		-> Add Condition
			Type: Path
			Path: /error
			-> Confirm
		-> Next
		Select Return Fixed Response
		Response Code: 404
		Content type: text/plain
		Response Body: "Page not found"
	Priority: 5 // Higher number, higher priority
	-> Next
	-> Create
//////////
//////////////////////////////////////////////////////////////////////////

Network Load Balancer
//////////////////////////////////////////////////////////////////////////

Overview
//////////
Network Load Balancers (NLBs) are Layer 4, and can:
	Forward TCP and UDP traffic to instances
	Handle millions of requests per second
	Have ultra-low latency

An NLB has one static IP per AZ, and supports Elastic IP
 				(Elastic IP: IP Address doesn't change on stop/start)

NLBs are used for extreme performance, TCP/UDP traffic

NOT included in the AWS Free Tier
//////////

TCP (Layer 4) Based Traffic
//////////
Diagram shows how an NLB routes traffic based on TCP vs HTTP
                                                          ____________________________
                        _____________        TCP         |Target Group (Users)        | 
   |  TCP + Rules      |             |<----------------->|                     Health |
WWW|<----------------->|  External   |                   |(EC2 Instances)      Check  |
   |                   |   Network   |                   |____________________________|
                       |Load Balancer|      HTTP          ____________________________
                       |    (v2)     |<----------------->|Target Group (Search)       |
                       |_____________|                   |                      Health|
                                                         | (EC2 Instances)      Check |
                                                         |____________________________|
//////////

Target Groups
//////////
Target Groups can be:
	EC2 Instances
	IP Addresses - MUST be private IPs
	An ALB
Health Checks support the TCP, HTTP, HTTPS Protocols

EC2 Instances:
              |-->EC2 Instance 1
NLB-----------|
              |-->EC2 Instance 2

IP Addresses:
              |-->192.168.23.23  (EC2 Instance)
NLB-----------|
              |-->10.0.0.21     (on-site server)

ALB:
NLB-----------> ALB
//////////
//////////////////////////////////////////////////////////////////////////

NLB Hands On
//////////////////////////////////////////////////////////////////////////
> EC2 > Load Balancers
	-> Create Load Balancer
	Select Network Load Balancer
	Set name
	Set Internet Facing
	Select all AZs
	Remove default Security Group
	// Create a new Security Group
		> EC2 > Security Group
		-> Create Security Group
		Set name
		Set Description
		Under Inbound Rules
			Type: HTTP
			Port: 80
			Source: Anywhere
	Set the Security Group to what was just created
	Under Listeners and Routing
		Protocol: TCP
		Port: 80
		Create a Target Group
			Select Instances
			Set Target Group Name
			Protocol: TCP
			Port: 80
			VPC: Same as before
			Under Health Checks
				Protocol: HTTP
				Health Check Path: /
			Under Advanced Health Check Settings
				Healthy Threshold: 2
				Timeout: 2
				Interval: 5
			-> Next
			Select the EC2 Instances
		Default Action: Select the new Target Group
	-> Create Load Balancer

Going to the Public IP of the NLB gives no result.
This is b/c the Security Group Rules for the EC2 Instances.
It still only allows the ALB as per the Inbound Rules.
In fact, according to the NLB, the EC2 Instances are Unhealthy.

Add the NLB to the EC2 Instances Security Group Rules
> EC2 > Security Group
Select the Inbound Rules
-> Edit
-> Add Rule
	Type: HTTP
	Port: 80
	Source: *NLB Security Group*
The EC2 Instances will be Healthy.
We can now see them by going to the NLB's Public IP

//////////////////////////////////////////////////////////////////////////

Gateway Load Balancer (GWLB)
//////////////////////////////////////////////////////////////////////////

Overview
//////////
Gateway Load Balancers greatly simplify the task of inspecting traffic to an app
This load balancer focuses on security system Target Groups
	Just like other Load Balancers, it spreads out the traffic between them
- Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS
- Ex: 
	Firewalls
	Intrusion Detection and Prevention Systems
	Deep Packet Inspection Systems
	Payload manipulation
- Operates at Layer 3 (Network Layer) - IP Packets
- Combines the following functions:
	Transparent Network Gateway - single entry/exit for all traffic
	Load Balancer - distributes traffic to virtaul appliances
- Uses the GENEVE protocol on port 6081
//////////

Diagram 1
//////////
In this diagram:    
1. 
Users send their connect and data through the GWLB
The GWLB spreads the tasks across a Target Group of security software

2.
The approved traffic is sent back to the GWLB

3.
The approved traffic is then routed to the application
                                   _____________________
                         ____     |  Target Group       |
                        |    |1.  | (3rd party Security |
Users----Route Table--->|GWLB|--->|  Virtual Appliances |
                        |    |    |                     |
                        |    |2.  |                     |
                        |    |<---|_____________________|
                        |    |
                        |    |3.    ___________
                        |____|---->|Application|
                                   |___________|

//////////

GWLB - Target Groups Diagram
//////////

Target Groups can be:
	resource endpoints, like EC2 Instances
	IP Addresses - MUST be private IPs
                                                          ____________________________
                                                         |Target Group (EC2 Instances)|
                                                         |  _                         |
                        _____________        |------------>|_| Instance 1             | 
                       |             |-------|           |  _                         |
                       |    GWLB     |       |------------>|_| Instance 2             |
                       |             |                   |____________________________|
                       |_____________|                    ____________________________
                        _____________                    |Target Group (IP Addresses) |
                       |             |                   |                            |
                       |    GWLB     |       |------------>EC2 Instance (192.168.1.10)|
                       |             |-------|           |                            |
                       |_____________|       |------------>Server (10.168.1.10)       |
                                                         |____________________________|
//////////
//////////////////////////////////////////////////////////////////////////

Elastic Load Balancers - Sticky Sessions
//////////////////////////////////////////////////////////////////////////

Overview
//////////
Sticky Sessions allow Users to keep data on the application.
This keeps users on the same EC2 Instance and not be bumped to a different one
	This is done by a cookie specifically used for Sticky Sessions
		The cookie has an expiration date controlled by the dev
This works for ALBs and NLBs
This of course can cause the Load Balancers to become unbalanced
//////////

Cookie Names
//////////
Application-based Cookies
	Custom Cookie
		Generated by the target (the app itself)
		Can include custom attributes required by the app
		Cookie name must be specified individually for each Target Group
		DO NOT USE:
			AWSALB
			AWSALBAPP
			AWSALBTG 
			These are all reserved by the ELB
	Application Cookie
		Generated by the Load Balancer
		Cookie name is AWSALBAPP

Duration-Based Cookies
	Cookie generated by the Load Balancer
	Cookie name is:
		AWSALB, for ALB
		AWSELB, for CLB
//////////

Hands On
//////////

> EC2 > Target Groups
	Under Actions dropdown
		Select Edit Attributes
			Scroll down to Target Selection Configuration
				-> Turn on stickiness
				Choices are:
					Load Balancer generated (duration based)
						Can set duration
					Application-Based ()
						Can set duration
						Set name as well
			-> Save Changes

Now, reloading the ALB page doesn't bring up a different EC2 Instance page.
Looking at the session details in the Web Developer Tools Console,
	The cookie can be found with the name that it was set with
//////////
//////////////////////////////////////////////////////////////////////////

Cross Availability Zone Load Balancing
//////////////////////////////////////////////////////////////////////////

There can be an issue where two AZs have an uneven number of EC2 Instances
	The traffic for the AZs are distributed evenly between the two AZs
	But, this overloads the AZ with fewer EC2 Instances
//////////
Diagram Ex:
//////////
NO Cross-Zone Load Balancing:
                                                          ____________________________
                                                         |AZ1                         |
                        _____________        50%         |                            |
                       |             |-------------------->EC2 Instances (x8)  50%    | 
                       |             |                   |   (6.25% per Instance)     |
                       |             |                   |                            |
                       |             |                   |____________________________|
                       |             |                    ____________________________
                       |             |                   |AZ2                         |
                       |             |       50%         |                            |
                       |      LB     |-------------------->EC2 Instances (x2)  50%    |
                       |             |                   |   (25% per Instance)       |
                       |_____________|                   |                            |
                                                         |____________________________|
WITH Cross-Zone Load Balancing:
                                                          ____________________________
                                                         |AZ1                         |
                        _____________        50%         |                            |
                       |             |-------------------->EC2 Instances (x8)  80%    | 
                       |             |                   | (10% per Instance)         |
                       |             |                   |                            |
                       |             |                   |____________________________|
                       |             |                    ____________________________
                       |             |                   |AZ2                         |
                       |             |       50%         |                            |
                       |      LB     |-------------------->EC2 Instances (x2)  20%    |
                       |             |                   | (10% per Instance)         |
                       |_____________|                   |                            |
                                                         |____________________________|
//////////

Enabling Cross-Zone Load Balancing
//////////
Application Load Balancer
	Enabled by default (can be disabled at Target Group Level)
	No charges for inter-AZ data
	
Network Load Balancer & Gateway Load Balancer
	Disabled by default
	Pay charges for inter AZ data if enabled
//////////

Hands On: NLB
//////////
> EC2 > Load Balancers > *NLB name*
	Scroll down
	Select Attributes tab
	-> Edit
		Enable/Disable Cross-zone Load Balancing
		-> Save Changes
//////////

Hands On: GWLB
//////////
> EC2 > Load Balancers > *GWLB name*
	Scroll down
	Select Attributes tab
	-> Edit
		Enable/Disable Cross-zone Load Balancing
		-> Save Changes
//////////

Hands On: ALB
//////////
> EC2 > Load Balancers > *ALB name*
	Scroll down
	Select Attributes tab
	-> Edit
		Cross-zone Load Balancing is locked on and can't be disabled here
	
	Select Listeners tab
		Under Forward to
			-> *Target Group name*
				Select Attributes tab
					-> Edit
						Change Cross-Zone Load Balancing
						-> Save Changes
//////////
//////////////////////////////////////////////////////////////////////////

SSL Certificates
//////////////////////////////////////////////////////////////////////////

SSL Certs are the way that hosts guarantee a secure connection from client to entry-point
	This is what makes traffic HTTPS vs HTTP
		HTTP is used for the internal services, which are secure w/ Security Groups
	TLS is latest version, but still referred to as SSL
	SNI is the way to have an LB acting as the entry-point for multiple websites
		(SNI: Server Name Indication)
		The LB has an SSL Cert for each website (hostname)
		SNI requires the client to give the desired hostname on initial handshake

SSL/TLS - Basics
//////////
These security certs ensure encryption over the web, from client to our entry-point.

An SSL Certificate allows for traffic between users and a Load Balancer to be encrypted
	AKA: In-Flight Encryption
SSL: Secure Sockets Layer, encrypts connections
TLS: Transport Layer Security, new version of SSL

Current practice is to use TLS, but is still referred to as SSL

SSL Certs are issued by Certificate Authorities (CAs)
	Ex. (GoDaddy, Comodo, Symantec, etc...)

SSL Certs have a set expiration date and must be renewed
//////////

SSL Certificates Diagram
//////////
for all outer traffic, use HTTPS to ensure security.
For all inner traffic, can use just HTTP
	Make sure that inner traffic is secure with Security Groups

       HTTPS(encrypted) Over www                 HTTP, Over private VPC
Users <-------------------------> Load Balancer <----------------------> EC2 Instance

Outer traffic is done over HTTPS, making sure that it is secure
Inner traffic is done over HTTP, because we already know the traffic is secure

Load Balancer uses an X.509 cert (SSL/TLS server certificate)
Certificates can be managed by ACM (AWS Certificate Manager)
	Personal SSL Certs can also be uploaded

HTTPS Listener:
	MUST specify a default cert
	Can add a list of certs to support multiple domains
	Clients can specify a security policy to support older versions of SSL/TLS
	Clients can use an SNI to specify hostname of site they want
		(SNI: Server Name Indication)
See next section for more info on SNI
//////////

SNI - Server Name Indication
//////////
SNI is a newer protocol where the client gives the LB the desired hostname
	The LB uses this info to load the correct SSL Cert and direct traffic to that site

SNI allows for having multiple SSL certs on one web server
	This is to serve multiple websites behind the Load Balancer
SNI is a newer protocol (so not universally supported)
	This is the client stating the desired hostname on initial handshake
Server finds the correct cert and loads that
	If not found, loads the default

Note:
	Only works for ALB and NLB and CloudFront
		Does NOT work for Classic LB
//////////

Summary: Elastic Load Balancers - SSL Cert
//////////
ALB and NLB
	Supports multiple Listeners w/ multiple SSL Certs
		Uses SNI to make this work
//////////

//////////////////////////////////////////////////////////////////////////

SSL Certificates Hands On
//////////////////////////////////////////////////////////////////////////

Both ALB and NLB are very similar for setting up the SSL/TLS Cert
Basically, this is just adding a listener and loading in the cert

> EC2 > Load Balancers
	-> Select ALB
		-> Add Listener
			Protocol: TLS
			Port:443
			Under Forward to
				Choose: Target Group, w/ HTTP
			Secure Listener Settings:
				Set the ALB Security Policy
				Select SSL/TLS Cert location
	-> Select ALB
		-> Add Listener
			Protocol: TLS
			Port:443
			Under Forward to
				Choose: Target Group
			Secure Listener Settings:
				Set the ALPN Policy
				Select SSL/TLS Cert location

//////////////////////////////////////////////////////////////////////////

Connection Draining aka Deregistration Delay
//////////////////////////////////////////////////////////////////////////

Feature name:
	Classic Load Balancer: Connection Draining
	ALB and NLB: Deregistration Delay

Gives a delay to complete in-flight requests,
	while an Instance is de-registered OR marked unhealthy
Stops sending traffic to Instance which is de-registering

Diagram
//////////
While the EC2 Instance is in a Draining state, it will receive no new traffic.
Traffic will be routed to other EC2 Instances

                      |<----> EC2 Instance (Draining)
Users <----> ELB <--->|<----> EC2 Instance
                      |<----> EC2 Instance

Time to complete in-flight requests to the EC2 Instance
Stops sending new requests to EC2 Instance which is de-registering
Default is 300 seconds, can be set from 1 to 3600 seconds
Can be disabled (set to 0)
Set to a low value if requests are short (30 seconds)
If the app is preforming a long process, it's better to set a longer time
//////////
//////////////////////////////////////////////////////////////////////////

Auto Scaling Group (ASG) Overview
//////////////////////////////////////////////////////////////////////////
ASGs increase or decrease the number of Instances based on load or CloudWatch Alarms
Attributes for the new Instances need to have been set

Overview
//////////
Website load can change very quickly
Cloud has features to add and remove Instances very quickly

Goal of an ASG is to:
	Scale out (add EC2 Instances) when load increases
	Scale in (remove EC2 Instances) when load decreases
	Ensure min/max number of EC2 Instances running
	Automatically register new Instances to a Load Balancer
	Re-create an EC2 Instance in case a previous one is terminated
ASGs themselves are free, but of course we must pay for all resources used
//////////

Diagram:
//////////           
ELB can check the health of EC2 Instances, removing them if unhealthy
Scales out on load increase, up to set max
Scales in on load decrease, down to set min

Normal, from Initial Capacity: (Min Instances: 2, Max Instances: 4)
                           ________________
                          |   ASG          |
                          |                |
                      |<----> EC2 Instance |
             ELB <--->|<----> EC2 Instance |
                      |<----> EC2 Instance |
                          |________________|

Load Increased: (Min Instances: 2, Max Instances: 4)
                           ________________
                          |   ASG          |
                          |                |
                      |<----> EC2 Instance |
             ELB <--->|<----> EC2 Instance |
                      |<----> EC2 Instance |
                      |<----> EC2 Instance |
                          |________________|

Load Decreased: (Min Instances: 2, Max Instances: 4)
                           ________________
                          |   ASG          |
                          |                |
             ELB <--->|<----> EC2 Instance |
                      |<----> EC2 Instance |
                          |________________|

//////////

Auto Scaling Group Attributes
//////////
There is a list of Attributes that are used when launching new Instances
The ASG itself also has Attributes needed for Instance count boundaries

A Launch Template has:
	AMI + Instance Type
	EC2 User Data
	EBS Volumes
	Security Groups
	SSH Key Pair
	IAM Roles for your EC2 Instances
	Network + Subnets Info
	Load Balancer Info
Min / Max / Initial Capacity
Scaling Policies
//////////

CloudWatch Alarms and Scaling
//////////
Scaling can done using CloudWatch Alarms
Alarms monitor a metric (Average CPU or custom metric)
Metrics like Average CPU are computed for overall ASG Instances
Actions based on alarm:
	Create scale-out policies
	Create scale-in policies
//////////
//////////////////////////////////////////////////////////////////////////

Auto Scaling Group (ASG) Hands On
//////////////////////////////////////////////////////////////////////////

Creating an ASG mostly

> EC2
Select: Auto Scaling Groups on the left-hand side
	-> Create Auto Scaling Group
		Set name
		-> Create Launch Template
			Set name
			Set Description
			Select Amazon Linux 2 (Free Tier)
			Select Instance Type: t2.micro
			Key Pair: Select key-pair
			Subnets: Don't include in Launch Template
			Select the Security Group
			For User Data: Input the standard website installer script
			-> Create Launch Template
		Set Launch Template to what we just created
		-> Next
		Leave Defaults
		-> Next
		Select: Attach to Existing Load Balancer
		Select: Choose from your Load Balancer Target Groups
			Chooses the Target Group attached to the LB
		Health Checks
			Check ELB
		-> Next
		Leave Defaults
		-> Next
		Leave Defaults
		-> Next
		Review details
		-> Create Auto Scaling Group

Now the chosen LB will show the EC2 Instance(s) that are a part of the ASG

In setting up the ASG, we left the min/max/initial all set to 1
	So only 1 EC2 Instance was created
> EC2 > Auto Scaling Groups > *ASG name*
	Select: Details tab
		-> Edit
			Change the min/max/desired capacities as needed
//////////////////////////////////////////////////////////////////////////

Auto Scaling Groups - Scaling Policies
//////////////////////////////////////////////////////////////////////////
There are a few types of triggers that can be used with an ASG
	How much the trigger scales in/out can also be modified
We can also create custom triggers with CloudWatch


Types of Scaling
//////////
Dynamic Scaling
	Target Tracking Scaling
		Simple to set up
		Ex: (Keep average ASG CPU to around 40%)
	Simple / Step Scaling
		CloudWatch Alarm triggers when CPU > 70%, then add 2 Instances
		CloudWatch Alarm triggers when CPU < 30%, then remove 1 Instance
Scheduled Scaling
	Scaling based on known usage patterns
	Ex: Increase capacity on Fri at 5pm
Predictive Scaling:
	Forecasts what the need will be and adjusts to meet that
	Forecast will be trying to find cyclical data
//////////

Good Metrics to Scale On
//////////
CPUUtilization: Avg CPU load across all Instances
RequestCountPerTarget: Keep requests to each Instance stable
Avg Network In/Out: (For network bound apps) 
Any custom metric: Uses CloudWatch triggers
//////////

Scaling Cooldowns
//////////
Triggers don't cause an effect right away.
The ASG gives the Instances a chance to stabilize

Cooldown period takes effect after a scaling activity happens
During the cooldown period, ASG will not perform any more scaling 
	(giving a chance for metrics to stabilize)
Advice: Using a ready-to-use AMI will reduce config time
	Thus, allowing us to reduce cooldown periods
//////////
//////////////////////////////////////////////////////////////////////////

Auto Scaling Groups - Scaling Policies Hands On
//////////////////////////////////////////////////////////////////////////
> EC2 > Auto Scaling Groups > *ASG_name*

Here we can change the Scaling Policy that the ASG uses
There are three broad categories
	Scheduled Actions
	Predictive Scaling
	Dynamic Scaling

Scheduled Actions
//////////
-> Create Scheduled Action
	Set name
	Set Desired Capacity, Min, and Max
	Set Recurrence
	Can also set Start and End times
//////////

Predictive Scaling
//////////
-> Create Predictive Scaling Policy
	This is machine-learning driven!
	Under: Turn on Scaling
		Turn on Scaling Based on Forecast
	Under: Metrics and Target Utilization
		Select the Metric to Scale on
		Set Target Utilization
	-> Create
This does need about a week or so of data
//////////

Dynamic Scaling
//////////
-> Create Dynamic Scaling Policy
	Set Policy Type
		Step Scaling: How much to add/remove based on the breached metric
			So, Add 10 Instances if CPUs at 50%, 15 if at 60%
		Target Tracking Scaling: Things like CPU Utilization
	Select: Metric Type
	Set: Target Value
	-> Create
This will also create the CloudWatch Alarms for us

Go to the Details tab
	-> Edit
		Set the Min, Max, and Desired Capacities

We can use a stress program to fully utilize the EC2 Instance, so CPU at 100%
	This triggers the CloudWatch Alarm and new Instances are created
The EC2 Instances can be observed under the Monitoring tab
Actions like Adding/Removing can be seen under the Activity tab
The created CloudWatch Alarms can be seen at: > CloudWatch > Alarms
//////////
//////////////////////////////////////////////////////////////////////////

Auto Scaling - Instance Refresh
//////////////////////////////////////////////////////////////////////////
Instance Refresh is a way of updating the EC2 Instances
	This involves gradually removing the old Instances and spinning up new ones

Goal: Update launch template and re-creating all EC2 Instances
Set the minimum healthy percentage
Specify warm-up time (how long until the new EC2 Instance should be ready)

//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 8
  __         __        
 /  \ |  |  /  \  
 |    |__|  \__/  
 |    |  |  /  \   
 \__/ |  |. \__/

AWS Fundamentals: RDS + Aurora + Elasticache
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

Amazon RDS Overview
//////////////////////////////////////////////////////////////////////////

RDS: Relational Database Service
This is a managed DB service, using SQL as the query language
These DBs are in the cloud and managed by AWS
	Postgres
	MySQL
	MariaDB
	Oracle
	Microsoft SQL Server
	IBM DB2
	Aurora (AWS Proprietary database)

AWS Managed DBs
//////////
These databases being managed means that Amazon is taking care of their infrastructure
This means that AWS gives us a lot of features here
	Automated provisioning, OS patching
	Continuous backups and restore to a timestamp (Point in Time Restore)
	Monitoring dashboards
	Read replicas for increased read performance (horizontal scaling)
	Increased size on demand (vertical scaling)
	Multi AZ setup for Disaster Recovery (DR)
	Maintenance windows for upgrades
	Scaling (vertical and horizontal)
	Storage backed up by EBS

However, can't SSH into Instances of these DBs
//////////

RDS - Storage Auto Scaling
//////////
Dynamically increase storage on RDS DB Instance
Auto-scales on detecting that free storage is running out
Lets us avoid manually scaling the DB
Should set a Maximum Storage Threshold
Storage will automatically modify if:
	Free storage is less than 10% of allocated
	Low-storage lasts at least 5 minutes
	6 hours have passed since last modification
Very useful for apps with unpredictable workloads
Applies to all RDS database engines
//////////
//////////////////////////////////////////////////////////////////////////

RDS Read Replicas vs Multi AZ
//////////////////////////////////////////////////////////////////////////

RDS Read Replicas for read scalability
//////////
Up to 15 Read Replicas // Most likely an exam Q!!!!
These can be:
	Within AZ
	Cross AZs
	Cross Region
Replication is Async, which means that reads are eventually consistent
	Called: Eventually Consistent
	Data may not be completely up to date
While they are replicas, they are only read from, not written to
	They can become their own DB
Applications must update the connection string to use read replicas
//////////

RDS Read Replicas - Use Cases
//////////
Situation: Prod DB with normal load
           Client needs Reporting App to read from DB to run analytics
	   Adding these reads onto the main DB would overload it
Solution:  Create a Read Replica that serves the Analytics App
	   This leaves the main DB completely unimpacted
//////////

RDS Read Replicas - Network Cost
//////////
With other services, AWS will charge if data travels from one AZ to another
However, with RDS Read Replicas, AWS does NOT charge for cross AZ data transfer
Once this becomes cross-region, there will be charges

Cross AZ: Free                          Cross Region: $$$
 __________        __________          ___________        ____________
|  AZ 1a   |      |   AZ 1b  |        |  Region 1 |      |  Region 2  |
|__________|<---->|__________|        |___________|<---->|____________|
//////////

RDS Multi AZ (Disaster Recovery)
//////////
Sync Replication
One DNS name - automatic app failover to standby
	This is all done under the one hostname, so any replacing of main is invisible to users
	This means that if the main fails, the standby automatically replaces main
Increases availability
Failover in case of AZ, network, or storage loss
All automatic, so no manual intervention needed
This is not for scaling
	The Standby does not get any read/write requests
	It is only a backup that replaces the main DB if things go wrong
Can be set up as Multi AZ for disaster recovery
//////////

RDS - From Single AZ to Multi AZ
//////////
How to make an RDS in one AZ (Single AZ) available in multiple AZs (Multi AZ)

This can be done as a zero downtime operation (no need to stop the DB)
From the developer perspective, this just involves selecting "Modify" and changing a setting
What goes on internally:
	A snapshot is taken
	A new DB is restored from the snapshot into the new AZ
	The two DBs are synced (Sync Replication)
	// I'm guessing the snapshot is deleted
//////////
//////////////////////////////////////////////////////////////////////////

RDS Hands On
//////////////////////////////////////////////////////////////////////////
Goal:
	Creating an RDS DB, connecting to it, and uploading data

> RDS > Create Database

Select Standard Create // Longer, but shows all options
	Other Option is Easy Create
Choose MySQL
Select the version
Under: Templates
	Select: Free Tier
Under: Availability and Durability
	Select: Single DB Instance
Under: Settings
	Master Username: "admin"
		Should already be set to this
	Set a password
	Confirm password
Under: Instance Configuration
	Select: Burstable Classes
		Burstable Classes are EC2 Instances that don't usually make full use of the CPU
			But they have the capacity to make full use when needed
	Select: db.t3.micro // These settings let us stay in the Free Tier
Under: Storage
	Select: gp2 // Lets us stay in Free Tier
	Deselect Autoscaling // The Udemy tutorial keeps this checked, but why?
Under: Connectivity
	Select: Don't Connect to EC2 Instance
	// Connecting to an Instance makes things easier, but we'll look at all of the options
	Public Access: Check Yes
		This is VERY IMPORTANT for connecting from a desktop DB tool
	-> Create New Security Group
		Set name
		Set AZ to No-Preference
		Leave Port as 3306
Under: Database Authorization
	Select: Password Authorization
Under: Monitoring
	Disable Monitoring
Under: Additional Configuration
	Set the DB name
-> Create Database

From here, we'll open up a desktop B tool, like MySQL Workbench
Once open, we put in the details for the DB to a desktop DB tool
	DB Type (MySQL)
	DB Address
	DB Password
	DB Username (admin)
	DB name
Errors:
	Check that all of the details are correct
	Check that the DB is ready
	Check that the DB is set to Public
	Check that the DB has a Security Group that allows all traffic from Anywhere
Now we can use our DB tool to add, pull, and modify data using SQL commands

Additionally, from the Management console, we can:
	Make a snapshot of the DB
	Monitor the status and actions under the Monitoring tab
//////////////////////////////////////////////////////////////////////////

Amazon Aurora
//////////////////////////////////////////////////////////////////////////
Aurora is a cloud-focused DB by Amazon
This means that it has a lot of really great features specific to the cloud
Things like high availability, auto-scaling read replicas, and fast backup from standbys

Overview
//////////
- Aurora is proprietary to Amazon, not open sourced
- Postgres or MySQL DB connections are supported
	So, anything that connects to these can connect to Aurora
- It's AWS cloud optimized, (5x perf over MySQL on RDS, 3x perf over Postgres on RDS)
- Storage auto-grows in increments of 10GB, up to 128TB of storage
- Can have up to 15 Read Replicas, with a faster replication process (<10 seconds lag)
- Failover recovery is instant
- Aurora does cost more than RDS (20% more)
//////////

High Availability and Read Scaling
//////////
Aurora will create 6 copies of the data across 3 AZs
	4 copies out of 6 needed for writes
	3 copies out of 6 need for reads
	Self healing with peer-to-peer replication
	Storage is striped across 100s of volumes
All of this leads to high redundancy, high availability, and data that can be verified by the other copies of it

One Aurora Instance takes Writes (the Master)
Automated failover can happen in less than 30 seconds
The Master can have up to 15 Read Replicas, AND they can become the Master on a Failover
These Read Replicas also support Cross-Region Replication
//////////

Aurora DB Cluster
//////////
With the Aurora DB Cluster, there is a single Endpoint for Writes to the Master DB
In the case of a Failover, that Endpoint will automatically switch to the new Master
	And all clients will never have to change anything (sending Writes to the Endpoint)
Very important:
	The Read Replicas will have a Reader Endpoint
	The Reader Endpoint will load balance the reads

Normal:
            ____________________                        ________
Client---->|  Writer Endpoint   |--------> Master ---->| Shared |
           | Pointing to Master |                      | Storage|
           |____________________|                      | Volume |
						       |        |
            ___________________________                |        |
Client<----|  Reader Endpoint          |<|<-Replica1<--|        |
           | Connection Load Balancing | |<-Replica2<--|        |
           |___________________________| |<-Replica3<--|        |
					               |________|

On a Failover:
The Read Replica 1 becomes the Master, and the client sees no disruption
            ____________________                        ________
Client---->|  Writer Endpoint   |   (Former) Master    | Shared |
           | Pointing to Master |----> (New) Master--> | Storage|
           |____________________|                      | Volume |
						       |        |
            ___________________________                |        |
Client<----|  Reader Endpoint          |<|             |        |
           | Connection Load Balancing | |<-Replica2<--|        |
           |___________________________| |<-Replica3<--|        |
					               |________|
//////////

Features of Aurora
//////////
Bullet-point list of Aurora Features:
	Automatic Failover
	Backup and Recovery
	Isolation and security
	Industry compliance
	Push-button scaling
	Automated Patching w/ zero downtime
	Advanced monitoring
	Routine maintenance
	Backtrack (Restore data from any point in time w/o maintaining backups)
//////////
//////////////////////////////////////////////////////////////////////////

Aurora Hands On
//////////////////////////////////////////////////////////////////////////
Goal:
	Go through the process of creating an Aurora DB

> RDS > Create Database

Under: Choose a database creation method
	Select: Standard Create
Under: Engine Options
	Select: MySQL Compatible
	Keep default version
Under: Templates
	Select: Production
Under: Settings
	DB cluster identifier: database-2
	Master username: admin
	Set and confirm a password
Under: Cluster storage configuration
	Select: Aurora Standard
Under: Instance configuration
	Select: Include previous generation classes
	        Burstable classes (include t classes)
		db.t3.medium
Under: Availability and durability
	Select: Create an Aurora Read Replica // NOT FREE, but good to see options
Under: Connectivity
	Compute Resource: Don't connect to an EC2 compute resource
	Network Type: IPv4
	VPC: Default VPC
	Public Access: Yes
	VPC Security Group: Create new
		name: demo-db-aurora
		Database port: 3306 (Default)
Under: Monitoring
	Deselect: Enable Enhanced Monitoring
Under: Additional configuration
	name: mydb
	Backup retention: 1 day
	Encryption: Enabled
-> Create Database

In: > RDS > Databases
	There is now a database-2
	Under it is a Writer Instance and a Reader Instance
In the Connectivity and security tab:
	Under: Endpoints
		There is the Writer Endpoint and Reader Endpoint
		These Endpoints will always work, even during/after a Failover
Through the Actions dropdown:
	More Readers can be added
	Cross-Region Read Replicas can be created
	Add AWS Region
		Only works if the Global Database feature was enabled
		Wil require a large enough database instance size
	Delete the DB
		To delete the DB, first delete the Read and Write Instances
		Then the cluster can be deleted
//////////////////////////////////////////////////////////////////////////

RDS & Aurora Security
//////////////////////////////////////////////////////////////////////////

At-Rest Encryption:
	The data stored at the Endpoint is encrypted, and needs a password to be accessed
	Can encrypt DB master and Replicas using AWS KMS (must be defined at launch)
	Master must be encrypted for Read Replicas to be encrypted
	To encrypt an un-encrypted DB, go through a DB snapshot and retore as encrypted	
In-Flight Encryption: 
	The entire connection and its traffic is encrypted
	TLS-ready by default
	Clients MUST use the AWS TLS root certificates client-side
IAM Authentication: 
	IAM roles can be used to connect to the DB (Instead of username/password)
	Username and password can still be used
Security Groups: Control Network access to RDS/Aurora DB
No SSH available on RDS Custom
Audit Logs:
	These can be enabled to analyze traffic, but are short-lived
	For longer retention they can be sent to CloudWatch Logs
//////////////////////////////////////////////////////////////////////////

RDS Proxy
//////////////////////////////////////////////////////////////////////////
A proxy for the DB (RDS or Aurora) can be created to reduce load on the actual DB

- Fully managed DB proxy for RDS/Aurora
- Allows apps to pool and share DB connections
- Improves DB efficiency by:
	Reducing stress on DB resources (CPU and RAM)
	Minimizes open connections (and timeouts)
- Serverless autoscaling (Lambda is a big one here), and multi-AZ
- Reduced RDS and Aurora Failover time, up to 66%
- Supports:
	RDS - MySQL, PostgreSQL, MariaDB, MS SQL Server
	Aurora - MySQL, PostgreSQL
- No code changes needed for most apps
- Enforce IAM Authentication for DB and securely store credentials in AWS Secrets Manager
- RDS Proxy is never publicly accessible, only being accessed by the VPC (See diagram)

Diagram:
 __________________________________
|VPC:                              |
| _  _  _  _  _                    |
||_||_||_||_||_| (Lambda Fns)      |
| |  |  |  |  |                    |
| |  |  |  |  | IAM Authentication |
| |__|__|__|__|  to access Proxy   |
|       |                          |
|  _____|_______________________   |
| |     |      (Private Subnet) |  |
| |  RDS Proxy                  |  |
| |     |                       |  |
| |  RDS DB Instance            |  |
| |_____________________________|  |
|__________________________________|
//////////////////////////////////////////////////////////////////////////

ElastiCache Overview
//////////////////////////////////////////////////////////////////////////

Overview
//////////
- The same way RDS is to get managed Relational DBs...
	ElastiCache is to get managed Redis or Memcached
- Caches: in-memory DBs w/ really high performance, low latency
- Helps reduce load off of DB for read intensive workloads
- Helps make your app stateless (puts state of app into ElastiCache)
- This is managed, so AWS takes care of:
	OS maintenance/patching, 
	optimizations, 
	setup, 
	configuration, 
	monitoring, 
	failure recovery,
	backups
- This is not a simple change! It takes heavy app changes to work.
	(App has to be written to query the cache before)
//////////

Solutions Architecture - User Session Store
//////////

- Apps query ElastiCache instead of RDS
	On a "Cache Miss" the query result is saved in ElastiCache for future queries
	The whole idea is that by saving common queries, performance will improve
- Helps relieve load on the RDS
- Cache must have an invalidation strategy
	Make sure only the most current data is there
	Make sure only the most relevant data is there

Diagram:

This shows that in the case of a Cache Hit, the app reads from ElastiCache and finishes
In the case of a Cache Miss, the app reads from RDS
	The query is saved in ElastiCache
 __________                _______________
|          | Cache Hit    |               |
|          |<-----------> |   Amazon      |
|   App    |              | ElastiCache   |
|          | Cache Miss   |               |
|          |--------------->              |
|          | Read from RDS|               |
|          |<------------------------------------> Amazon RDS
|          | Cache Write  |               |
|          |--------------->              |
|__________|              |_______________|
//////////

Solution Architecture - User Session Store
//////////
- User logs into any part of the app
- App writes session data to ElastiCache to save it
	Now, if the User is redirected to another app Instance, the session just resumes
- User hits another Instance of the app
- Instance retrieves the data, and the User is already logged in
- This makes the app stateless
	(A stateless app doesn't save User session data, here it's saved in ElastiCache)
	What are we saving to keep the User logged in???
		Could it be a cookie???

Diagram:

The User has their session saved to ElastiCache while connected to Instance 1
User has is directed to Instance 2 on a Load Balance
	Their Session data is retrieved from ElastiCache, and continues seamlessly
 __________                _______________                         _______________
|          |              |               | Write User Session    |    Amazon     |
|          |<------------>|  Instance 1   |---------------------->|  ElastiCache  |
|  User    |              |_______________|                       |               |
|          |              |               | Retrieve User Session |               |
|          |<------------>|  Instance 2   |<----------------------|               |
|          |              |_______________|                       |               |
|          |<------------>|  Instance 3   |                       |               |
|          |              |               |<----------------------|               |
|__________|              |_______________|                       |_______________|
//////////

Redis vs Memcached
//////////
Redis:
	Multi-AZ w/ Auto-Failover
	Read Replicas to scale reads and high availability
	Data Durability using Append-only File (AOF) persistence
		AOF is used for data durability
		It is a record of every write command, used to restore data
	Backup and restore features
	Supports Sets and Sorted Sets
	Replication:
		Instance <--------------> Instance

Memcached:
	Multi-node for partitioning of data (sharding)
		Sharding: Breaking up a large DB for faster searching
			  These shards are stored in different DB Instances
				And sometimes different physical servers
	No high availability
	Non persistent
	Backup and restore (Serverless)
	Multi-threaded architecture
	Sharding:
		Instance <--------------> Instance
//////////
//////////////////////////////////////////////////////////////////////////

ElastiCache Hands On
//////////////////////////////////////////////////////////////////////////

> Amazon ElastiCache
//////////
-> Get Started
-> Redis

Under: Configuration
	Select: Design your own cache 
		// Serverless is simpler, but we want to see the options
		Cluster Cache
Under: Cluster Mode
	Selelct: Disabled
Under: Cluster Info
	Set a name: "DemoCluster"
Under: Location
	Select: AWS Cloud
	Multi-AZ: unchecked // Very good to have in production, but does incur costs
Under: Cluster Settings
	Node Type: micro t2 and micro t3 are in Free Tier
	Number of Replicas: 0 // Good to have in production, but does incur costs
Under: Subnet Group Settings
	Set name: "my-first-subnet-group"
Under: Availability Zone Placements
	// Another good to have in production, but not needed here
-> Next
Under: Security
	Uncheck: Encryption at Rest
			// We could choose a key for this
		 Encryption in Transit
			// We could choose Access Control, eith Redis AUTH or User Group
Under: Backup
	Uncheck: Enable automatic backups
-> Next
-> Create

We now have a light Redis ElastiCache
Selecting the cache name gives us details about the cache
	This does include the Primary adn Reader endpoints that would be used by an app
There isn't an easy way to see this in action, as it take a script to interact with it
//////////
//////////////////////////////////////////////////////////////////////////

ElastiCache Strategies
//////////////////////////////////////////////////////////////////////////

Caching Implementation Considerations
//////////
- Lots of good info at https://aws.amazon.com/caching/best-practices/
- Is it safe to cache data?
	Data should be secure
	Data may be out of date, but eventually consistent
- Is caching effective for the data?
	This depends on how/if the data is changing
	Pattern: data changing slowly, few keys are frequently needed
	Anti-pattern: data changing rapidly, all large key space frequently needed
- Is data structured correctly for caching?
	Ex: (Key-Value caching, or caching of aggregations results)
	The data has to be in an easily queried format for caching to be beneficial
- Which caching design pattern will be the most appropriate?
//////////

Lazy Loading / Cache-Aside / Lazy Population
//////////
The three names all describe the same thing

Diagram:
Basic cache hit/miss behavior
When first started, the ElastiCache will be empty and need to be "warmed"
	This is just when the ElastiCache will have nothing but misses until it has data

Pros:
	Only requested data is cached (cache doesn't fill up with unused data)
	Node failures are not fatal (just increased latency to warm the cache)
Cons:
	Cache miss penalty results in 3 round trips, noticeable delay for the request
	Stale data: data in the cache is not up to date with the RDS

 __________                _______________
|          | Cache Hit    |               |
|          |<-----------> |   Amazon      |
|   App    |              | ElastiCache   |
|          | Cache Miss   |               |
|          |--------------->              |
|          | Read from RDS|               |
|          |<------------------------------------> Amazon RDS
|          | Cache Write  |               |
|          |--------------->              |
|__________|              |_______________|
//////////

Python Pseudocode - Lazy Loading
//////////
Exam will require understanding Python Pseudocode
This script does a lazy load, just as described above

def get_user(user_id):
	#Check the cache
	record = cache.get(user_id)

	if record is None:
		# Run a DB query
		record = db.query("select * from users where id = ?", user_id)
		# Populate the cache
		cache.set(user_id, record)
		return record
	else:
		return record
# App code
user = get_user(17)
//////////

Write Through - Add or Update cache when DB is updated
//////////
In this scenario, the app makes a change to the data, and writes to the DB
	As the name implies (Write Through) the app also writes this change to the cache

Pros:
	Data in the cache is never stale, reads are quick
	Write penalty vs Read penalty (Each write requires two calls)
		This is more acceptable as Users will expect a write to take longer
			Ex: (Posting to a blog, Saving an app state, etc...)
Cons:
	Missing Data until the DB has been updated.
		
A middle-point between Lazy Loading and Write Through is to just use both
	This can result in Cache Churn, where there's a lot of unused data


 __________                _______________
|          | Cache Hit    |               |
|          |<-----------> |    Amazon     |
|   App    |              |  ElastiCache  |
|          |              |               |
|          |              |               |
|          | Write to DB  |               |
|          |-------------------------------------> Amazon RDS
|          | Cache Write  |               |
|          |--------------->              |
|__________|              |_______________|
//////////

Python Pseudocode - Write Through
//////////
def save_user(user_id, values):
	#Save to DB
	record = db.query("update users ... where id = ?", user_id, values)

	#Push into cache
	cache.set(user_id, record)
	return record
# App code
user = save_user(17,{"name": "John Doe"})
//////////

Cache Evictions and Time-to-Live (TTL)
//////////
Cache eviction can occur in three ways:
	- App owner deletes item
	- Item is evicted b/c memory is full and item is Least Recently Used (LRU)
	- App owner sets an item time-to-live (TTL)
TTL is useful for many kinds of data:
	- Leaderboards
	- Comments
	- Activity streams
	// Very helpful to remember that this data that will have a period of frequent access
	// And then very infrequent access after that
TTL can range from a few seconds to hours or days

In the case of constant evictions due to full memory, then the cache needs to scale up or out
//////////

Final thoughts on caching
//////////
- Lazy loading is easy to implement, and an easy start to caching
- Write-through is usually combined with Lazy Loading
	The hope is to focus on the data that benefits from this optimization
- TTLs are a good idea, especially when using Write-through
	Should be set to a value based on the data being saved
- Only cache the data that makes sense to cache (User profiles, blogs, etc)
	Make sure to not cache data that shouldn't be (bank info, passwords, ...)
//////////
//////////////////////////////////////////////////////////////////////////

Amazon MemoryDB for Redis - Overview
//////////////////////////////////////////////////////////////////////////

- Redis-compatible, durable, in-memory DB service
- Ultra-fast performance w/ over 160 millions requests/second
- Durable in-memory data storage w/ Multi-AZ transaction log
- Scale seamlessly from 10s of GBs to 100s of TBs
- Use Cases: Web and mobile apps, Online gaming, Media streaming, ....

//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 9
  __         __        
 /  \ |  |  /  \  
 |    |__|  \__|  
 |    |  |     |  
 \__/ |  |.    |

Route 53
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

What is a DNS
//////////////////////////////////////////////////////////////////////////

DNS Overview
//////////
DNS: Domain Name System
	Translates human-readable hostnames into their IP addresses
	Ex: (google.com -> 172.17.18.36)
	DNS is the backbone of how the Internet works
DNS uses a hierarchical naming structure:
	With each word separated by a '.', the name gets more precise

           .com
    example.com
www.example.com
api.example.com
//////////

DNS Terminologies
//////////
- Domain Registrar: Sources that have the registered Domain Names (Route 53, GoDaddy,...)
- DNS Records: Stored records associating a hostname with an IP address (A, AAAA, ...)
- Zone File: Contains DNS records
- Name Server: Resolves DNS queries (Authoritative or Non-Authoritative)
	Authoritative: customer can update the DNS records
- Top Level Domain (TLD): .com, .us, .in, ...
- Second Level Domain (SLD): amazon.com, google.com, ...

		http://api.www.example.com
		    |  |  |   |       |TLD
		    |  |  |   |SLD________
		    |  |  |Sub Domain_____
	    protocol|  |FQDN (Fully Qualified Domain Name)
		|_________________________|
			    URL
//////////

How DNS Works (DNS in action)
//////////

Web Browser is seeking example.com
Here are the steps to find it:

1. Ask local DNS server for example.com
	Local DNS Server doesn't have that

2. Local DNS Server asks Root DNS Server
	Root DNS Server doesn't have that, directs to the .com TLD DNS Server

3. Local DNS Server asks TLD DNS Server
	TLD DNS Server doesn't have that, directs to the example.com SLD DNS Server

3. Local DNS Server asks SLD DNS Server
	SLD DNS Server has that, directs to example.com

4. Local DNS Server fetches example.com

Diagram:

 1. Ask local DNS server for example.com

                                    2. Ask Root DNS Server
                    ________________    example.com?    _______________
Web Browser ------>|Local DNS Server| ---------------->|Root DNS Server| Managed 
                   |                |.com is NS 1.2.3.4|               | by ICANN
                   |                |<-----------------|               |
                   |                |  3.              |_______________|
                   |                |   example.com?    _______________
                   |                | ---------------->|TLD DNS Server | Managed 
                   |                |.com is NS 5.6.7.8|    (.com)     | by IANA
                   |                |<-----------------|               |
                   |                |  4.              |_______________|
                   |                |   example.com?    _______________
                   |                | ---------------->|SLD DNS Server | Managed by 
                   |                |  example.com is  |(example.com)  | Domain Registrar
                   |                |  (9.10.11.12)    |               |
                   |                |<-----------------|               |
                   |                |                  |_______________|
                   |                |
                   |                | 4. Accesses example.com
                   |________________|-----------------> example.com
//////////
//////////////////////////////////////////////////////////////////////////

Amazon Route 53
//////////////////////////////////////////////////////////////////////////
This is AWS's Domain Name Service.
This allows customers to register a Domain Name, and point it to an IP with a site.
This also goes over Records and Hosted Zones, which allow (or deny) Clients finding the site

Overview
//////////
-A highly available, scalable, fully managed and Authoritative DNS
	Authoritative: Customer can update the DNS records

-Route 53 is a Domain Registrar
-Ability to check the health of resources
-The only AWS service which provides 100% availability SLA (Service Level Agreement)
	100% availability SLA: AWS will do everything it can to ensure availability
-(Just fun)Route 53 is a reference to the traditional DNS port
//////////

Route 53 - Records
//////////

- How you want to route traffic for a domain
- Each record contains:
	Domain/Subdomain Name, Ex: (example.com, google.com, ...)
	Record Type, Ex: (A, AAAA, CNAME, ...)
	Value, Ex: (12.34.45.56)
	Routing Policy, this is how Route 53 responds to queries
	TTL: amount of time the record stays cached at DNS Resolvers
Route 53 supports the following DNS record types
	MUST KNOW: A, AAAA, CNAME, NS
	Good to know: CAA, DS, MX, NAPTR, PTR, SOA, TXT, SPF, SRV
//////////

Record Types
//////////
A: maps a hostname to IPv4
AAAA: hostname to IPv6
CNAME: hostname to another hostname
	Target is a domain name which must have an A or AAAA record
	Can't create a CNAME record for the top node of a DNS namespace (Zone Apex)
	Example: can't create for example.com, but can for www.example.com
NS: Name Servers for the Hosted Zone
	Control how traffic is routed for a domain (How???)
//////////

Hosted Zones
//////////
- A container for records that define how to route traffic to a domain and subdomains
- Public Hosted Zones: contains records that specify how to route traffic on Internet
	Ex: (application | .mypublicdomain.com)
- Private Hosted Zones: contain records that specify how to route internal traffic
	This is for use within a VPC, Ex: (application | .company.internal)
- Any hosted zone in AWS will cost $0.50 per month, so no Free Tier
//////////

Hosted Zones - Public vs Private
//////////

Public Hosted Zones are accessible from outside of AWS, and any VPC in use
	These work just like everything covered before, Ex: (publicly accessible EC2s)

Diagram:
External Client requests public Hostname, Public Hosted Zone finds the IP and returns it

         ______    example.com?      ______
	|Client|<-----------------> |Public|     |-----------> S3 Bucket
        |______|  Re: 54.22.33.44   |Hosted|     |
				    |Zone  |---->|-----------> AWS CloudFront
				    |      |     |           ______________
				    |      |     |          | VPC:         |
                                    |      |     |          |              |
				    |      |     |------------>EC2 Instance|
				    |      |     |          |  (Public IP) |
				    |      |     |          |              |
				    |      |     |------------>Application |
				    |      |                |  Load        |
				    |      |                |  Balancer    |
				    |______|                |______________|

Private Hosted Zones are only accessible from within their VPC
	These are used as internal tools, DBs, and apps not for the public
	Instances in the VPC use the Private Hosted Zone to find the IP addresses
		(IP addresses within the VPC)
Does this mean that when we are asking for Resource that we are already doing that through a Hosted Zone???
	(Ex. EC2 Instance to DB) 

Diagram:
Internal resource requests private Hostname, Private Hosted Zone finds the IP and returns it
         ____________________________________________________________________
	|VPC:                                         |       Private     |  |
	|           1. requests api.example.internal  |       Hosted      |  |
	|   EC2 Instance 1 -------------------------> |       Zone        |  |
	|         | 2. Gets IP (10.0.0.10) <--------- |                   |  |
	|         |                                   |                   |  |
	|         |                                   |                   |  |
	|         | 3. Uses IP to get to other EC2    |                   |  |
	|         |                                   |                   |  |
	|         |                                   |                   |  |
	|         |                                   |                   |  |
	|         |     4. EC2_I-2 requests           |                   |  |
	|         |        db.example.internal        |                   |  |
	|      EC2_I-2 -----------------------------> |                   |  |
	| (EC2 Instance 2)                            |                   |  |
	| (api.example.internal) <------------------- |                   |  |
	|         |             5. Gets IP (10.0.0.35)|                   |  |
	|         |                                   |                   |  |
	|         |                                   |                   |  |
	|         |                                   |                   |  |
	|         |                                   |                   |  |
	|      DB Instance                            |___________________|  |
	|  (db.exmple.internal)                                              |
	|____________________________________________________________________|

//////////
//////////////////////////////////////////////////////////////////////////

Registering a Domain Name (Hands On 1/4)
//////////////////////////////////////////////////////////////////////////
Goal: Register a Domain Name through Route 53.

> Route 53

Select: Registered Domain Names on the left nav bar
-> Register domains
	Set a unique domain name that isn't taken
	-> Search
	// Repeat as needed
	-> Proceed to checkout
The page will now show the price and let you set a duration
	Duration: 1 year
	Auto-Renew: On if wanting to keep the Domain Name
	-> Next
Contact Information - This page lets you set your contact info
	AWS does pre-populate this page, but this can be whatever you want
	Under: Privacy Protection
		MUST check Turn on privacy protection (hides personal details)
	-> Next
-> Submit

Let the Domain Name finish initializing

> Route 53 > Hosted Zones > *domain name*

There are now two very important records here, the NS and SOA records
	These records will be the source-of-truth to requests for the Domain Name
//////////////////////////////////////////////////////////////////////////

Creating our First Records (Hands On 2/4)
//////////////////////////////////////////////////////////////////////////
By creating a Record, a Hostname (Domain Name) can be translated into an IP address

> Route 53 > Hosted Zones > *Domain Name*
-> Create record
Under: Quick create record
	Record name: *Something to go in front of Domain Name*
	Record Type: A // Standard IPv4
	Value: IP Address where site is hosted
	-> Create record

This can be seen from an EC2 Instance using the command "nslookup *Domain Name*"
	This command will find info from the Record that was created

The command "dig *Domain Name*" will provide even more info, like the TTL
//////////////////////////////////////////////////////////////////////////

EC2 Setup (Hands On 3/4)
//////////////////////////////////////////////////////////////////////////

First, launch three EC2 Instances, putting each one into a different Region
> EC2 > Instances > Launch an Instance
Standard Setup:
	AWS Linux as OS
	t2.micro machine
	No key-pair
	Create Security Group
		Allow SSH and HTTP from Anywhere
	In Advance Details put in a slightly modified script from Standard:
Script that adds the AZ of the Instance to the page
//////////
#!/bin/bash
# Use this for your user data (script top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
EC2_AVAIL_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)
echo "<h1>Hello World from $(hostname -f) in AZ $EC2_AVAIL_ZONE </h1>" > /var/www/html/index.html
//////////
	These steps are repeated two more times
		Putting the EC2 Instances into different Regions

Launch an ALB in one of the Regions
> EC2 > Load Balancers > Create Application Load Balancer
Set a name
Turn on: Internet Facing
IP Address Type: IPv4
Select: All of the Subnets
Select: just the Security Group made by the EC2 Instance in that Region
	Remove the default Security Group
Create a Target Group
	Target Type: Instances
	Set a name
	-> Next
	-> Include as Pending Below
	-> Create target group
Refresh the list and select the new Target Group
-> Create load balancer
-> View load balancer

Go to each EC2 Instance and confirm that it's working and available publicly
Go to the Load Balancer and confirm that it's working and available publicly
//////////////////////////////////////////////////////////////////////////

TTL (Hands On 4/4)
//////////////////////////////////////////////////////////////////////////
Goal: Create a record with a TTL
	The TTL will be saved by the Client
	The Client will wait until the TTL expires before looking for an update to the IP Address


-> Create a new record
Set the name: "demo" // Attaches to the front of the Domain Name
Record Type: A
Value: Set this as the IP address of the EC2 Instance in the same Region as the ALB
TTL: 2 Minutes
Routing Policy: Simple Routing

The Record now makes the Instance available through the set name
Because the TTL lasts two minutes, the backend IP can be changed, and the client still points to the old IP
Once the two minutes are up, the Client will be directed to the new Instance
//////////////////////////////////////////////////////////////////////////

CNAME vs Alias
//////////////////////////////////////////////////////////////////////////
CNAME:
	Points a hostname to any other hostname (app.mydomain.com => any.anything.com)
	ONLY FOR NON ROOT DOMAIN (Ex: any.mydomain.com)
 can point to something.domain.com
Alias:
	Points a hostname to an AWS Resource (Ex: app.mydomain.com => thing.amazonaws.com)
	Works for ROOT DOMAIN and NON ROOT DOMAIN (Ex: mydomain.com)
	Free of charge
	Native Health Check
///////////////

Alias Records
///////////////
- Maps a hostname to an AWS resource
- An extension to DNS functionality
- Automatically recognizes changes in resource's IP addresses
- Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), Ex: example.com
///////////////
//////////////////////////////////////////////////////////////////////////

Routing Policies
//////////////////////////////////////////////////////////////////////////

Routing Policies can be different types:
 - Simple
 - Weighted
 - Failover
 - Latency
 - Geolocation
 - Multi-value Answer
 - Geoproximity


//////////////////////////////////////////////////////////////////////////

Routing Policies - Simple
//////////////////////////////////////////////////////////////////////////

Typically routes traffic to a single resource, but it can give multiple values
	Of these, one is randomly chosen by the client

Can't be associated with health checks

With Alias: Only specify one resource

Creation:
In Route 53 > Hosted Zones > domain.com
	Create a Record
		Record Type: A - (to IPv4)
		Value: ip addresses
		TTL: 20 seconds
		Routing Policy: Simple

	Going to simple.domain.com will then randomly get one of the IP addresses in Value
	

////////////////////////////////////////////////////////////////////////////


Routing Policies - Weighted
//////////////////////////////////////////////////////////////////////////

Weights don't have to add up to 100 // can they be more than 100???

Can be associated with health checks

DNS Records must have the same name and type //Below ex. Name: Weighted, Type: A - (to IPv4)

Use cases:
	Load balancing
	Testing new version (% of traffic goes to new version to test it)

Weight of 0 in one stops traffic to that resource
Weight of 0 in all is even distribution

Creation:
In Route 53 > Hosted Zones > domain.com
	Create a Record
		Name: Weighted // Very important to keep the same across all weighted records
		Record Type: A - (to IPv4)
		Value: 1st ip address
		TTL: 3 seconds
		Routing Policy: Weighted
		Weight: 10
	Create another Record
		Name: Weighted // Very important to keep the same across all weighted records
		Record Type: A - (to IPv4)
		Value: 2nd ip address
		TTL: 3 seconds
		Routing Policy: Weighted
		Weight: 70

	Going to weighted.domain.com will then randomly get one of the IP addresses based on the weights
	

//////////////////////////////////////////////////////////////////////////


Routing Policies - Latency
//////////////////////////////////////////////////////////////////////////

Redirect to resource in list that has the least latency for client

Very useful when speed is priority

Latency is based on traffic between users and AWS Regions

Can be associated with health checks
	Has a failover capability -so it can reroute to a recovery region if needed


We pick different regions to get good global coverage

Creation:
In Route 53 > Hosted Zones > domain.com
	Create a Record
		Name: Latency
		Record Type: A - (to IPv4)
		Value: 1st ip address
		TTL: 3 seconds
		Region: Asia Pacific
		Routing Policy: Latency
		Weight: 10
	Create another Record
		Name: Latency
		Record Type: A - (to IPv4)
		Value: 2nd ip address
		TTL: 3 seconds
		Region: US East
		Routing Policy: Weighted
		Weight: 70

	Going to latency.domain.com will then get the IP address with the smallest latency
	

//////////////////////////////////////////////////////////////////////////


Health Checks
//////////////////////////////////////////////////////////////////////////

Health Checks can be set up for Automated DNS Failover:
1. monitor an endpoint (app, server, AWS resource,...)
2. monitor another Health Check // We do this to create a Calculated Health Check (see below)
3. monitor a CloudWatch Alarm // Useful for private resources

Health Checkers Monitoring an Endpoint:
	Health Checkers are about 15 all around the world
		Healthy/Unhealthy threshold - 3 (default)
		Interval - 30 sec (10 sec is a higher cost, fast checker)
		Supports HTTP, HTTPS, TCP
		If > 18% of checkers need to say that a resource is healthy, otherwise is unhealthy
		Ability to choose which locations for Route 53 to use
	Health checks pass on a response of 2xx or 3xx from the resource
	Health checks can be setup to pass/fail on response of first 5120 bytes
	Must have router/firewall allow traffic from Route 53 Health Checker IP range


Calculated Health Checks
	Combine the results of multiple Health Checks into a single Health Check
	Can be used to have a parent Health Check monitor child Health Checks
		Can use OR, AND, NOT conditions
	Can have up to 256 child Health Checks
	Specify number of children that need to be healthy for the parent to pass
	Use case: update website without causing all Health Checks to fail

//Health checks are only for public resources, but we can use CloudWatch to get around this
Health Checks - Private Hosted Zones
	Route 53 Health Checkers are outside of the VPC
	They can't access private resources

	We can set a CloudWatch Metric, associate a CloudWatch Alarm, and that alarm is checked by a Health Checker


//////////////////////////////////////////////////////////////////////////


Routing Policies - Geolocation
//////////////////////////////////////////////////////////////////////////

Routing based on location
 
User can specify continent, country, or state

Should have a default record in case of no match

Use cases: Website localization, restrict content distribution, load balancing

Can be associated with health checks

 // We can use a VPN to be able to test if these are working

//////////////////////////////////////////////////////////////////////////


Routing Policies - Geoproximity
//////////////////////////////////////////////////////////////////////////

Routing based on location, but with an added bias value
	The bias value can pull clients from other resources if the bias value is high enough
 
Ex:
////////////////////////////////////////////
There are two resources, and four clients
All of the clients are spaced at even intervals between the resources,
and the resources each have a bias of 0
R0<<<C0<<<C1...C2>>>C4>>>R1

We then change R1 to have a higher bias, which then redirects C1 to also go to R1
r0<<<C0...C1>>>C2>>>C3>>>R1
////////////////////////////////////////////


Traffic Flow
//////////////////////

To set up Geoproximity, we define Traffic Flow rules called Traffic Policies

These policies are edited in the Traffic Policies page
After creating a new traffic policy, we specify that it's for a Geoproximity routing
We then get an actual map that shows where the sources will draw in clients from
So one resource will cover its part of the map in blue, the other in orange,
	and then clients in those areas will be routed to that resource. Cool stuff!

//////////////////////////////////////////////////////////////////////////


Routing Policies - IP-based Routing
//////////////////////////////////////////////////////////////////////////

Routing is of course based on the IP of the client connecting to the resource

This uses CIDR: Classless Inter-Domain Routing (CIDR)

Use cases:
	Optimize performance
	Reduce network costs
Ex:
	Route users from a specific ISP to a specific endpoint
	Users from a 203 IP go to resourceA, while the users from 200 IP got to resourceB

//////////////////////////////////////////////////////////////////////////


Routing Policies - Multi-Value
//////////////////////////////////////////////////////////////////////////

Used for routing to multiple resources

Route 53 returns multiple values/resources

Can be associated with Health Checks // Making them more powerful than Simple

Up to 8 records for each Multi-Value query

Multi-Value is not a substitute for having an ELB

//////////////////////////////////////////////////////////////////////////


Party Domains and Route 53
//////////////////////////////////////////////////////////////////////////
Domain Registrar vs DNS Service:
We can buy our domain name through any service, 
	and then have Amazon Route 53 (or any other DNS Service) manage the DNS records

To do this, we create a public hosted zone in Route 53
	We then update the NS Records on the 3rd party website to use the Route 53 Name Servers

In this way, we have the domain name provider have their name server point to Route 53
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 10
  __              __   
 /  \ |  |  /|   /  \
 |    |__|   |   | /|
 |    |  |   |   |/ |
 \__/ |  |. _|_  \__/

VPC Fundamentals
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////

VPC - Virtual Private Cloud

Needed by:
	AWS Certified Solutions Architect Associate
	AWS Certified SysOps Admin

An AWS Certified Developer should know:
	VPC, Subnets, Internet Gateways & NAT Gateways
	Security Groups, Network ACL (NACL), VPC Flow Logs
	VPC Peering, VPC Endpoints
	Site to Site VPN & Direct Connect


//////////////////////////////////////////////////////////////////////////


VPC and Subnets, a Primer
//////////////////////////////////////////////////////////////////////////

VPC - A private network to deploy resources in a Region
	Has a CIDR Range of IP addresses that are allowed into the VPC

Subnets - Allows for partitioning a network inside of a VPC

Public Subnet - Allows traffic to and from the Internet

Private Subnet - Does NOT allow traffic to and from the Internet
		 This is for security and privacy

Route Tables - Defines access to Internet and between subnets

Internet Gateway - The way EC2 instances in a Public Subnet access the Internet

NAT Gateway - The way that EC2 instances in a Private Subnet access the Internet
		This is done through the NAT Gateway inside of the Public Subnet
			The NAT Gateway then accesses the Internet Gateway
		NAT Instances are self-managed where NAT Gateways are AWS-managed
//////////////////////////////////////////////////////////////////////////


VPC Diagram (See Slide 202)
//////////////////////////////////////////////////////////////////////////
Slide shows AWS Cloud being the outermost layer
	Then the Region contains AZs
	The VPC goes across AZs
		In each AZ is a Public and Private Subnet
//////////////////////////////////////////////////////////////////////////


VPC - NACL(network access control list)
//////////////////////////////////////////////////////////////////////////
Flow of data going into AWS Resources:
Internet ---> VPC ----> NACL ----> Subnet ----> Security Groups ----> ENI/EC2 Instance
	ENI: Elastic Network Interface

NACL - This is a firewall that controls traffic to and from the subnet that is inside of our VPC
	Stateless // Traffic can be allowed in and/or out
	Has Allow and Deny rules
	These are attached to the subnet level
	Rules only operate on IP addresses
	Default NACL allows all traffic in and out

Security Groups // These are the second layer of defense AFTER NACLs
	Stateful // Traffic allowed out can go in
	Firewall that controls traffic to and from ENI/EC2 instances inside of the subnet
	Can only have Allow rules
	Rules affect IP addresses and security groups

VPC Flow Log - Info about the traffic flowing through our VPC, Subnet, and ENI
		ENI: Elastic Network Interface
	Helps to monitor subnets to Internet, subnet to subnet, and Internet to subnet
	Looking over these will help with connectivity problems
	Can be sent to S3, CloudWatch Logs, and Kinesis Data Fire Hose
//////////////////////////////////////////////////////////////////////////


VPC Peering
//////////////////////////////////////////////////////////////////////////

VPC Peering - Connect two VPCs privately
	Same behavior as if they were part of the same network
	Must not have overlapping address ranges
	Each instance only connects two VPCs, even if those VPCs have other peering connections
	So if C<-->A<-->B, C and A, A and B, are connected, C and B are not

	
VPC Endpoints
	All of AWS is publicly accessible, but we can access endpoints privately
	VPC Endpoint Gateways can only connect to S3 and DynamoDB
	VPC Endpoint Interfaces can connect to everything else

There are two types of VPN connections that connect a Site to an AWS VPC
	Site to Site
		Connect over the Internet through an AWS VPN
		Encrypted
	Direct Connect
		Physical connection between the site and AWS (takes at least a month)
		Completely private network


//////////////////////////////////////////////////////////////////////////

VPC Closing Comments and Summary
//////////////////////////////////////////////////////////////////////////

VPC - Virtual Private Cloud
	One for each AWS region used
	We've been using the default one this entire time
Subnets - Tied to AZ, network partition of the VPC
Internet Gateways - at VPC level, provides Internet access
NAT Gateway/Instances - give Internet access to private subnets
NACL - Stateless, subnet level rules for inbound/outbound traffic
Security Groups - Stateful, ENI/EC2 instance level
VPC Peering - Connects two VPCs with non-overlapping IP ranges, non-transitive
VPC Endpoints - Provides a way to get AWS Resources from within a VPC
VPC Flow Logs - Monitoring traffic to and from a VPC
Site to Site VPN - VPN connection over public Internet
Direct Connect - A physical connection from site to AWS

//////////////////////////////////////////////////////////////////////////

Three Tier Architecture
//////////////////////////////////////////////////////////////////////////

The typical three tier architecture follows
	A public-facing ELB via Route 53 / Public Subnet
	A private VPC with an auto-scaling group
		The group has three subnets, 
			The subnets each have an EC2 instance across three different AZs
	A private VPC with a Database and an Elasticache
		The Elasticache will save user sessions and respond to database queries

LAMP stack for EC2:
	Linux - OS for the EC2
	Apache - Web server on Linux
	MySQL - Database
	PHP - App logic

	This stack can add Redis/Memcached(Elasticache) to include caching
	It can also store local app data and software on EBS drive (root)

Wordpress on AWS:
	Send data to an ELB
	Data then goes to EC2 instances
	Data finally goes through ENI to an EFS
	Wordpress has a very helpful diagram that covers Wordpress on AWS

//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 11
  __                 
 /  \ |  |  /|   /|  
 |    |__|   |    |  
 |    |  |   |    |  
 \__/ |  |. _|_  _|_

Amazon S3 Introduction
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

S3 Security
//////////////////////////////////////////////////////////////////////////

User-Based
	IAM Policies - controls which IAM Users can access the S3 Bucket

Resource-Based
	Bucket Policies - bucket-wide policies from S3 console, allows cross-account access
	Object Access Control List (ACL) - finer grain
	Bucket Access Control List (ACL) - less common

So, an IAM principal can access an S3 object if
	The user's IAM permissions allow it OR the resource policy allows it
		AND there is no explicit deny

Encryption - encrypts objects in S3 using encryption keys

S3 Bucket Policies
	JSON based policies
		Resources: buckets and objects
		Effect: Allow/Deny
		Actions: Set of API actions (read, write, ...) to Allow or Deny
		Principal: Account or user to apply policy to
	S3 bucket policies
		Grant public access to the bucket
		Force objects to be encrypted at upload
		Grant access to another account (Cross-Account access)

JSON Ex: (allows anyone to access any object from the examplebucket)
{
	"Version": "2020-10-06",
	"Statement": {
		"Sid": "PublicRead",
		"Effect": "Allow",                     // Allow
		"Principal": "*",                      // anyone
		"Action": [                            //
			"s3:GetObject"                 // to get objects
		],                                     //
		"Resource": [                          //
			"arn:aws:s3:::examplebucket/*" // from anything in examplebucket
		]
	}
}

When granting access
	Users will be given access through IAM Policies
	EC2 instances will be given access through EC2 instance roles
	Cross-Account will be given access through an S3 Bucket Policy

Bucket Settings for Block Public Access
	Settings created to prevent company data leaks
	If the bucket should never be public, leave these on
	If no buckets should ever be public, just set this at the account level

//////////////////////////////////////////////////////////////////////////

S3 - Static Website Hosting
//////////////////////////////////////////////////////////////////////////

S3 can host static websites and give public Internet access to them

Website URL will be
	http://bucketname.s3-website-aws-region.amazonaws.com
	OR
	http://bucketname.s3-website.aws-region.amazonaws.com // has a '.' instead of a '-'

This of course requires that the bucket has public reads
	Will give a 403 error if no public reads
//////////////////////////////////////////////////////////////////////////

S3 - Versioning
//////////////////////////////////////////////////////////////////////////
We can version in S3
	Enabled at the bucket level
Uploading a file takes a key to identify the file
	Uploading a file with that same key will create a new version
It is best practice to version buckets so as to not delete files accidentally

Notes
	Any file uploaded prior to versioning enabled will be version null
	Suspending versioning does not delete previous versions

//////////////////////////////////////////////////////////////////////////

S3 - Replication (CRR && SRR)
//////////////////////////////////////////////////////////////////////////

CRR - Cross-Region Replication
SRR - Same-Region Replication
Must enable Versioning in source and destination buckets
Buckets can be in different AWS accounts
Copying is asynchronous
Must give proper IAM permissions to S3

Use Cases
	CRR - Compliance, lower latency access, replication across accounts
	SRR - Log aggregation, live replication between production and test accounts

Notes:
Enabling Replication only replicates new objects
	Optionally, existing objects can be replicated with S3 Batch Replication

Delete Operations
	Can replicate delete markers from source to target (optional)
	Deletions with a version are not replicated (avoids malicious deletes)

No chaining of replication
	Bucket A is replicated to Bucket B, which does have replication to Bucket C
		Bucket B does not replicate Bucket A items into Bucket C		
//////////////////////////////////////////////////////////////////////////


S3 - Storage Classes Overview
//////////////////////////////////////////////////////////////////////////
S3 Standard - General Purpose
S3 Standard-Infrequent Access (IA)
S3 One Zone-Infrequent Access
S3 Glacier Instant Retrieval
S3 Glacier Flexible Retrieval
S3 Glacier Deep Archive
S3 Intelligent Tiering

Can move between classes manually or using S3 Lifecycle configurations


S3 Durability and Availability
Durability
	High durability (99.999999...) of objects across multiple AZs
	If you store 10,000,000 objects w/ S3, you will, on average, lose one object in 10,000 years
	Same for all storage classes

Availability
	Measures how readily available a service is
	Varies depending on storage class
	Ex: S3 standard has 99.99% availability, means not available 53 mins a year
		Although low, we do have to take this into account for apps

S3 Standard - General Purpose
	99.99% availability
	Used for frequently accessed data
	Low latency and high throughput
	Sustain 2 concurrent facility failures on the side of AWS
	Use cases: Big data analytics, mobile and gaming apps, content distribution....

S3 Storage Classes - Infrequent Access
///////////////////
Data that is rarely accessed, but is rapid when accessed
Lower cost than S3 Standard
	
S3 Standard-Infrequent Access (S3 Standard-IA)
	99.9% availability
	Use cases: Disaster recovery, backups
S3 One Zone-Infrequent Access (S3 One Zone-IA)
	High durability (99.999....) in single AZ, data lost when AZ is destroyed
	99.5% availability
	Use cases: Storing secondary backups of on-premise data, or data that can be recreated
///////////////////

S3 Glacier Storage Classes
///////////////////
Low-cost storage meant for archiving/backup
Pricing: price for storage + object retrieval cost

S3 Glacier Instant Retrieval
	Millisecond retrieval, great for data accessed once a quarter
	Minimum storage length is 90 days
S3 Glacier Flexible Retrieval
	Expedited (1 to 5 mins), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free
	Minimum storage length is 90 days
S3 Glacier Deep Archive - for LONG term storage
	Lowest cost
	Standard (12 hours), Bulk (48 hours)
	Minimum storage length is 180 days
///////////////////

S3 Intelligent-Tiering
///////////////////
Small monthly monitoring and auto-tiering fee
Moves objects automatically Access Tiers based on use
There are no retrieval charges

Frequent Access tier (automatic): default
Infrequent Access tier (automatic): objects not accessed for 30 days
Archive Instant Access tier (automatic): objects not accessed for 90 days
Archive Access tier (optional): configurable from 90 to 700+ days
Deep Archive Access tier (optional): configurable from 180 to 700+ days
///////////////////
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 12
  __              __
 /  \ |  |  /|   /  \
 |    |__|   |      /
 |    |  |   |     /
 \__/ |  |. _|_   /__

AWS CLI, SDK, IAM Roles and Policies
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

EC2 Instance Metadata
//////////////////////////////////////////////////////////////////////////

EC2 Instance Metadata (IMDS) - Powerful, but least known by developers

Allows EC2 instances to "learn about themselves" without using an IAM Role

Instances get their metadata from a url: http://169.254.169.254/latest/meta-data

We can get the IAM Role name for the metadata, but NOT the IAM Policy

Metadata - Data about the instance
Userdata - Launch script of the EC2 instance

IMDSv1 vs IMDSv2
	IMDSv1 - accessed from the url directly (http://169.254.169.254/latest/meta-data)
	IMDSv2 - access is done in two steps:
		Get session token using headers and PUT
		Use session token in IMDSv2 calls, using headers
//////////////////////////////////////////////////////////////////////////

AWS CLI Profiles
//////////////////////////////////////////////////////////////////////////
We can switch between AWS Profiles using the AWS CLI

We just need to set the name
	aws configure --profile <new-profile-name>
Set the Access Key ID
	<access key id string>
Set the Secret Access Key itself
	<secret access key string from actual AWS Account>
Set the region
	<us-west-2 as an example>
And then just hit Enter for the Default output format

From here, we can execute commands from either the default or other accounts
	Default: aws s3 ls
	Other:   aws s3 ls --profile <new-profile-name>
	This is just good to know as a developer
//////////////////////////////////////////////////////////////////////////

Using MFA on the CLI
//////////////////////////////////////////////////////////////////////////
To use MFA on the CLI, we use a temporary session
We run STS GetSessionToken
	aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration-seconds 3600

We then get back a JSON object in the form of:
	Credentials
		SecretAccessKey
		SessionToken
		Expiration
		AccessKeyId

Assign a virtual MFA device in the AWS Console
In the CLI, use the STS get-session-token call, and fill with info from the virtual MFA device
The response is temporary credentials
Create an MFA profile in the CLI
	aws configure --profile mfa
	Fill out all of the fields with data from the credentials, or just hit Enter (default)
	Open the credentials file at aws/credentials
		Create the entry for aws_session_token and paste the token from the MFA profile

Now when the user needs to execute commands that require MFA:
	They will need to provide the MFA token
	This gives them a token for a temporary session used by the CLI
//////////////////////////////////////////////////////////////////////////

AWS SDK Overview
//////////////////////////////////////////////////////////////////////////
Used when coding against AWS services like DynamoDB

Trivia: The AWS CLI uses Python SDK (boto3)
Exam expects takers to know when to use an SDK
We'll get to practice this in using the Lamba Fns

Important: If we don't specify, or configure, a region, it will default to us-east-1
//////////////////////////////////////////////////////////////////////////

AWS Limits
//////////////////////////////////////////////////////////////////////////
API Rate Limits
	DescribeInstances API for EC2 has a limit of 100 calls per second
	GetObject on S3 has a limit of 5500 GET calls per second per prefix
		A prefix is anything in front of a filename, so Imgs/Photos/photo0.jpg has two prefixes
	For Intermittent Errors, implement Exponential Backoff
	For Consistent Errors: request am API throttling limit increase

Service Quotas (Service Limits)
	Running on-demand standard instances: 1152 virtual CPUs
	We can request a service limit increase by opening a ticket
	We can request a service quote increase by using the Service Quotas API


Exponential Backoff
	If getting ThrottlingException intermittently, use exponential backoff
	Retry mechanism already included in AWS SDK API calls
	Must implement yourself if using the AWS API as-is, or in specific cases
		Only use this for 5xx server errors and throttling
		Do not use on 4xx errors, because these are client errors, not throttling errors

How Exponential Backoff works
	1st attempt: try after 1 second
	2nd attempt: try after 2 seconds
	3rd attempt: try after 4 seconds
	4th attempt: try after 8 seconds
	5th attempt: try after 16 seconds
	And so on...

By having all, or many, clients doing this, the server or resource gets less and less hammered by requests
//////////////////////////////////////////////////////////////////////////

AWS Credentials Provider and Chain
//////////////////////////////////////////////////////////////////////////

The AWS Credentials Provider has a chain of priority
	So any conflicts are resolved by the higher priority entry

1. Command Line Options: 
	--region, --output, --profile
2. Environment Variables: 
	AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN
3. CLI credentials file:
	Linux/Mac: ~/.aws/credentials
	Windows: C:\Users\<username>\.aws\credentials
4. CLI configuration file:
	Linux/Mac: ~/.aws/config
	Windows: C:\Users\<username>\.aws\config
5. Container credentials
	For EC2 tasks
6. Instance profile credentials
	For EC2 Instance Profiles


The SDKs also have a priority chain.
Here is the one for Java SDK as an example

1. Java system properties
	aws.accessKeyId, aws.secretKey
2.Environment Variables
	AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
3. Default credentials profiles file
	~/.aws/credentials // shared by many SDKs
4. Amazon ECS container credentials
	For EC2 containers
5. Instance profile credentials
	Used on EC2 instances

Example AWS Credentials Scenario:
	App deployed on an EC2 instance is using environment variables.
	Credentials are from an IAM user to call the Amazon S3 API

	IAM user has S3FullAccess permissions
	
	App only uses one S3 bucket, so best practices:
		IAM role and EC2 Instance Profile created for EC2 instance
		Role was assigned minimum permissions for the S3 bucket

	IAM Instance Profile was assigned to EC2 instance
		Still has access to all S3 buckets. Why?

	The reason is that the credentials given to the IAM user
		These come from the Environment Variables
		We must get rid of these to use the EC2 instance rules

AWS Credentials Best Practices
	NEVER store credentials in code
	Best practice is to use the credentials chain, and inherit rules
	Inside of AWS, use IAM Roles
		And EC2 Instance Roles for EC2 Instances
		ECS Roles for ECS tasks
		Lambda Roles for Lambda functions
	Outside of AWS, use environment variables / named profiles
		So for an outside app/resource, use environment variables/named profiles
//////////////////////////////////////////////////////////////////////////

Signing AWS API Requests
//////////////////////////////////////////////////////////////////////////

Calling the AWS HTTP API signs the request for identification using AWS credentials (access key and secret key)

Some requests don't need to be signed
	Requests for publicly available objects

Using the SDK or CLI signs the HTTP requests automatically

AWS HTTP requests should be signed using Signature v4 (Sigv4)

There are two ways to send the Sigv4
	HTTP Header option (signature in Auth header) // How CLI works
	Query String option, Ex: S3 pre-signed URLS (signature in X-Amz-Signature)

Example of an HTTP request for a file saved inside of an S3 Bucket:
///////////////
https://tempthisismybucket.s3.us-west-2.amazonaws.com/Wallpaper.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&
X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&
X-Amz-Credential=ASIASKY2OJFSKE7XNTG6%2F20241123%2Fus-west-2%2Fs3%2Faws4_request&
X-Amz-Date=20241123T135426Z&
X-Amz-Expires=300&
X-Amz-Security-Token=IQoJb3JpZ2luX2VjED4aCXVzLXdlc3QtMiJGMEQCIElODFZYmbwbzvUxtJuUlVt%2FCpcUIJ4GSfMJFmEay%2FosAiBcz6yRkPpqwnyQuB4lRIzGYF1EjDnYoRLRJE4w2Xg5bSr2AgjX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDE2MDU3OTg2NDkzMiIMurrIyOFOKfBN%2BES4KsoCjozw2nPremL4XPZreqDmZBm8rrh56K3euHCfbtweLVVdOgpGmRnohKvBJqPHdgJ9f6xGHY7t2RAWtYi34Mnqpp88fFNCG%2BSS427kDe0p9CY8F6l7rP2S65Z3pOjMfBBN%2FLm9OVYKLauuwDG44K09n3jHMdDy%2Fl%2FvaQoXF0QeYsizlNjZpmxMUFQRL0yczBiAo4RybDHEj1JW8M%2BztQblX3n6lJv9OrZErJKn7zx5iZbIxu%2BfjGyDArxc3K%2Fw2t0MREi%2ByEJJssW65%2FOMD2%2BMXkzPBwPTGXQacvEl9j6PUkuA15iXEwLCdHyPi2ZVlogemLCwWzpX2Li4yrEUe5WDyMfdfNi%2FZGAxC0rcYwgqBZbut6sTX1vXKY%2FLIooTjeoeUURpx%2FAo3N9N5aGMdOB79p%2FIt%2BaKtvrAHKhx56EadVYF83F59v7WQ%2Fj8MMGjhboGOrQCEvqTXRRuqN7P67qeGBEpz%2B16DpJORfhzFEy35kK38YHsE8yZgdLMyRaXlgRmK9p1FHhYeETn5rjkZB2K0KNtotmv2I9qypDGBoPiva3tsNOqbdlqndnI%2FSFdrtHhZE2OVjqpv%2BNG7Yktjogj7BI6vQ%2BaQLNNeorqiRV1%2FDfy%2FIAyMIkQWwlO72K7MN9AOip8wH0dyWLsHCYogLqXYSHzUYSu%2BqvVpUzBy%2BprSvHs4TMZ81liHevLZhDs%2FyVcKhH9%2Ba9HSBtBnrCCbplRrkCLVlcSjwZPsAiV7P2%2FsD468T9P92vWB9df%2BoR5AvIxzCzvjBLvGErxRL549nVUkagHQ4xEV3obOKVv52WHXGrlUuPfHuQqzPlDiephsiXLNqzY17FYeEB3eO3BwMsXxyOla%2F%2BoVgU%3D&
X-Amz-Signature=0fa43d653a873c40a30c120ffb1cc1eedffaf2749dfd1273d96b3323c67b6e55&
X-Amz-SignedHeaders=host&response-content-disposition=inline
///////////////
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 13
  __              __
 /  \ |  |  /|   /  \
 |    |__|   |     _/
 |    |  |   |      \
 \__/ |  |. _|_  \__/

Slide 255
Advanced Amazon S3
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


S3 Lifecycle Rules
//////////////////////////////////////////////////////////////////////////

Moving Between Storage Classes
//////////////
Can transition objects between storage classes
	Standard, Intelligent Tiering, Infrequent Access, Glacier

For infrequently accessed objects, move them to StandardIA

For archive objects that don't need fast access, move them to Glacier or Glacier Deep Archive

Moving objects can be automated with Lifecycle Rules
//////////////

S3 Lifecycle Rules
//////////////
	Transition Actions - configure objects to transition to another storage class
		Ex: Move objects to Standard IA class after 60 days of creation
		Ex: Move to Glacier for archiving after 6 months

	Expiration Actions - objects expire (delete) after a set time
		Ex: Access log files can be set to delete after 365 days
		Ex: Delete old versions of files (if versioning enabled)
		Ex: Delete incomplete Multi-Part uploads

	Rules can be created for a certain prefix (Ex: s3://mybucket/mp3/*)
	Rules can be created for certain object Tags (Ex: Department: Finance)
		Object Tags are applied by selecting the S3 object and applying a (Key: Value) Tag
//////////////

Ex Scenario 1
//////////////
App on EC2 creates image thumbnails after profile photos are uploaded to S3.
Thumbnails are easily recreated, and only kept for 60 days.
Source images should be immediately accessible for those 60 days.
After 60 days the user can wait up to 6 hours.
How to design this?

The App should have an IAM Role with PUT and GET permissions for that S3 bucket.
App puts photos on Standard.
App puts thumbnails on One-Zone IA after creation. // Not able to do this with Transition rules
Set a Lifecycle Rule, After 60 days: Photos will move to Glacier Flexible Retrieval
	                             Thumbnails get deleted
//////////////


Ex Scenario 2
//////////////
S3 objects can be recovered from deletion immediately for 30 days.
	Rarely happens.
After 30 days, and up to 365 days, deleted objects can be recovered within 48 hours.
How to design this?

Enable versioning, so we can recover objects.
App puts noncurrent versions of objects directly into Standard IA.
Set a Lifecycle Rule: noncurrent versions of objects are moved to Glacier Deep Archive after 30 days.
Set a Lifecycle Rule: delete noncurrent versions of objects after 365 days.

//////////////

S3 Analytics - Storage Class Analysis
//////////////
These are analytics to help decide when to transition objects, and which storage class to go to.

Recommendations for Standard and Standard IA
	Does NOT work for One-Zone IA or Glacier

Report is updated daily
	Start seeing data analysis after 24 to 48 hours

The analytics are a good first step to put together Lifecycle Rules, or improve them.
//////////////
//////////////////////////////////////////////////////////////////////////

S3 Event Notifications
//////////////////////////////////////////////////////////////////////////
We can create rules around object events, like an object being created, deleted, restored, etc
	ObjectCreated, ObjectRemoved, ObjectRestore, etc...

Can filter by name (*.jpg)
	Use case: Generate thumbnails of images loaded to S3 bucket

Create as many S3 Events as needed

S3 event notifications typically deliver events in seconds, but can take a minute or longer

Events can be sent to
	SNS (Simple Notification Service)
	SQS (Simple Queue Service)
	Lambda (Script to perform action)
	Amazon EventBridge (Sends to other resource endpoints)

SNS, SQS, and Lambda must all have a Resource Policy
	SNS: SNS Resource (Access) Policy
	SQS: SQS Resource (Access) Policy
	Lambda: Lambda Resource Policy

S3 Event Notifications w/ Amazon EventBridge
	Advanced filtering options w/ JSON rules
	Multiple Destinations - Step Fns, Kinesis Streams/Firehose, etc...
	EventBridge Capabilities - Archive, Replay Events, Reliable delivery
//////////////////////////////////////////////////////////////////////////

S3 Performance
//////////////////////////////////////////////////////////////////////////
S3 Baseline Performance
//////////////
S3 automatically scales to high request rates, latency 100-200 ms

An App can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in an S3 bucket
	There are no limits to the number of prefixes in a bucket

Per Second Per Prefix
	Ex: (Object Path => prefix)
	bucket/folder/subfolder1/file => /folder/subfolder1/
	bucket/folder/subfolder2/file => /folder/subfolder2/
	bucket/folder1/file => /folder1/
	bucket/folder2/file => /folder2/
	Reads spread evenly across all four prefixes, can achieve 22,000 requests per second for GET and HEAD
//////////////
		
S3 Performance - Uploads
//////////////
Multi-Part Uploads
	Recommended for big files (> 100MB)
	Must use for BIG files (> 5GB)
	Can help parallelize uploads (speed up transfers)

S3 Transfer Acceleration
	Increase transfer speed by moving file to edge location, and then the target S3 bucket
	Compatible with Multi-Part uploads
	Ex: File in USA -> Edge Location in USA -> S3 in Australia
//////////////

S3 Byte-Range Fetches
//////////////
Parallelize GETs by requesting specific byte ranges
Better resilience in case of failures
Can be used to speed up downloads
Can be used to retrieve only partial data (Ex: head of a file)
//////////////
//////////////////////////////////////////////////////////////////////////

S3 Object Tags and Metadata
//////////////////////////////////////////////////////////////////////////
S3 User-Defined Object Metadata
	When uploading an object, you can also assign metadata
	Name-value (key: value) pairs
	User-defined metadata names must begin with "x-amz-meta-"
	Metadata can be retrieved while retrieving the object
	
S3 Object Tags
	Key-value pairs for objects in Amazon S3
	Useful for fine-grained permissions (only access specific objects w/ specific tags)
	Useful for analytics purposes (using S3 Analytics to group tags)

CANNOT search the object metadata or object tags directly
	Instead, use an external DB as a search index such as DynamoDB
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 14
  __
 /  \ |  |  /|   |  |
 |    |__|   |   |__|
 |    |  |   |      |
 \__/ |  |. _|_     |

Slide 268
Amazon S3 Security
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

S3 Security
//////////////////////////////////////////////////////////////////////////

S3 - Object Encryption
//////////////

Objects can be encrypted in S3 buckets using one of 4 methods.
3 of these methods are Server-Side Encryption (SSE):
	Server-Side Encryption (SSE)
		SSE with S3 Managed Keys (SSE-S3) // Enabled by default
			Encrypts S3 objects using keys handled and owned by AWS
		SSE with KMS Keys stored in AWS KMS (SSE-KMS)
			Leverages AWS KMS to manage encryption keys
		SSE with Customer-Provided Keys (SSE-C)
			When you want to manage your own encryption keys
The last is Client-Side Encryption
	Client-Side Encryption - Client uploads already encrypted files
		Encryption Keys stay on-site
		
It is important to know which one to use where, for the exam
//////////////

Server-Side Encryption (SSE)
//////////////
Encryption using keys handled, managed, and owned by AWS
Objects are encrypted server-side
Encryption type is AES-256
Must send header "x-amz-server-side-encryption": "AES256"
Enabled by default for new buckets and objects
                                                ___________________________
User-> File uploaded w/ (HTTP(S) + Header) --> |                          |
                                               |Object                    |
                                               |   +  -> encryption -> S3 |
                                               |  Key                     |
                                               |__________________________|
//////////////

AWS Key Management Service (SSE-KMS)
//////////////
Encryption keys are handled and managed by AWS KMS
KMS Advantages: user control + audit key usage using CloudTrail
Object is encrypted server-side
Must set header "x-amz-server-side-encryption": "aws:kms"
                                                ___________________________
User-> File uploaded w/ (HTTP(S) + Header) --> |                          |
                                               |Object                    |
                                               |   +  -> encryption -> S3 |
                                               |KMS Key                   |
                                               |__________________________|
SSE-KMS Limitation:
	Use may be impacted by KMS limits
When uploaded, calls are made to the GenerateDataKey KMS API

When downloaded, calls are made to the Decrypt KMS API

These count toward the KMS quota per second
	(5500, 10000, 30000, req/s based on region)

A quota increase can be requested with the Service Quotas Console
//////////////

Customer-Provided Keys (SSE-C)
//////////////
Server-Side Encryption using keys that are managed client-side
S3 does NOT store the encryption key, which is provided by the client
HTTPS must be used
Encryption keys must be provided in HTTP headers for every TTP request made
                                                ___________________________
User->File uploaded w/(HTTPS only) Key in Header)->                       |
                                               |Object                    |
                                               |   +  -> encryption -> S3 |
                                               |Client Provided Key       |
                                               |__________________________|
//////////////

Client-Side Encryption
//////////////
Use client libraries such as S3 Client-Side Encryption Library
Clients must encrypt data themselves before sending to Amazon S3
Clients must decrypt data themselves when retrieving from Amazon S3
Customer fully manages keys and encryption cycle                                                                
                                                   ________________________
User->File encrypted w/ Client Key -> Uploaded -> |                       |
                                     w/ HTTP(S)   |                       |
                                                  |                    S3 |
                                                  |                       |
                                                  |_______________________|
//////////////

Encryption in Transit (SSL/TLS)
//////////////
Encryption in flight exposes two endpoints:
	HTTP Endpoint - not encrypted
	HTTPS Endpoint - encryption in flight
	Sites w/ green locks signify they are using encryption in flight

HTTPS is recommended whenever using Amazon Services
HTTPS is mandatory when using SSE-C
Most clients use the HTTPS endpoint by default
//////////////

Force Encryption in Transit (aws: Secure Transport)
//////////////
To force using HTTPS, we can create a Deny bucket policy

...
"Effect": "Deny",
"Principal": "s3:GetObject",
"Resource": "arn:aws:s3:::my-bucket/*",
"Condition": {
	"Bool": {
		"aws:SecueTransport": "false"
	}
}
...
//////////////

About DSSE-KMS
//////////////
A new type of encryption has come out, called DSSE-KMS
	This is just "Double-Encryption based on KMS"
	This shouldn't show up in the exam
//////////////

//////////////////////////////////////////////////////////////////////////

S3 Default Encryption
//////////////////////////////////////////////////////////////////////////
S3 Default Encryption vs Bucket Policies
	All new buckets have SSE-S3 enabled by default
	Optionally, can "force encryption" using a bucket policy
		 Can refuse any API call that tries a PUT without encryption headers (SSE-KMS or SSE-C)

Note that bucket policies are always evaluated before default encryption settings
//////////////////////////////////////////////////////////////////////////

CORS
//////////////////////////////////////////////////////////////////////////
CORS: Cross-Origin Resource Sharing
//////////////
Origin: scheme (protocol) + host (domain) + port
	Ex: http://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
Web Browser based mechanism to allow requests to other origins from main origin
Main Origin: http://ex.com/page
Other Origin: http://other.com/component
Requests will only be fulfilled if Other Origin allows for requests, using CORS Headers
	Ex: Access-Control-Allow-Origin
                                                  ______________
CORS:                         Preflight Request->|  other.com   |
 ex.com        |           |<--Preflight Response|    Web       |
Web Server <-->|Web Browser|                     |   Server     |
 (Origin)      |           |  <-CORS Headers->   |(Other Origin)|
                     (received already by Origin)|______________|
//////////////

Amazon S3 - CORS
//////////////
A client making a cross-origin request on our S3 bucket
	Need to enable correct CORS headers (specific site or *)

POPULAR EXAM QUESTION!!!!

Can call for a specific origin, or * for all origins	

Ex: CORS w/ S3 Buckets
                                     ______________________
CORS:       |--->GET/index.html----->|  mainBucket.html   |
|           |                        |____________________|
|Web Browser|                         ______________________
|           |                         |  imageBucket.html  |
            |->GET/images/coffee.jpg->|____________________|
//////////////
//////////////////////////////////////////////////////////////////////////

S3 - MFA Delete
//////////////////////////////////////////////////////////////////////////
(Multi-Factor Authentication Delete)

MFA - force users to generate a code on a device (usually mobile or USB MFA Device), before doing important operations on S3

MFA will be required to:
	Permanently delete an object version
	Suspend versioning on the bucket

MFA won't be required to:
	Enable versioning
	List deleted versions

To use MFA Delete, Versioning must be enabled on the bucket

Only the bucket owner (root account) can enable/disable MFA Delete
	This is one of the few things the root account will be used for

To enable/disable MFA Delete, it must be done on the AWS CLI, AWS SDK, or the Amazon S3 REST API
//////////////////////////////////////////////////////////////////////////

S3 Access Logs: Warning
//////////////////////////////////////////////////////////////////////////
S3 Access Logs
//////////////
For audit purposes, may want to log all access to S3 buckets

Any request to S3, from any account, authorized or denied, will be logged in a DIFFERENT bucket
	See below for the Logging loop warning

That data can be analyzed using data analysis tools

The target logging bucket must be in the same region
//////////////

WARNING:
	Do not set your logging bucket to be the monitored bucket
	It will create a logging loop, creating an infinite amount of data
		Logging observes bucket
		Logging writes log after observing an access
		Logging accesses bucket and places log inside
		Logging sees its own access to bucket
		Logging writes log after observing the access
		Cycle repeats

//////////////////////////////////////////////////////////////////////////

S3 Logging - Hands On
//////////////////////////////////////////////////////////////////////////
Create Log-Storage Bucket // Logging-storage bucket

Create Activity Bucket // Bucket to have activity logged

Go to Properties of Activity Bucket, and enable Server Access Logging
	Set file storage target as the Log-Storage Bucket
		Permissions to do this are automatically written into the bucket's Bucket Policy

It can take several hours for logs to start being written to the Logging-Storage Bucket
//////////////////////////////////////////////////////////////////////////

S3 - Pre-Signed URLs
//////////////////////////////////////////////////////////////////////////
Overview:
	A User will create a pre-signed URL that allows a non-authorized user to access private S3
		This access is equal to the permissions of the user that pre-signed the URL
	This access is temporary, and allows for GET and/or PUT actions

Generate pre-signed URLs using the S3 console, AWS CLI, or SDK

URL Expiration
	S3 Console - 1 min up to 720 mins (12 hours)
	AWS CLI - configure expiration with -expires-in parameter in seconds
		default is 3600 seconds, max 604,800 seconds (168 hours)

Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET/PUT

Examples:
	Allow only logged-in users to download a premium video from the S3 bucket
	Allow an ever-changing list of users to download files by generating URLs dynamically
	Allow temp access to a user for uploading a file to a specific location in the S3 Bucket
//////////////////////////////////////////////////////////////////////////

S3 - Pre-Signed URLs - Hands On
//////////////////////////////////////////////////////////////////////////
Select object in private bucket

Confirm that it is private by copying the URL and attempting to view it in a new tab

Once the page shows an access error, we know that the object is private

Go to the object in the S3 Console
	Select Create Pre-Signed URL under Actions
	Select the number of minutes that the URL will be active
	Copy the URL and go there in a new tab
		The object should now be accessible
//////////////////////////////////////////////////////////////////////////

S3 - Access Points
//////////////////////////////////////////////////////////////////////////
Overview:
	Create Access Points so users of the system can get specific permissions (READ/WRITE) into S3
	These Access Points have permissions through a policy, and the S3 Bucket's Policy must allow the access from that Access Point

Access Points simplify security management for S3 Buckets
Each Access Point has:
	Its own DNS Name (Internet Origin or VPC Origin)
	An Access Point Policy (similar to a Bucket Policy) - allows for managing security at scale

Users (Finance) --> Policy w/ R/W to /finance in Bucket --> Finance Access Point --> ðŸ“/finance/...

Users (Sales) --> Policy w/ R/W to /sales in Bucket --> Sales Access Point --> ðŸ“/sales/...

Users (Analytics) --> Policy w/ R to entire Bucket --> Analytics Access Point --> ðŸ“/finance/...
                                                                              \__>ðŸ“/sales/...

VPC Origin
/////////////
Can define the Access Point to be accessible only from within VPC
Must create a VPC Endpoint to access the Access Point (Gateway or Interface Endpoint)
VPC Endpoint Policy must allow access to the target bucket and Access Point

VPC:______________________________
|                                |
| EC2 Instance -> VPC Endpoint ----->Access Point (VPC Origin) --> S3 Bucket
|             (w/Endpoint Policy)| (w/ Access Point Policy)      (w/ Bucket Policy)
|________________________________|
/////////////
//////////////////////////////////////////////////////////////////////////

S3 Object Lambda
//////////////////////////////////////////////////////////////////////////
Use AWS Lambda Fns to change an object before sending it to a calling application

Only one S3 Bucket is needed, then create S3 Access Points and S3 Object Lambda Access Points
	These Access Points modify, enrich, and/or redact per customer need

Use Cases :
	Redacting PII for analytics or non-production environments
	Converting between data formats (Ex: XML to JSON)
	Resizing and watermarking images using caller-specific details
                  _____________________________________
                 |AWS Cloud:                           |
E-Commerce App<-------------------------->S3 Bucket    |
                 |                               |     |
		 |                       Supporting    |
                 |                     S3 Access Point |
                 |                               |     |
                 |                               |     |
Analytics App <---S3 Object Lambda<-->Redacting<-|     |
		 |  Access Point      Lambda Fn  |     |
                 |                               |     |
Marketing App <---S3 Object Lambda<-->Enriching<--     |
		 |  Access Point      Lambda Fn        |
                 |                        ^            |
                 |                        |            |
                 |                 Customer Loyalty    |
                 |                    Database         |
                 |_____________________________________|

//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 15
  __              ___
 /  \ |  |  /|   |   
 |    |__|   |   |__
 |    |  |   |      |
 \__/ |  |. _|_  ___|

Slide 288
Amazon CloudFront
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFront Overview
//////////////////////////////////////////////////////////////////////////
AWS CloudFront
///////////////////
Content Delivery Network (CDN)
Uses: Edge Locations for faster delivery of content to clients
Improves users experience for static data retrieval
216 Points of Presence
DDoS Protection (b/c worldwide), integration w/ Shield, AWS Web App Firewall
///////////////////

CloudFront Origins
///////////////////
S3 Bucket, Uses:
	For distributing files and caching them at edge
	Enhanced security w/ CloudFront Origin Access Control (OAC)
		OAC is replacing Origin Access Identity (OAI)
	CloudFront can be used as an ingress (upload files to S3)

Custom Origin (HTTP), Examples:
	An Application Load Balancer
	EC2 Instance
	S3 static website
	Any HTTP backend as desired
///////////////////

CloudFront High-Level
///////////////////
Client requests data from CloudFront Edge Location
CloudFront gets data from S3 Bucket or other website
CloudFront stores data in its local cache
Next client to request same data
CloudFront gets that data from its local cache, and gives that to the client
                                                               ________________
Client --> www.example.com --> CloudFront Edge Location <---> | Origin:        |
	                             (local cache)            |         S3     | 
                                                              |         or     | 
                                                              |        HTTP    | 
                                                              |________________|
///////////////////

CloudFront vs S3 Cross Region Replication
///////////////////
While these can have similar roles, they have different use cases

CloudFront
	Global Edge Network
	Files are cached for a TTL(time to live) (about a day)
	Great for static content that must be available everywhere
	Ex: A single static website that doesn't change often
	
S3 Cross Region Replication
	Must be setup for each region that needs a replication
	Files are updated in real-time
	Read-Only
	Great for dynamic content that needs to be available at low-latency in few regions
	Ex: A region-specific weather service with real-time updates
///////////////////
//////////////////////////////////////////////////////////////////////////

CloudFront - Hands On
//////////////////////////////////////////////////////////////////////////
Go to CloudFront in AWS Console

Create a Distribution
	Choose a resource (S3 Bucket, Load Balancer, etc.)
	Select Origin Access Control Settings
		Create a new OAC
		Copy the policy
		Paste the policy to the resource's Access Policy (Ex: S3's Bucket Policy)
	No need to enable security protections (WAF) for a demo
	Once the Distribution is deployed, we can access the resource
		And further requests pull from the faster cache, rather than the resource
//////////////////////////////////////////////////////////////////////////

CloudFront - Caching and Caching Policies
//////////////////////////////////////////////////////////////////////////
CloudFront Caching
///////////////////
Each CloudFront Edge Location has its own cache

CloudFront identifies each object in the cache using a Cache Key

On a request, CloudFront looks in the cache
	On a miss, it goes to the origin, and saves that data in the cache as well
	On a hit (determined by the cache key) it just returns from the cache
	Data a TTL (time to live), after which it expires

Data in the cache can be marked as invalid before the TTL is up
		Uses the CreateInvalidation API
///////////////////

Cache Key
///////////////////
Unique identifier for every object in the cache

Default consists of hostname + resource portion of URL

Sometimes a more complex approach is needed:
	If an app that serves content varies based on user, device location, etc.
	Can add other elements (HTTP Headers, cookies, etc...) to the Cache Key
		Uses CloudFront Cache Policies
///////////////////

Cache Policy
///////////////////
Cache based on:
	HTTP Headers: None or Whitelist
	Cookies: None, Whitelist, Include All Except, or All
	Query Strings: None, Whitelist, Include All Except, or All

Control the TTL (0 seconds to one year)
	Can be set by the origin using Cache-Control header or Expires header

Create a custom policy or use Predefined Managed Policies

ALL HTTP headers, cookies, and query strings included in the Cache Key are auto-included in (forwarded to) origin requests
But note, the Origin Request Policy will have its own list of required data that is also pulled from the client's request on a cache miss

/////
Ex: HTTP Headers
/////
_______________
Request|       |
--------       |
Get ...        |
User-Agent:... |
Date:...       |
Auth:...       |
Keep-Alive:... |
Language: fr-fr| // This request is for the French language
_______________|

None:
	Don't include any headers in Cache Key
	Headers are not forwarded (except default)
	Best caching performance
Whitelist:
	Only specific headers included in Cache Key
	Specified headers are also forwarded to Origin
/////
Ex: Query Strings
/////
Get /image/cat.jpg?border=red&size=large HTTP/1.1

None:
	Don't include any headers in Cache Key
	Query strings are not forwarded
Whitelist:
	Only specific query strings included in Cache Key
	Only specific query strings are forwarded
Include All-Except:
	Include all query strings in Cache Key except the specified list
	All query strings are forwarded except the specified list
All:
	Include all query strings in Cache Key
	All query strings are forwarded
	Worst caching performance
/////
///////////////////

CloudFront Policies - Origin Request Policy
///////////////////
Specify values that will be included in origin requests
	None of these values in the Cache Key and Origin Request Policy will overlap
		So, no duplicated cache content
Can include:
	HTTP Headers: None, Whitelist, All - viewer headers options
	Cookies: None, Whitelist, All
	Query Strings: None, Whitelist, All

Can add CloudFront HTTP Headers and Custom Headers in origin request
	These do not need to be present in the client's request

Can create custom policy or Predefined Managed Policies for the Origin Request Policy
///////////////////

Cache Policy vs Origin Request Policy
///////////////////
            Request                      Forward
Client -----------------> CloudFront ----------------> Origin (EC2 Instance)
                          (w/ Cache)
         Cache Policy                                   Origin Request Policy (Whitelist)
         - URL                                          - HTTP Headers: User-Agent, Authorization
         - Prefixes                                     - Cookies: session_id
	 - Header: Authorization                        - Query Strings: ref

Request ________        Forward ________
|              |        |              |
Get ...        |        Get ...        |
Host:...       |        Host:...       |
User-Agent:... |        User-Agent:... |
Date:...       |        Auth:...       |
Auth:...       |        Cookie:...     |
Keep-Alive:... |        |______________|
Cookie:...     |
|______________|

In this example, the Request is received and then enhanced before being sent to the Origin
// It seems that the Forwarded Origin Request is composed of the fields from both the Cache Policy and the Origin Request Policy
///////////////////
//////////////////////////////////////////////////////////////////////////

CloudFront - Cache Invalidations
//////////////////////////////////////////////////////////////////////////
When data is cached, it is given a TTL, but we may update the backend data prior to the TTL expiring
	TTL: time to live
When this happens, we can send a command to invalidate a specific file (file.fileType), or the content of an entire folder (folder/*) saved in the cache, or all files (*)
	This of means that the cache will consider any request for that data a cache miss, and make a request to the Origin
//////////////////////////////////////////////////////////////////////////

CloudFront - Cache Behaviors
//////////////////////////////////////////////////////////////////////////
Overview
///////////////////
Configure different setting for a given URL path pattern

Ex:
One specific cache behavior to images/*.jpg files on your origin web server

Route to different kind of origins/origin groups based on the content type or path pattern
	/images/*
	/api/*
	/* (default cache behavior)

When adding additional Cache Behaviors, the Default Cache Behavior is always the last to be processed and is always /* (the default)
	So we check for any modified behavior, and will default to /* otherwise
                                               _____________________
                                              |Origins:             |
                                              |                     |
                         |------> (/api/*)------> App Load Balancer |
Route to Multiple Origins|                    |                     |
                         |------> (/*)----------> S3 Bucket         |
                                              |_____________________|
///////////////////

Ex: Sign-In Page
///////////////////
In this example, we are requiring users to login
Once logged in they are given a signed cookie, and able to access the static website in the S3 bucket
If they do not have the signed cookie, they are redirected to the login page when they try to access any other page

                                           _________________ (returns  ______________________________
                                          |Cache Behaviors: | signed  |Origins:                      |
                             ____________ |                 | cookies)|              (Generate signed|
     |<-Signed Cookies->|<->|CloudFront  |<->(/login)<-----------------> EC2 Instance    cookies)    |
Users|                  |   |Distribution|                  |         |                              |
     |<-authenticate--->|<->|____________|<->(/*)----------------------> S3 Bucket                   |
                                          |_________________|         |______________________________|
///////////////////

CloudFront - Maximize Cache Hits by Separating Static and Dynamic Distributions
///////////////////
In this example, we are requiring users to log in
Once logged in they are given a signed cookie, and able to access the static website in the S3 bucket
If they do not have the signed cookie, they are redirected to the login page when they try to access any other page

                         ______________  Cache based
                        |CDN Layer     | on correct   
                        |CloudFront    | headers and   
                        |              | cookie        Dynamic Content (REST, HTTP server): ALB + EC2              
     |-Dynamic--------->|(Dynamic dist)|-------------->(hub)------------------> EC2 M5 Instance
Users|                  |              |                 
     |-Static Requests->|(Static dist)-|-----(static content)-----------------> S3 Bucket          
                        |______________|
            For the static dist, no headers/session caching rules                
            Required for maximizing cache hits
///////////////////
//////////////////////////////////////////////////////////////////////////

CloudFront - Caching & Caching Invalidations - Hands On
//////////////////////////////////////////////////////////////////////////
Distribution
///////////////////
Go to: CloudFront > Distributions > *Distribution Name* > Edit Behavior

We can't change the Default Behavior (*)

In: CloudFront > Policies > Cache > Create Cache Policy
	Put in a name
	TTL Min, Max, Default
	Set: Headers, Query Strings, Cookies // For demo we don't set any
	Create origin request policy
		Set Name
		

In: CloudFront > Distributions > *Distribution Name* > Create Behavior
	Set the Path
	Set: Origin and origin groups // Origin the behavior applies to (S3 Bucket, EC2 instance, etc)
	Set: Cache Key and Origin Requests

// Most specific is selected first, (/images/*) is more specific than (/*)
///////////////////

Invalidations
///////////////////
Files loaded into the cache will stay there until the TTL expires
	Changing the actual file will not show up because the cached version gets loaded
The files can be invalidated through the online console, which will load the newest version

This is done at CloudFront > Distributions > *Distribution Name* > Invalidations
	Select: Create Invalidation
	For all items, put in "/*"
	After a minute, it completes, and the newest version of the file will be loaded
	
///////////////////
//////////////////////////////////////////////////////////////////////////

CloudFront - ALB or EC2 as an Origin
//////////////////////////////////////////////////////////////////////////
This is the layout of how an EC2 instance or an ALB can be the Origin for a CloudFront Distribution

EC2 Instance:
////////////////////
- Must be public to be accessed
- Accessed through the CloudFront URL

         Edge                  Allow Public
       Location                   IP of           ______________________________________
          _                    Edge Locations    |  Security Group:                     |
Users <->|_|<-------------------------------------------->EC2 Instance (MUST be public) |
                               CloudFront URL    |______________________________________|
////////////////////

Application Load Balancer (ALB):
////////////////////
- In front of an EC2 Instance(s)
	- These instances can be private
- The ALB itself must be public



         Edge        Allow Public   ________________ (Allow Security ______________________________
       Location           IP of    |Security Group: | Group Load    |                              |              
          _          Edge Locations|  ALB           | Balancer)     |  Security Group:             |
Users <->|_|<--------------------->|________________|<-------------->EC2 Instances(can be private) |
                                  (ALB MUST be public)              |______________________________|
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFront - Geo Restriction
//////////////////////////////////////////////////////////////////////////
CloudFront Distributions can restrict based on the location of the user

- These can be:
	No Restriction: Allows all
	Allowlist: Allow access only from those countries on the Allowlist
	Denylist:  Allow access only from those countries NOT on the Denylist

- The "country" is determined using a 3rd party Geo-IP database
- Use case: Copyright Laws to control access to content
- Found under:
	CloudFront > Distributions > *Distribution* > Security
		Under CloudFront geographic restrictions
			Next to Countries, select Edit
//////////////////////////////////////////////////////////////////////////

CloudFront - Signed URL / Cookies
//////////////////////////////////////////////////////////////////////////
The Goal: Distribute paid shared content to premium users all over the world
Solution: Use CloudFront Signed URL / Cookie. Attach policy with:
	Includes URL expiration
	Includes IP ranges to access data from
	Trusted Signers // AWS accounts that can create signed URLs

How long should the URL be valid for?
	Shared content (movie, music, etc.), make it short (a few minutes)
	Private Content (private to the user), this can last years

Signed URL: access to individual files (one signed URL per file)
Signed Cookie: access to multiple files (one signed cookie for many files)

In the below, Signed URL and Signed Cookie can be interchanged,
	but use the correct one where needed (URL for single files, Cookie for multiple)

Signed URL
////////////////////
- Place an Origin Access Control (OAC) between the Edge Locations and the S3 Object
- 1. Client uses an authorization to access the application
- 2. The application will then get the Signed URL from CloudFront
- 3. The application will then send the Signed URL to the Client
- 4. Client uses the Signed URL to access CloudFront and then the S3 Object
                   
                   4.
                   Signed URL   _________________          ___________
            Client<----------->|Amazon CloudFront|  OAC   | Amazon S3 |
1.            | | 2.           |                 <-------->           |
Authentication| | Return       | Edge Location   |        |           |
+ Authorization | Signed URL   |                 |        |           |
              | |              | Edge Location   |        |  Object   |
          Application<-------->|_________________|        |___________|
                    3.Use AWS SDK
		      Generate Signed URL
////////////////////

CloudFront Signed URL vs S3 Pre-Signed URL
////////////////////

- CloudFront Signed URL:
	Allow access to path, not considering the Origin (S3, EC2 instance, etc.)
	Account wide key-pair, only the root can manage it
	Can filter by IP, path, date, expiration
	Can leverage caching features

          Signed URL    Edge Location          Origin 
Client <-------------------> |_| <---------------------> EC2 M5 Instance

- S3 Pre-Signed URL:
	Issue a request as the AWS account who pre-signed the URL
	Uses the IAM key of the signing IAM principle
		This also means the pre-signed URL has all of the permissions of the signing account
	Limited lifetime
	Not using CloudFront

          Pre-Signed URL 
Client <------------------> S3 Bucket
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFront - Signed URL - Key Groups + Hands On
//////////////////////////////////////////////////////////////////////////
Two types of signers:
	1. Trusted Key Group (recommended)
		Can leverage APIs to create and rotate keys, using IAM for API security
	2. An AWS Account containing the CloudFront Key Pair
		Root Account manages keys in AWS console (NOT recommended b/c uses root)

In CloudFront distribution, create one or more trusted key groups

Generate a public key and private key
	Private key is used by app (EC2 instance) to sign URLs
	Public key is uploaded to CloudFront, and then used by CloudFront to verify URLs		
Create Key Group, Hands On - New Way (Recommended)
////////////////////
Go to any online key generator, and generate a private and public 2048 bit key

Go to CloudFront > Public Keys > Create Public Key and paste in the public key

Go to CloudFront > Key Groups > Create Key Group
	Name the Key Group
	Select the Public Key
////////////////////

Create Key Group, Hands On - Old Way (NOT Recommended)
////////////////////
// MUST be logged in as root account
Go to AWS Management Console > My Account (dropdown) > My Security Credentials > CloudFront Key Pairs
	Select Create New Key Pair
	Download the Private Key
	Download the Public Key
	Key Pair is added to CloudFront
	Upload the Private Keys to the EC2 Instances

// I really looked around for where and why the EC2 Instances would have a private key
// It seems like they would use it to grant access to authorized users that just accessed the app they run, but I couldn't find anywhere that said this
// All of the advice seems to say that private keys are only stored in very secure storage
////////////////////

// If my understanding is correct, a signed URL has a "Signature" parameter that has a hash of the public key
// This parameter is then used by CloudFront to compare it to the private key, which is stored on CloudFront as the Key Pair, but is never accessed directly
// We would store the private key in a safe location (personal computer) just to have a backup in case the private key in CloudFront was ever lost
//////////////////////////////////////////////////////////////////////////

CloudFront - Advanced Concepts
//////////////////////////////////////////////////////////////////////////
CloudFront - Pricing
////////////////////
Pricing differs by geographic region, and by amount of data transferred.

Pricing starts at first 10TB, and goes up to over 5PB
////////////////////

CloudFront - Price Classes
////////////////////
The number of Edge Locations can be reduced to save on costs
There are three Price Classes for this:
	1. Price Class All - all regions, best performance
	1. Price Class 200 - most regions, but excludes most expensive
	1. Price Class 100 - only least expensive regions
////////////////////

CloudFront - Multiple Origin
////////////////////
Route to different kinds of Origins based on content type
Or based on path pattern (/images/*, /api/*, /*)

                                _________________          ___________
                               | Cache Behaviors:|        | Origins:  |
 _________________             |                 |        |           |
|Amazon CloudFront|----------------> /api/* -----------------> ALB    |
|                 |            |                 |        |           |
|                 |            |                 |        |           |
|_________________|-----------------> /* -----------------> S3 Bucket |
                               |_________________|        |___________|
////////////////////

CloudFront - Origin Groups
////////////////////
Increase high-availability and do failover
Origin Group: one primary and one secondary Origin
If the primary Origin fails, the second one is used

EC2 Instances:
                                     1.(Request 
                     _________________  Returns _____________________________
                    |Amazon CloudFront| Error  | EC2 Instances Origin Group: |
 _________________  |                 | Code)  |                             |
|Client           |--->               |<--------->Origin A (Primary Origin)  |
|                 | |                 |        |                             |
|                 | |                 |        |                             |
|_________________| |                 |<--------->Origin B                   |
                    |_________________|2.(Retry|_____________________________|
                                          Request)

S3 Buckets:
                                     1.(Request 
                     _________________  Returns ___________________________
                    |Amazon CloudFront| Error  | S3 Origin Group:          |
 _________________  |                 | Code)  |                           |
|Client           |--->               |<--------->Origin A (Primary Origin)|
|                 | |                 |        |      |                    |
|                 | |                 |        |Replication across Regions |
|                 | |                 |        |      |                    |
|_________________| |                 |<---------->Origin B                |
                    |_________________|2.(Retry|___________________________|
                                          Request)
////////////////////

CloudFront - Field Level Encryption
////////////////////
Protect sensitive user info through app stack
Adds an additional layer of security along with HTTPS
Sensitive info encrypted at the edge closest to user
Use asymmetric encryption
Usage:
	Specify set of fields in POST requests to be encrypted (up to 10 fields)
	Specify the public key to encrypt them

In this example, each step of the process is encrypted with in-flight encryption b/c of HTTPS

Ex. Encrypt credit card number of Request

Request:
POST ...
Host ...
cc#  ... // Credit Card Number

                        |-All along this stack, the CC# is encrypted-|

       HTTPS   _________  HTTPS  __________  HTTPS   ________  HTTPS    ___________      
Client------->|  Edge   |------>|Amazon    |------->|Origins:|-------> |Web Servers|
              |Location |       |CloudFront|        |        |         |___________|
              |_________|       |__________|        | ALB    |        Decrypt CC# w/
            Encrypt CC# w/                          |________|         Private Key
             Public Key
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFront - Real Time Logs
//////////////////////////////////////////////////////////////////////////
Get real-time logs sent to Kinesis Data Streams
Content delivery performance used for events or analysis
//////////////////////////////////////////////////////////////////////////


//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 16
  __              ___
 /  \ |  |  /|   |   
 |    |__|   |   |___
 |    |  |   |   |   |
 \__/ |  |. _|_  |___|

Slide 318
Amazon ECS, ECR, and Fargate - Docker in AWS
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

Docker Intro
//////////////////////////////////////////////////////////////////////////

Docker Overview
////////////////////
Docker is a software development platform to deploy apps
Apps are packaged in containers that can be run on any OS
Apps run the same, regardless of where they're run
	Any machine
	No compatibility issues
	Predictable behavior
	Less work
	Easier to maintain and deploy
	Works with any language, OS, and technology
Use cases:
	Microservices architecture
	Lift-and-shift apps from on-premises to AWS cloud
	More....
////////////////////

Docker on an OS
////////////////////
Docker can run on a server (Ex: EC2 Instance)

The individual containers can all be running different things on the same server
	Java app
	NodeJS app
	MySQL database

Where are Docker images stored?
	Stored in Docker Repositories
	Docker Hub (https://hub.docker.com)
		This is a public repository
		Can also find base images for many technologies or OS
	Amazon ECR (Elastic Container Registry)
		Private repository (default)
		Public repository option
			ECR Public Gallery: (https://gallery.ecr.aws)
////////////////////

Docker vs Virtual Machines
////////////////////
Virtual Machines:

Apps run in VMs,
which are handled by a hypervisor,
which runs on a host OS,
which runs on dedicated Infrastructure

The VMs all run individually with a virtual copy of a physical machine.
This makes them large, and require a dedicated amount of memory when set up
 ________   ________   ________
|  Apps  | |  Apps  | |  Apps  | 
|________| |________| |________|
|Guest OS| |Guest OS| |Guest OS| 
|  (VM)  | |  (VM)  | |  (VM)  | 
|________|_|________|_|________|
|          Hypervisor          | //Communicates resource use between host and guest(s)
|______________________________|
|          Infrastructure      | 
|______________________________| 


Docker is "sort of" a virtualization technology, but not exactly
Resources are shared with the host => many containers on one server 
Each container runs a virtual copy of an OS
This makes containers much more like any other application that runs on the computer
	Containers have all of the libraries and dependencies they require to run
Containers can have small file sizes
 _________   _________   _________
|Container| |Container| |Container| 
|_________| |_________| |_________|
|Container| |Container| |Container| 
|_________| |_________| |_________|
|Container| |Container| |Container| 
|_________|_|_________|_|_________|
|          Docker Daemon          | //Communicates resource use for container(s)
|_________________________________|
|     Host OS (EC2 Instance)      | 
|_________________________________|
|          Infrastructure         |
|_________________________________|
////////////////////

Getting Started with Docker
////////////////////

Docker containers start out as a script

Script:
_________________
FROM ubuntu:18.04

COPY . /app
RUN make /app

CMD python3 /app/app.y
_________________


The Script is built into a read-only Docker Image
	These builds can be pushed to a Docker Repository
		Docker Repositories are Docker Hub and Amazon ECR
		Containers can then be pulled from the repository
Docker Images can then be run as an application
////////////////////

Docker Containers Management on AWS
////////////////////
There are four AWS tools that are used to manage Docker Containers:

Amazon Elastic Container Service (ECS)
	Amazon's container platform

Amazon Elastic Kubernetes Service (EKS)
	Amazon's Kubernetes (open source)

AWS Fargate
	Amazon's Serverless container platform
	Works with ECS and EKS

AWS Elastic Container Registry (ECR)
	Store container images
////////////////////
//////////////////////////////////////////////////////////////////////////

Amazon Elastic Container Service (ECS)
//////////////////////////////////////////////////////////////////////////
ECS - EC2 Launch Type
////////////////////
Launch Docker containers on AWS = Launch ECS Tasks on ECS Clusters
EC2 Launch Type: must provision & maintain infrastructure (EC2 instances)
Each EC2 Instance must run the ECS Agent to register in the ECS Cluster
	(The EC2 Instances are all part of an ECS Cluster)
AWS takes care of starting/stopping the containers

 _________________________________
|       Amazon ECS Cluster        | 
|          _                      |
|         |_|New Docker Container |  // Docker Container gets pulled from repository
|       ___|_________             | 
|      |             |            | 
|   ___|_________  __|__________  |
|  |EC2 Instance ||EC2 Instance | |
|  |             || ___________ | | 
|  |             |||Docker     || |  
|  |             |||Container  || |
|  |             |||___________|| |  
|  |  __________ || ___________ | | 
|  | |Docker    ||||Docker     || |  
|  | |Container ||||Container  || |
|  | |__________||||___________|| |  
|  |  __________ || ___________ | | 
|  | |Docker    ||||Docker     || |  
|  | |Container ||||Container  || |
|  | |__________||||___________|| |    
|  |  __________ || ___________ | | 
|  | |Docker    ||||Docker     || |  
|  | |ECS Agent ||||ECS Agent  || |
|  | |          ||||           || | 
|  | |          ||||           || | 
|  | |__________||||___________|| | 
|  |_____________||_____________| | 
|_________________________________|
////////////////////

ECS - Fargate Launch Type
////////////////////
Fargate is a serverless way to execute Container Tasks.
There's no EC2 Instances to create and manage, and it scales with the workload.

Launch Docker containers on AWS
Don't need to provision infrastructure (no EC2 Instances to manage)
All serverless!
Create task definitions
AWS runs ECS Tasks based on CPU/RAM needed
To scale, just increases the number of tasks.
	No more EC2 Instances
          _                      
         |_|New Docker Container   // Docker Container gets pulled from repository
       ___|_________              
      |             |             
   ___|_____________|__________  
  |  AWS Fargate / ECS Cluster |  
  |                ___________ |  
  |               |Docker     ||   
  |               |Container  || 
  |               |___________||   
  |  __________    ___________ |  
  | |Docker    |  |Docker     ||   
  | |Container |  |Container  || 
  | |__________|  |___________||   
  |  __________    ___________ |  
  | |Docker    |  |Docker     ||   
  | |Container |  |Container  || 
  | |__________|  |___________|| 
  |____________________________| 
////////////////////

ECS - IAM Roles for ECS
////////////////////
EC2 Instance Profile (EC2 Launch Type only)
	Used by ECS Agent
	Makes API calls to ECS Service
	Send container logs to CloudWatch Logs
	Pull Docker image from ECR
	Reference sensitive data in Secrets Manager or SSM Parameter Store

ECS Task Role
	Allows each task to have a specific role
	Use different roles for different ECS Services
	Task Role is defined in the Task Definition
                        _____
                    |->| ECS |
                    |  |_____|
   _______________  |   _____
  | EC2 Instance  | |  | ECR |
  |  ___________  | |->|_____| 
  | |Docker     | |-|   _________________ 
  | |ECS Agent  | | |->| CloudWatch Logs |
  | |___________| |    |_________________|
  |  __________   |
  | |Task A    |  |  
  | |        ECS Task A Role ------> S3 Bucket
  | |__________|  |
  |  __________   |
  | |Task B    |  |   
  | |        ECS Task B Role ------> DynamoDB
  | |__________|  | 
  |_______________| 
////////////////////

ECS - Load Balancer Integrations
////////////////////
Application Load Balancer
	Supported for most use cases

Network Load Balancer
	Recommended only for: 
		High throughput/performance cases
		To pair with AWS Private Link 

Classic Load Balancer
	Deprecated, not recommended
 __________________
|Amazon ECS Cluster|
|   _____________  |
|  |EC2 Instance | | 
|  |  __________ | |
|  | |ECS Task  || | 
|  | |          |<-----|
|  | |__________|| |   |
|  |  __________ | |   |
|  | |ECS Task  || |   |
|  | |          |<-----|
|  | |__________|| |   |
|  |_____________| |   |
|   _____________  |   |----------> Application Load Balancer <---> Users
|  |EC2 Instance | |   |
|  |  __________ | |   |
|  | |ECS Task  || |   |
|  | |          |<-----|
|  | |__________|| |   |
|  |  __________ | |   |
|  | |ECS Task  || |   |
|  | |          |<-----|
|  | |__________|| | 
|  |_____________| | 
|__________________|
////////////////////

ECS - Data Volumes, Elastic File System (EFS)
////////////////////
The EFS File System is how we can give the ECS Tasks memory, and to share data between them.
It is important to note that all ECS Tasks in an AZ will have access to the File System.

Mount EFS file Systems onto ECS tasks
Works for both EC2 and Fargate
Tasks running in any AZ will share the same data in the EFS file system
Fargate + EFS = Serverless
Use Cases:
	Persistent multi-AZ shared storage for containers
Note:
	Amazon S3 cannot be mounted as a file system

 __________________
|Amazon ECS Cluster|
|   _____________  |
|  |EC2 Instance | | 
|  |  __________ | |
|  | |ECS Task  || | 
|  | |          |<-----|
|  | |__________|| |   | Mount onto ECS Tasks
|  |  __________ | |   |
|  | |ECS Task  || |   |
|  | |          |<-----|
|  | |__________|| |   |
|  |_____________| |   |             _____________
|   _____________  |   |----------->| Amazon EFS  |
|  |EC2 Instance | |   |            |             |
|  |  __________ | |   |            | File System |
|  | |ECS Task  || |   |            |_____________|
|  | |          |<-----|
|  | |__________|| |   |
|  |  __________ | |   |
|  | |ECS Task  || |   | Mount onto ECS Tasks
|  | |          |<-----|
|  | |__________|| | 
|  |_____________| | 
|__________________|
////////////////////
//////////////////////////////////////////////////////////////////////////

ECS - Hands On Part 1
//////////////////////////////////////////////////////////////////////////
Create the ECS instance:
Amazon Elastic Container Service > Create Cluster
	Give a unique name
	Select both Fargate and EC2 Instances
	Select Create new ASG
	Leave the rest of the defaults
	Create the Cluster
	Takes a minute to finish
	Under EC2, a new ASG will have been created, called Infra-*something*
Under Amazon Elastic Container Service > Clusters > *Cluster Name* > Infrastructure
	The active EC2 instances are shown under Container Instances
	In EC2 > Amazon Auto Scaling Groups, increasing the desired count will create an EC2 Instance
	Under Amazon Elastic Container Service > Clusters > *Cluster Name* > Instance Management
		The EC2 Instance will be seen here
//////////////////////////////////////////////////////////////////////////

ECS - Hands On Part 2
//////////////////////////////////////////////////////////////////////////
The Docker container used is on Docker Hub: nginxdemos/hello
Go to Amazon Elastic Container Service
Create a new Task Definition
	Give it a unique name
	Launch on Fargate to keep it easy, but can launch on an EC2 Instance
	Set CPU: 0.5, and Memory: 1GB // Just to keep it free/cheap
	Set Task Role: None // Important if it uses any AWS resources
	Task Execution Role: *Set automatically by AWS*
	Container name: "nginxdemos-hello", Image URI: "nginxdemos/hello"
		This pulls the nginx demos hello container from Docker Hub
	Container Port: 80, Port Name: "nginxdemos-hello-80"
	Leave the rest as Default
Launch a Cluster
	Go to Amazon Elastic Container Service > Clusters > *Cluster Name* > Services
	Create a new Service
		Set Compute Options to Launch Type
			Launch Type: Fargate, Platform Version: Latest
		Application Type: Service
		Family: *our nginxdemos-hello*, Revision: Latest
		Assign a unique name: "nginxdemos-hello"
		Desired Tasks: 1 // This is how many we want to create right now
		Create a new Security Group
			Security Group Name: "nginxdemos-hello"
			Description: "SG for NGINX"
			Type: HTTP, Port: 80, Source: Anywhere
		Create a Load Balancer
			Name: "DemoAlbForEcs"
			Select the nginxdemo container to load balance
			Create new listener
				Port: 80, Protocol: HTTP
			Create new target group
				Name: "tg-nginxdemos-hello"
				Both Protocols: HTTP, Health Check Path: "/"
			Defaults from here are good
Amazon Elastic Container Service > Clusters > *Cluster Name* > Services
	Under Cluster Overview, status should be active, with 1 registered instance
	In Health, there should be 1 desired task, and is running
	Target group should be linked
		Target Group has link to ALB
			Copy DNS name
			Go to the URL
				The NGINX demo page should come up
Launch some more tasks
	Go to Amazon Elastic Container Service > Clusters > *Cluster Name* > Services
	Select nginxdemos-hello > Update Service
	Set Desired Tasks to 3
	Defaults are good
	Two more tasks are created
	This can be seen under Tasks
	Now refreshing the page with the Nginx demo, the IP address changes
		This is the ALB cycling through our tasks
Finish the demo by setting Desired Tasks back to 0
	This removes all tasks
//////////////////////////////////////////////////////////////////////////

ECS - Auto Scaling
//////////////////////////////////////////////////////////////////////////
Automatically increase/decrease desired number of ECS Tasks
ECS Auto Scaling uses AWS Application Auto Scaling // Triggers for auto-scaling
	ECS Service Average CPU Utilization
	ECS Service Average Memory Utilization - Scale on RAM
	ALB Request Count per Target - Metric coming from ALB

Target Tracking - scale based on target value for a specific CloudWatch metric

Step Scaling - scale based on specific CloudWatch Alarm

Scheduled Scaling - scale based on specified date/time for predictable changes

Must Remember:
ECS Service Auto Scaling (task level) is NOT EC2 Auto Scaling (EC2 Instance level)
	This is why using Fargate is an easier option (b/c Fargate is Serverless)

EC2 Launch Type - Auto Scaling EC2 Instances
////////////////////

Accommodate ECS Service Scaling by adding underlying EC2 Instances

Auto Scaling Group Scaling
	Scale your ASG based on CPU Utilization
	Add EC2 Instances over time

ECS Cluster Capacity Provider
	Used to automatically provision and scale the infrastructure for the ECS Tasks
	Capacity Provider paired w/ an Auto Scaling Group
	Add EC2 Instances when you're missing capacity (CPU, RAM, etc....)
////////////////////

Diagram: ECS Scaling - CPU Usage Ex.
////////////////////

This diagram shows the flow of overloaded Tasks being auto-scaled using CloudWatch Alarms

1. An increase in Customer usage causes the CPU Usage of the Tasks to increase

2. The increase in CPU Usage is seen by the CloudWatch Metric
	This triggers the attached CloudWatch Alarm

3. The CloudWatch Alarm being triggered, sends a signal to the Autoscaler

4. The Autoscaler creates a new Instance of the Tasks
 _____________________________________________________________
| Auto Scaling Group                                          |
| ___________________________________________________________ |
||  Auto Scaling                                             ||
|| _______________________________________________           ||
|||  Service A                                    |          ||
|||                                               |          ||
|||  1. *CPU Usage has an increase* --------------------------------> CloudWatch Metric 
|||                                               |          ||     (ECS ServiceCPU Usage)
|||            ______                             |          ||   2. *CloudWatch alarm triggered*
|||            Task 1                             |          ||                 |
|||            ______                             |          ||                 |
|||            Task 2                             |          ||                 |
|||            ______                             |          ||         CloudWatch Alarm
|||            Task 3 (new)                       |__________||   3. *CloudWatch Alarm send signal to 
|||       4. *New Task created* <----------------- Autoscaler <-------autoscale*
|||                                               |__________||
|||_______________________________________________|          ||
||___________________________________________________________||
|_____________________________________________________________|
////////////////////
//////////////////////////////////////////////////////////////////////////

Amazon ECS - Solutions Architectures
//////////////////////////////////////////////////////////////////////////
Diagram: ECS Tasks Invoked by Event Bridge
////////////////////
In this diagram, it shows the flow of a client uploading files to an S3 Bucket
The S3 Bucket sends Events to Amazon Event Bridge
Amazon Event Bridge is triggered to run an ECS Task // Spins up a Docker container
The Task has an ECS Task Role w/ Permissions to access S3 and DynamoDB
The Task gets the file from S3 Bucket, processes it, and then saves it to DynamoDB
          ___________________________________________________
         | Region                                            |
         |                     _____________________________ |
         |                    | VPC                         ||
         |                    | ___________________________ ||
         |                    ||Amazon ECS Cluster         |||
         |                    || _________________________ |||
         |                    ||| AWS Fargate             ||||
         |                    |||                         ||||
         |                   Get Object       __________  ||||
Client ----> S3 Bucket <---------------------|          | ||||
         |       |            |||            |          | |||| *Save Result*
         |*Event*|            |||            |          |------------------> Amazon DynamoDB
         |       |      Rule: Run ECS Task   |          | ||||
         |     Amazon   -------------------->|Task (New)| |||| Note:
         |   Event Bridge     |||            |__________| |||| Task has an ECS Task Role
         |                    |||_________________________|||| (Access S3 and DynamoDB)
         |                    ||___________________________|||
         |                    |_____________________________||
         |___________________________________________________|
////////////////////

Diagram: ECS Tasks Invoked by Event Bridge Schedule
////////////////////
In this diagram, it shows the flow of an Event firing every hour
Every hour the Amazon Event Bridge runs an ECS Task
The Task has an ECS Role, giving it Permissions to access S3
The Task does some batch processing on the files in S3

                                ________________________
                               |Amazon ECS Cluster      |
                               | ______________________ |
                               || AWS Fargate          ||
                               ||                      ||
    Amazon      Every 1 Hour   ||    __________        ||
Event Bridge ---------------------->|          |       ||
             Rule: Run ECS Task||   |          |       || *Batch Processing*
                               ||   |          |----------------------------> Amazon S3
                               ||   |          |       ||
                               ||   |Task (New)|       || Note:
                               ||   |__________|       || Task has an ECS Task Role
                               ||______________________|| (Access S3)
                               |________________________|
////////////////////

Diagram: ECS Tasks Invoked by Event Bridge Schedule
////////////////////
In this diagram, it shows how Tasks are created based on the workload given
In this Ex, 3 messages come in, so the Auto Scaling creates 3 Tasks to handle them

                                    ________________________
                                   |ECS Service Auto Scaling|
                                   | ______________________ |
                                   || Service A            ||
*3 Messages             *Poll for  ||                      ||
 come in*     Amazon   messages*   ||    ______            ||
----------->Event Bridge ---------->|    Task 1            ||
                                   ||    ______            ||
                                   ||    Task 2            ||
                                   ||    ______            ||
                                   ||    Task 3            ||
                                   ||                      ||
                                   ||______________________||
                                   |________________________|
////////////////////

Diagram: ECS - Intercept Stopped Tasks using Event Bridge
////////////////////
In this diagram, it shows the how a state change in a Task can trigger an Event
An ECS Task stops for some reason
This triggers an Event by Event Bridge
The details are sent to an SNS (Simple Notification Service)
This service then emails the Admin

 ________________________
|ECS Tasks               |
|                        | Event    ____________   Trigger   _____  Email
|   _   _   _ (Exited)   |-------> |Event Bridge| --------> | SNS | ------> Admin
|  |_| |_| |X|           |         |____________|           |_____|
|  (Containers)          |
|________________________|

{
    "source": ["aws.ecs"],
    "detail-type": ["ECS Task State Change"],
    "detail":
    {
        "lastStatus": ["STOPPED"],
        "stoppedReason": ["Essential container in task edited"]
    }
}
////////////////////
//////////////////////////////////////////////////////////////////////////

Amazon ECS Task Definitions - Deep Dive
//////////////////////////////////////////////////////////////////////////
Amazon ECS - Task Definitions
////////////////////
- Task definitions are metadata in JSON form to tell ECS how to run a Docker container
	This can be one or more containers
- It contains crucial info:
	Image Name
	Port Binding for Container and Host (Host is for EC2)
	Mem and CPU required
	Environment variables
	Networking info
	IAM Role // This is attached to the definition
	Logging configuration, Ex: (CloudWatch)
- Can define up to 10 Containers in a Task Definition

Diagram:
This shows how a container inside of an EC2 Instance can have its Networking Info set
so that it can get Internet to it.
This requires that the Container have an open Port, the EC2 (Host) has an open Port,
and the Internet is able to be routed through the Host to the Container

                 Internet
                     |
 ____________________|_______
| E2 Instance   |Port8080|   | // Host Port 8080, does not have to be 8080,
|               |________|   | // it could be 80 if desired
|                    |       | // Container Port is mapped to Host Port
|       _____________|______ |
|      |         |Port80|   || // Port 80 (HTTP) exposes the
|      |         |______|   || // container to the EC2 Instance
|      |                    || 
|      |   Apache HTTP      || // This is how the container can get data
|      |  Server Project    || // from the Internet
|      | (Docker Container) ||
|      |____________________||
|                            |
|       ____________________ |
|      |     ECS Agent      ||
|      |____________________||
|____________________________|
////////////////////

Load Balancing (EC2 Launch Type)
////////////////////
Dynamic Host Port Mapping: Dynamically get Ports on the Host (EC2s here)
	This means that the Containers don't need a specific Host Port

- Get Dynamic Host Port Mapping if defining ONLY the Container Port in the Task Definition (Host port is set to 0, which means NOT set)
- The ALB finds the right Port on the EC2 Instances
- Must allow the EC2 Instance's Security Group ANY Port from the ALB's Security Group

Diagram:
                                      _____________________________________
                                     | ECS Cluster                         |
                           *Dynamic  |  _________________________________  |
                            Host Port| | EC2 Instance                    | |
      Port 80 or 443        Mapping* | |__________     _______  ________ | |
Users<---------------> ALB <-----|---->|Port 36789|<->|Port 80||ECS Task|| |
                                 |   | |__________|   |_______||________|| |
                                 |   | |__________     ________ ________ | |
                                 |---->|Port 36777|<->|Port 80||ECS Task|| |
                                 |   | |__________|   |_______||________|| |
                                 |   | |_________________________________| |
                                 |   |  _________________________________  |
                                 |   | | EC2 Instance                    | |
                                 |   | |__________     _______  ________ | |
                                 |---->|Port 36788|<->|Port 80||ECS Task|| |
                                 |   | |__________|   |_______||________|| |
                                 |   | |__________     _______  ________ | |
                                 |---->|Port 36799|<->|Port 80||ECS Task|| |
                                     | |__________|   |_______||________|| |
                                     | |_________________________________| |
                                     |_____________________________________|
////////////////////

Load Balancing (Fargate)
////////////////////
- Each Task has a unique private IP
- Only define the Container Port (no Host Port to define)
Ex: (ECS ENI Security Group), Allow Port 80 from ALB
    (ALB Security Group), Allow Port 40/443 from web

Diagram:
                                      ____________________________
                                     | ECS Cluster                |
                                     |(ECS ENI Security Group,    |
                                     | Allows Port 80)            |
      Port 80 or 443                 |  __________     ________   |
Users<---------------> ALB <-----|---->|Port 80|__|<->|ECS Task|  |
             (ALB Security Group,|   | |_______|      |________|  |
              Allows Port 80/443)|   | (172.16.4.5)               |
                                 |   |                            |
                                 |   |  __________     ________   |
                                 |---->|Port 80|__|<->|ECS Task|  |
                                 |   | |_______|      |________|  |
                                 |   | (172.16.44.55)             |
                                 |   |                            |
                                 |   |  __________     ________   |
                                 |---->|Port 80|__|<->|ECS Task|  |
                                 |   | |_______|      |________|  |
                                 |   | (172.16.88.54)             |
                                 |   |                            |
                                 |   |  __________     ________   |
                                 |---->|Port 80|__|<->|ECS Task|  |
                                     | |_______|      |________|  |
                                     | (172.16.77.44)             |
                                     |____________________________|
////////////////////

One IAM Role per Task Definition
////////////////////
Diagram:
VERY IMPORTANT - This is an exam question: Where do you define the IAM Role for ECS Task?
	Answer: On your Task Definition

The role is assigned at the Definition level, not the Service level
	So, all of the Tasks are assigned the Role A, and can access S3
A different Definition can be given different access Permissions
	In the case of Role B, they are given access to DynamoDB
                                      _____________
                                     | Service A   |     __________
    (Assigned ECS Task A Role)       |  ________   |    |          |
      (Can access S3 Buckets)        |  ________   |    |          |
         Task Definition A --------->| |  Task  |------>|          |
                                     | |________|  |    | S3 Bucket|
                                     |  ________   |    |          |
                                     | |  Task  |------>|          |
                                     | |________|  |    |          |
                                     |  ________   |    |          |
                                     | |  Task  |------>|          |
                                     | |________|  |    |          |
                                     |  ________   |    |          |
                                     | |  Task  |------>|          |
                                     | |________|  |    |__________|
                                     |_____________|
                                      _____________
                                     | Service B   |     __________
    (Assigned ECS Task B Role)       |  ________   |    |          |
      (Can access DynamoDB)          |  ________   |    |          |
         Task Definition B --------->| |  Task  |------>|          |
                                     | |________|  |    | DynamoDB |
                                     |  ________   |    |          |
                                     | |  Task  |------>|          |
                                     | |________|  |    |          |
                                     |  ________   |    |          |
                                     | |  Task  |------>|          |
                                     | |________|  |    |          |
                                     |  ________   |    |          |
                                     | |  Task  |------>|          |
                                     | |________|  |    |__________|
                                     |_____________|
////////////////////

Environment Variables
////////////////////
- Environment Variable
	There are several places where these can be stored:
	-Hardcoded, Ex: (URLs - fixed, non-secret URL) // Right into the Task Definition
	// There can be Sensitive Variables, Ex: (API Keys, shared configs, DB passwords)
	// Sensitive Variables should be stored securely
	-SSM Parameter Store - sensitive variables
	-Secrets Manager - sensitive variables
		The sensitive vars can be put in SSM Parameter Store OR Secrets Manager
		They are then referenced from the Task Definition
		They are fetched and resolved at runtime, and injected as Environment Variables

// The last option is to load the Environment Variables from an S3 Bucket
// This is called bulk loading
- Environment Files (bulk) - Amazon S3

Diagram:
                                   Fetch Values
Task Definition ---------------->|--------------> SSM Parameter Store
                                 | Fetch Values
                                 |--------------> Secrets Manager
                                 | Fetch Values
                                 |--------------> S3 Bucket
////////////////////

Data Volumes (Bind Mounts)
////////////////////
- Share data between multiple containers in the same Task Definition
- Works for both EC2 and Fargate Tasks
- EC2 Tasks - using EC2 Instance storage
	Data are tied to the lifecycle of the EC2 Instance
- Fargate Tasks - using ephemeral storage
	Data are tied to the Container(s) using them
	20 GiB - 200 GiB (Default is 20 GiB)

Use Cases:
	Share ephemeral data between multiple Containers
 _______________________________________________
| ECS Cluster                                   |
|  ___________________________________________  |
| | ECS Task                                  | |
| |                           Metrics & Logs  | |
| |    App Containers      Container (Sidecar)| |
| |   ______     ______            ______     | | // App Containers:
| |  |      |   |      |          |      |    | | //	Write to Storage
| |  |______|   |______|          |______|    | |
| |     |           |                 |       | | // Metrics & Logs Container:
| |     |           |                 |*Read* | | //	Reads from Storage
| |     | *Writes*  |                 |       | |
| |    _|___________|_________________|___    | |
| |   |                                   |   | |
| |   |    Shared Storage (/var/logs/)    |   | |
| |   |___________________________________|   | |
| |___________________________________________| |
|_______________________________________________|

////////////////////
//////////////////////////////////////////////////////////////////////////

ECS Task Definitions - Hands On
//////////////////////////////////////////////////////////////////////////
Goal:

> Amazon Elastic Container Service > Task Definitions
-> Create new task definition
U: Task definition Family
	Task definition Family: wordpress
U: Infrastructure Requirements
	Checkmark: AWS Fargate
                   Amazon EC2
	Task Role // IAM Role for ECS Tasks, allows using API calls to AWS services
		  // Leave blank
	Task Execution Role // IAM Role for the Container Agent to make API calls
                            // Standard Role for ECS
			    // Use Default
U: Container - 1
	Container Details
		Name: wordpress
		Image URI: wordpress
		Essential Container: Yes // At least one Container has to be this
				// Non-Essential Containers can be stopped w/o stopping the Task
	Private Registry // Can store credentials in Secrets Manager
			 // Secrets Manager stores/manages keys and other access credentials
	Port Mappings // Can define multiple
	Resource Application Limits // Limits how much resources a Container can use
	Environment Variables // Can be custom
		              // Can access secret variables from a source
	Add from File // Can add files from sources like S3
	Use Log Collection // Can create and send logs
	Container Timeouts
		Start Timeouts // Kill if startup goes past this time
		Stop Timeouts // Kill if stopping goes past this time
U: Storage
	Volume - 1 // Set external storage, Ex: (EFS or EBS w/ Bind Mount)
U: Monitoring
	Trace Collection // Routes traces through AWS X-Ray 
			 // (analyze microservices to trace issues and performance)
	Metric Collection // Send metrics to services like CloudWatch
-> Create
	*Review the settings* // Can see the JSON script created for making this Container
	-> Create new definition



//////////////////////////////////////////////////////////////////////////

ECS  - Task Placement
//////////////////////////////////////////////////////////////////////////
These are the placement strategies for balancing/distributing ECS Tasks on EC2 Instances.
These are:
	Binpack - Pack as many Containers into as few EC2 Instances as possible (cost savings)
	Random - Place Tasks randomly into EC2 Instances
	Spread - Spread Containers among EC2 Instances as evenly as possible (high availability)
Requirements on what EC2 Instances qualify can be set:
	CPU
	Memory
	Available Ports
Constraints can be written into the JSON:
	distinctInstance - Each Container is alone on an EC2 Instance
	memberOf - EC2 Instance attribute must be met (Ex: Instance type)

ECS Tasks Placement
////////////////////
- When a Task of type EC2 is launched, the Placement Strategy determines which EC2 takes it
	This is determined by Strategy and Constraints
	Strategies - (Binpack, Random, Spread)
	Constraints - (distinctInstance, memberOf)
	Task Definition Requirements (CPU, Memory, Available Port)
- When a Service scales in (reduces horizontally), the strategy determines which Tasks to stop
- This is only for ECS w/ EC2, not Fargate
////////////////////

ECS Task Placement Process
////////////////////
- Task Placement Strategies are a best effort
	(None of these are perfect, and their effectiveness depends on the situation)
- Amazon uses the following to determine Task Placement:
	- Identify Instances that satisfy requirements in Task Definition (CPU, Memory, Ports) 
	- Identify Instances that satisfy the Task Placement Constraints
	- Identify the Instances that satisfy the Task Placement Strategies
	- Select a qualifying Instance for Task Placement
////////////////////

ECS Task Placement Strategies - Binpack
////////////////////
Binpack:
	- Place Tasks based on the least available amount of CPU or Memory
	- Minimizes the number of Instances in use (cost savings)
	// Basically, this Strategy places as many Tasks on existing EC2 Instances
	// before creating any new EC2 Instances

JSON:
"placementStrategy": {
	{
		"field": "memory",
		"type": "binpack"
	}
}
////////////////////

ECS Task Placement Strategies - Random
////////////////////
Random:
	- Place Tasks randomly

JSON:
"placementStrategy": {
	{
		"type": "random"
	}
}
////////////////////

ECS Task Placement Strategies - Spread
////////////////////
Spread:
	- Place Tasks evenly across all EC2 Instances, based on value
	- Ex: (instanceId, attribute:ecs.availability-zone)
	// Good for high availability

JSON:
"placementStrategy": {
	{
		"field": "attribute:ecs.availability-zone",
		"type": "spread"
	}
}
////////////////////

ECS Task Placement Strategies - Mixed
////////////////////
These Strategies can be mixed by having different Strategies be focused on different attributes

Ex: Spread on attribute:ecs.availability-zone AND instanceId
JSON:
	"placementStrategy": [
		{
			"field": "attribute:ecs.availability-zone",
			"type": "spread"
		},
		{
			"field": "instanceId",
			"type": "spread"
		}
	]

Ex: Spread on attribute:ecs.availability-zone AND instanceId
JSON:
	"placementStrategy": [
		{
			"field": "attribute:ecs.availability-zone",
			"type": "spread"
		},
		{
			"field": "memory",
			"type": "binpack"
		}
	]
////////////////////

ECS Task Placement Constraints
////////////////////
There are two types of Constraints:
	distinctInstance - place each Task on a different Container Instance
	memberOf - places Task on Instances that satisfy an expression
		 - Uses the Cluster Query Language (advanced)

// Here the Constraint of distinctInstance is set
distinctInstance JSON:
	"placementConstraints": [
		{
			"expression": "distinctInstance"
		}
	]

// Here, the Constraint requires being put on any Instance of type t2
memberOf JSON:
	"placementConstraints": [
		{
			"expression": "attribute:ecs.instance-type =~ t2.*",
			"type": "memberOf"
		}
	]
////////////////////
//////////////////////////////////////////////////////////////////////////

ECS - Clean Up - Hands On
//////////////////////////////////////////////////////////////////////////
Important to clean up the created
> ECS > *Cluster Name*

- Stop all Tasks
- Set desired Tasks to 0
- Wait for all Services to be deleted, then delete Cluster
 
//////////////////////////////////////////////////////////////////////////

Amazon ECR
//////////////////////////////////////////////////////////////////////////
ECR: Elastic Container Registry
Store and manage Docker Images on AWS // Like Docker Hub
Private and Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)
Fully integrated w/ECS, backend by Amazon S3
Access is controlled through IAM (permission errors => policy
Supports image vulnerability scanning, versioning, image tags, image lifecycle, ...

Diagram:
Shows how an EC2 Instance can be set to pull Docker images from the ECR repo
	The images are started up in the ECS Cluster
 _______________________________
| ECS Cluster       __________  |
|                  | IAM Role | |
|  ________________|__________| |     ________________
| | EC2 Instance              | |    | ECR Repository |
| |   ______                  | |    |                |
| |  |      | <---|------------------> Docker Image A |
| |  |______|     |           | |    |                |
| |               |           | |    |                |
| |   ______      |           | |    |                |
| |  |      | <---|           | |    |                |
| |  |______|                 | |    |                |
| |                           | |    |                |
| |   ______                  | |    |                |
| |  |      | <----------------------> Docker Image B |
| |  |______|                 | |    |________________|
| |___________________________| |
|_______________________________|
//////////////////////////////////////////////////////////////////////////

ECR - Hands On
//////////////////////////////////////////////////////////////////////////

Using AWS CLI
////////////////////
Login Command
	AWS CLI v2
		aws ecr get-login-password --region *region* | docker login --username AWS --password-stdin *aws_account_id*.dkr.ecr.*region*.amazonaws.com

Docker Commands
	Push
		docker push *aws_account_id*.dkr.ecr.*region*.amazonaws.com/demo:latest
	Pull
		docker pull *aws_account_id*.dkr.ecr.*region*.amazonaws.com/demo:latest
	Troubleshooting:
		In case an EC2 Instance can't pull a Docker Image, check IAM permissions


////////////////////

Hands On
////////////////////
Goal: Pull down the public Docker image and push it to our private repo

> Amazon ECR > Repositories
-> Create Repository
	|_|General Settings
		Repository name: demostephane
	-> Create repository

// Now we have a repo under the Private Repos
-> *demoStephane* repo
// There are no Images

// Now, 
-> View push commands
// Verify that Docker is working on personal computer's CLI:
	docker --version
// Copy-paste the login command from the Amazon Console.
// The actual command will have the items (*item*), filled in
	aws ecr get-login-password --region *region* | docker login --username AWS --password-stdin *aws_account_id*.dkr.ecr.*region*.amazonaws.com

// Pull the Docker Image from the public repo:
	docker pull nginxdemos/hello

// Re-tag the Image to go our private repo:
	docker tag nginxdemos/hello:latest *aws_account_id*.dkr.ecr.*region*.amazonaws.com/demostephan:latest

// Push to our private repo:
	docker push *aws_account_id*.dkr.ecr.*region*.amazonaws.com/demostephan:latest

// We now have this Image in our private repo
////////////////////
//////////////////////////////////////////////////////////////////////////

AWS Copilot - Overview
//////////////////////////////////////////////////////////////////////////
AWS Copilot is a service which can provision all of the standard services needed for an app
This is a pre-written set of instructions that will put in place things like DB, ALB, etc

- CLI Tool to build, release, and operate production-ready containerized apps
- Run apps on AppRunner, ECS, and Fargate
- Helps to focus on building apps, rather than setting up infrastructure
- Provisions all required infrastructure for containerized apps (ECS, VPC, ALB, ECR, ...)
- Automated deployments w/ one command using CodePipeline
- Deploy to multiple environments
- Troubleshooting, logs, health status // These can get setup through Copilot

Diagram:
Shows how using CLI or YAML file can get AWS Copilot to create all of the needed services
This is a super quick way to get all of the standard infrastructure in place 
                                       ___________________________________     _________
 __________________________      _____|_____        ____________________  |   |         |
|Microservices Architecture|--->|AWS Copilot|      |  Well-architected  | |   | ECS     |
|__________________________|    |___________|      |infrastructure setup| |-->|         |
(Use CLI or YAML to describe          |            |____________________| |   | Fargate |
the architecture of apps)             |             ____________________  |   |         |
                                      |            |Deployment Pipeline | |   | App     |
                                      |            |____________________| |   | Runner  |
                                      |             ____________________  |   |_________|
                                      |            |Effective Operations| |
                                      |            |and Troubleshooting | |
                                      |            |____________________| |
                                      |___________________________________|
//////////////////////////////////////////////////////////////////////////

AWS Copilot - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: use the CLI to create an app w/ Copilot
	This app will be a simple load-balanced website
	We will:
		install Copilot, 
		download a pre-made app-creation script, 
		execute the script to create our app,
		delete the app when finished 

// Go to the tutorial page:
	https://aws.github.io/copilot-cli/docs/getting-started/first-app-tutorial/

// Install Copilot (This is for Linux, but adept to OS):
	brew install aws/tap/copilot-cli

// Verify the copilot installation:
	copilot init --help

// Verify the aws and docker installations:
	aws --version
	docker --version

// Clone a repo with a copilot example:
	git clone https://github.com/aws-samples/aws-copilot-sample-service example
	cd example

// Initialize the Copilot:
	copilot init

// Choose the workload type:
	-> Load Balanced Web Site

// Confirm deployment:
	Yes

// In: > CloudFormation > Stacks
	// Here we can see the CloudFormation Stack being created

// Once finished, we are given a URL we can visit
// Visiting the URL, we see the Copilot logo
// Studying the architecture at > CloudFormation > Stacks shows best practices in action

// When finished w/ the demo, delete the app:
	copilot app delete

// Even after deletion, there are some good files to look at
// The first is copilot/environments/test/manifest.yml
// This has some of the settings for the app to be created

// The other is copilot/front-end/manifest.yml
// This has the majority of the settings
//////////////////////////////////////////////////////////////////////////

Amazon EKS
//////////////////////////////////////////////////////////////////////////
// EKS is how AWS utilizes Kubernetes Service

Amazon EKS Overview
////////////////////
- EKS: Elastic Kubernetes Service
- Launch managed Kubernetes clusters on AWS
- Kubernetes is an open-source system for automatic deployment, scaling, and management of containerized (usually Docker) applications
- An alternative to the closed-source ECS, similar goal, different API
- EKS support E2 and Fargate
- Use case: If already using Kubernetes, and want to migrate to AWS
- So, Kubernetes is used on multiple cloud services, so has broad applicability
////////////////////

Diagram
////////////////////
Shows EKS Nodes deployed in multiple AZs
The Nodes are managed by the ASG
The EKS Pods are like ECS Tasks
 _________________________________________________________________________________
|AWS Cloud                                                                        |
|          _____________________                       ______________________     |
|         | Availability Zone 1 |                     | Availability Zone 2  |    |
|  _______|_____________________|_____________________|______________________|___ |
| |VPC    |   _______________   |                     |   _______________    |   ||
| |       |  |Public Subnet 1|  |                     |  |Public Subnet 2|   |   ||
| |       |  |  ___    ___   |  |                     |  |  ___    ___   |   |   ||
| |       |  | |ELB|  |NGW|  |  |                     |  | |ELB|  |NGW|  |   |   ||
| |       |  |_______________|  |                     |  |_______________|   |   ||
| |       |   ________________  |                     |   ________________   |   ||
| |       |  |Private Subnet 1| |                     |  |Private Subnet 2|  |   ||
| |       | _|________________|_|_____________________|__|________________|_ |   ||
| |       || |  EKS Node (EC2)| | Auto Scaling Group  |  |  EKS Node (EC2)| ||   ||
| |       || |   _   _   _    | |                     |  |   _   _   _    | ||   ||
| |       || |  |_| |_| |_|   | |                     |  |  |_| |_| |_|   | ||   ||
| |       || |                | |                     |  |                | ||   ||
| |       || |   EKS Pods     | |                     |  |   EKS Pods     | ||   ||
| |       ||_|________________|_|_____________________|__|________________|_||   ||
| |       |  |________________| |                     |  |________________|  |   ||
| |       |_____________________|                     |______________________|   ||
| |                                EKS Worker Nodes                              ||
| |______________________________________________________________________________||
|_________________________________________________________________________________|

////////////////////

EKS - Node Types
////////////////////
- Managed Node Groups
	- Creates and manages Nodes (EC2 Instances) for us
	- Nodes are part of an ASG managed by EKS
	- Supports On-Demand and Spot Instances

- Self-Managed Nodes
	- Nodes created by us, and registered to the EKS cluster and managed by ASG
	- Can use pre-built AMI - Amazon EKS Optimized AMI, or build one ourselves (complex)
	- Supports On-Demand and Spot Instances

- AWS Fargate
	- Supported, and no Nodes to maintain or manage
////////////////////

Amazon EKS - Data Volumes
////////////////////
- Need to specify StorageClass manifest on EKS Cluster
- Leverages a Container Storage Interface (CSI) compliant driver

- Supports:
	- Amazon EBS
	- Amazon EFS (only type that works w/ Fargate)
	- Amazon FSx for Lustre
	- Amazon FSx for NetApp ONTAP

// Lustre is a parallel distributed file system generally used for Clusters
// ONTAP is NetApp's internal OS specially optimized for storage functions
////////////////////
//////////////////////////////////////////////////////////////////////////

reviewreviewreviewreview
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 17
  __            ___
 /  \ |  |  /|     |   
 |    |__|   |     |
 |    |  |   |     |
 \__/ |  |. _|_    |

AWS Elastic Beanstalk
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

AWS Elastic Beanstalk Overview (High Level)
//////////////////////////////////////////////////////////////////////////
Developer Problems
////////////////////
- Managing infrastructure
- Deploying code
- Configuring all of the DB, Load Balancers, etc
- Scaling concerns

Patterns we can recognize:
- Most web apps have the same architecture
- All devs want is for code to run (ALB + ASG)
- Consistencies across different apps and environments
////////////////////

Elastic Beanstalk - Overview
////////////////////
Elastic Beanstalk is a dev-centric view of deploying apps on AWS
- Uses all components seen before (EC2, ASG, ELB, RDS, ...)
- Managed service:
	- Automatically handles:
		capacity provisioning
		load balancing
		scaling
		app health monitoring
		Instance config
		...
	- Just the app code is the responsibility of the dev
- Still maintain full control of configuration
- Beanstalk itself is free, but the underlying services all have their costs (of course)
////////////////////

Elastic Beanstalk - Components
////////////////////
- App: collection of Elastic Beanstalk components (environments, configs, ...)
- App Version: iteration of app code
- Environment
	- Collection of AWS resources running an app version (only one app version at a time)
	- Tiers: Web Environment Tier, Worker Environment Tier
	- Can create multiple environments (dev, test, prod, ...)

Here is the process of creating, launching, and managing versions

1. Create App
2. Upload Version
3. Launch Env
4. Manage Env

In the maintenance phase, we just keep updating and deploying new Versions
5. Update Version
6. Deploy new Version

  Create App
       |                              update version
       |------> Upload Version <------------------------------|
                      |                                       |
                      |--------> Launch Environment           |
                      |                 |                     |
                      |                 |------------> Manage Environment <-|
                      |_____________________________________________________|
                                         deploy new version
////////////////////

Elastic Beanstalk - Supported Platforms
////////////////////
Many different languages and deployment types are covered:
- Go
- Java SE
- Java w/ Tomcat
- .NET Core on Linux/Window Server
- Node.js
- PHP
- Python
- Ruby
- Packer Builder
- Docker (Single, Multi-Container, Preconfigured)
////////////////////

Web Server Tier vs Worker Tier
////////////////////
In the web Server Tier the Elastic Beanstalk is creating and managing web servers
These are in multiple AZs, w/ an ELB and Auto Scaling Group
 _________________________________________________________________
|           Web Environment                                       |
|  (myapp.us-west-2.elasticbeanstalk.com)                         |
|  ______________________       ___       ______________________  |
| | Availability Zone 1  |     |ELB|     | Availability Zone 2  | |
| |  ____________________|_______|_______|____________________  | |
| | |                   Auto Scaling Group                    | | |
| | |  ________________  |               |  ________________  | | |
| | | |Security Group 1| |               | |Security Group 2| | | |
| | | |     _          | |               | |     _          | | | |
| | | |    |_|         | |               | |    |_|         | | | |
| | | |  EC2 Instance  | |               | |  EC2 Instance  | | | |
| | | |  (web server)  | |               | |  (web server)  | | | |
| | | |________________| |               | |________________| | | |
| | |____________________|_______________|____________________| | |
| |______________________|               |______________________| |
|_________________________________________________________________|


In the Worker Environment an SQS Queue stores requests
These EC2 Instances are Workers, so they pull from the SQS Queue 
	(They pull when they are ready, so no need for ELB)
 _________________________________________________________________
|           Worker Environment                                    |
|                                                                 |
|  ______________________    _________    ______________________  |
| | Availability Zone 1  |  |SQS Queue|  | Availability Zone 2  | |
| |                      |  |_________|  |                      | | 
| |  ____________________|_______|_______|____________________  | |
| | |    Auto Scaling Group      |(Scale based on message count)| |
| | |  ________________  |       |       |  ________________  | | |
| | | |Security Group 1| |       |       | |Security Group 2| | | |
| | | |     _          | |       |       | |     _          | | | |
| | | |    |_|<------------------|------------->|_|         | | | |
| | | |  EC2 Instance  | |   (Pulls SQS  | |  EC2 Instance  | | | |
| | | |    (worker)    | |    messages)  | |    (worker)    | | | |
| | | |________________| |               | |________________| | | |
| | |____________________|_______________|____________________| | |
| |______________________|               |______________________| |
|_________________________________________________________________|

Can use both of these together, pushing messages from the Web Server Tier to the SQS Queue

////////////////////

Elastic Beanstalk Deployment Modes
////////////////////
Single Instance (Great for dev)
 _____________________
| Availability Zone 1 |
|   ---> Elastic IP   |
|    _                |
|   |_|               |
| EC2 Instance        |
|                     |
|   RDS Master        |
|_____________________|


High Availability w/ Load Balancer (Great for production)
  ______________________    _________    ______________________  
 | Availability Zone 1  |  |   ALB   |  | Availability Zone 2  | 
 |                      |  |_________|  |                      |  
 |  ____________________|_______|_______|____________________  | 
 | |    Auto Scaling Group      |                            | | 
 | |  ________________  |       |       |  ________________  | | 
 | | |Security Group 1| |       |       | |Security Group 2| | | 
 | | |     _          | |       |       | |     _          | | | 
 | | |    |_|<------------------|------------->|_|         | | | 
 | | |  EC2 Instance  | |               | |  EC2 Instance  | | | 
 | | |________________| |               | |________________| | | 
 | |                    |               |                    | | 
 | |     RDS Master     |               |     RDS Standby    | |  
 | |____________________|_______________|____________________| | 
 |______________________|               |______________________|

////////////////////
//////////////////////////////////////////////////////////////////////////

Beanstalk First Environment
//////////////////////////////////////////////////////////////////////////
Goal: Create a Web Env (dev)
- Sample app based off of language choice (Node.js)
- Single EC2 Instance
- Create EC2 Instance template and Roles
- CloudFormation creates provisions the AWS services and resources for us
- We can see these pieces are present under their section of the AWS Console
	So EC2 Instances are in the EC2 section
- Going to the public URL, we get an Elastic Beanstalk starter page
//////////////////////////////////////////////////////////////////////////

Beanstalk Second Environment
//////////////////////////////////////////////////////////////////////////
Goal: Create a Web Env (prod)
- Sample app based off of language choice (Node.js)
- High Availability (multiple EC2 Instances)
- Choose the EC2 Templates we made before
- Choose all subnets (all of the AZs in one Region)
- Creating a DB here will link it to the Stack (delete Stack -> auto deletes DB)
	Bu we could make a Snapshot of the DB to preserve it
- Make sure it creates an ALB and ASG (1 - 4 Instances)
- Set the Health Check parameters
- Going to the public URL, we get an Elastic Beanstalk starter page
//////////////////////////////////////////////////////////////////////////

Beanstalk Deployment Modes
//////////////////////////////////////////////////////////////////////////

Elastic Beanstalk Deployment Options for Updates
////////////////////
- All at once (deploy all in one go)
	fastest, but Instances are unavailable meanwhile (downtime)
- Rolling
	Maintain capacity, gradually replace old versions w/ new
- Rolling w/ Additional Batches
	Go over capacity, creating new buckets for the new version, then replace old
- Immutable
	Creates new Instances in new ASG, replaces old ASG when determined to be healthy
- Blue/Green
	Create new Environment, direct some traffic to new, replace old if health checks pass
- Traffic Splitting
	Create temp Environment, direct some traffic to temp, migrate new if health checks pass
////////////////////

Elastic Beanstalk Deployment All-at-Once
////////////////////
Shut down all Instances, and have Clients be unable to access App while new Version is deployed

- Fastest
- App has downtime as all Instances are shut down
- Great for quick iterations in dev environment
- No additional cost from extra Instances being created
////////////////////

Elastic Beanstalk Deployment Rolling
////////////////////
Limit traffic by only shutting down some Instances, replace them w/ new Version
Keep doing this until all Instances have been replaced

- App runs below capacity
- Can set the batch size (what % to have running normally vs being replaced w/ new Version)
- App stays running, and is running both Version while deploying
- No additional cost from extra Instances being created
////////////////////

Elastic Beanstalk Deployment Rolling w/ Additional Batches
////////////////////
Keep all traffic by creating extra Instances, replace some old Instances w/ new Version
Keep doing this until all Instances have been replaced, delete extra Instances

- App runs over capacity, creating extra Instances
- Can set the batch size (how many Instances w/ new Version)
- App stays running, and is running both Version while deploying
- There is additional cost from extra Instances being created
- Good for prod
////////////////////

Elastic Beanstalk Deployment Immutable
////////////////////
Recreate entire ASG w/ Instances that have the new Version
Once all health checks pass, delete the old ASG

- Zero downtime
- New code is deployed to new Instances on a temp ASG
- High cost as it doubles capacity
- Longest deployment
- Quick rollback in case of failures
- Great for prod
////////////////////

Elastic Beanstalk Deployment Blue/Green
////////////////////
Create a new environment w/ the new Version, send some traffic there
Once the new Version has been seen as valid, swap the URLs and delete the old environment

- Not a direct feature of Elastic Beanstalk (but is something we can do manually)
- Zero Downtime
- New environment can be validated by Clients and rolled back if thee are issues
- Route 53 can use weighted policies to redirect traffic to the stage environment
- Use Beanstalk to swap the URLs once validated
////////////////////

Elastic Beanstalk - Traffic Splitting
////////////////////
Create a temp environment w/ the new Version, send some traffic there
Once the new Version has been seen as valid, migrate the new Instances from temp to the Main ASG
Delete the temp ASG
Delete the Instances w/ the old Version
This process is automated:
	We give it a time to monitor
	It is monitored for that amount of time
	Migration automatically happens after time is up (and all checks passed)

- Canary Testing: Having Clients do our testing and validating for us
- New App Version is deployed to a temp ASG w/ same capacity
- Small % of traffic is sent to temp ASG
- Deployment health is monitored
- Very quick automated rollback on failure
- No downtime
- New Instances are migrated to Main ASG
- Old App Versions are deleted
////////////////////

Elastic Beanstalk Deployment Summary from AWS Doc
////////////////////
Amazon has docs that cover all of these

https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html
////////////////////
//////////////////////////////////////////////////////////////////////////

Beanstalk Deployment Modes Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Deploy App updates
> Elastic Beanstalk > Environments > *prod environment name* > configuration
- Application deployments, where we can choose one of the Deployment Modes
- Setting this just sets the Deployment Mode used when we do updates
- Make some change to the current version
	We can get the default Node.js build from searching for:
		"sample application node.js beanstalk"
		Make the Welcome Page blue, rather than green
	We can also make changes to the .ebextensions if desired
- Zip the changed project


> Elastic Beanstalk > Environments > *prod environment name*
-> Upload and Deploy
- Once we select the zipped project, we hit deploy, and that's it!
	The deployment is happening
- Going to the prod's public URL, it is now blue

We can also swap the environment domains of dev and prod
> Elastic Beanstalk > Environments > *prod environment name*
	This is done through the Actions dropdown
		Select the dev environment to swap them
	Their URLs are now swapped
		The prod's URL is now green, and the dev's URL is now blue
//////////////////////////////////////////////////////////////////////////


Beanstalk CLI and Deployment Process
//////////////////////////////////////////////////////////////////////////
Elastic Beanstalk CLI
////////////////////
- Elastic Beanstalk has its own CLI which can make working w/ it much easier
- Lots of commands (see slide)
- Helps w/ automated deployment pipelines
////////////////////

Elastic Beanstalk Deployment Process
////////////////////
Process is as follows: 
	Describe dependencies, package code, upload -> deploy
	Details depends on language (Python, Node.js, etc)
Elastic Beanstalk handles all of: 
	deploying to Instances, resolving dependencies, and starting app
////////////////////

//////////////////////////////////////////////////////////////////////////

Beanstalk Lifecycle Policy Overview + Hands On
//////////////////////////////////////////////////////////////////////////

Beanstalk Lifecycle Policy
////////////////////

- Limited to 1000 app versions, so we can use a Lifecycle Policy to delete them.
- The Lifecycle Policy will delete unused app versions based on age or space
	Can opt to save source bundle in S3
////////////////////

Hands On
////////////////////
Elastic Beanstalk > Applications > *App Name* > Applications versions
-> Settings
	Here we can turn on the Lifecycle Policy and change the settings
		Delete by age or count
		Save source bundle or not 

////////////////////
//////////////////////////////////////////////////////////////////////////

Elastic Beanstalk Extensions
//////////////////////////////////////////////////////////////////////////

Elastic Beanstalk Extensions
////////////////////
Can create/edit the .config files in .ebextensions/ to control the app extensions
	Files must be in YAML/JSON format
	Can modify default settings and/or add AWS resources
	Everything managed by .ebextensions gets deleted when the Environment does
////////////////////

Hands On
////////////////////
Shows how by opening the environment-variables.config file and editing variables here is reflected in the AWS Console after deploying the new version.

To deploy new version:
> Elastic Beanstalk > Environments > *dev app name*
-> Upload and deploy

To see Environment Variables:
> Elastic Beanstalk > Environments > *dev app name*
-> Configuration tab
Scroll down to Environment properties
////////////////////
//////////////////////////////////////////////////////////////////////////

Elastic Beanstalk and CloudFormation
//////////////////////////////////////////////////////////////////////////

Elastic Beanstalk Under the Hood
////////////////////
- Uses CloudFormation
- CloudFormation provisions the AWS services needed (from the .ebextensions)
	docs: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html	
////////////////////

Looking Under the Hood
////////////////////
> CloudFormation > Stacks
Each stack has a Template tab that has the raw commands to provision the AWS services
	This is not very human-readable, it is for the "compiler"

// Each stack has a list of services created for it, seen under the Resources tab
// The prod stack is has more features as it will actually be deployed

> CloudFormation > Stacks > *dev stack name*
	ASG, ASG Launch Config, Elastic IP, EC2 Security Group, ...

> CloudFormation > Stacks > *prod stack name*
	ASG, ASG Launch Config, Scaling Policies, CloudWatch Alarms, 
	EC2 Security Groups, ELB, ELB Listener, ...

By editing the Template, we can have anything we want created
////////////////////
//////////////////////////////////////////////////////////////////////////

Beanstalk Cloning
//////////////////////////////////////////////////////////////////////////

Elastic Beanstalk Cloning
////////////////////
- Copy-Paste an Environment configuration
- Good for creating a test Environment
- All resources are copied, but DBs are only copied in structure (no data)
////////////////////

Hands On
////////////////////
Goal: Clone an Environment
> CloudFormation > Stacks > *dev stack name*

-> Action dropdown > Clone
Very limited options, but we are free to edit the clone after its creation
////////////////////
//////////////////////////////////////////////////////////////////////////

Beanstalk Migrations
//////////////////////////////////////////////////////////////////////////

Elastic Beanstalk Migration: Load Balancer
////////////////////
Changing the Load Balancer type (ALB/NLB) requires a rework of the Environment.
To do this, the environment must be copied, and then the new Environment created, 
	connecting to the new Load Balancer
////////////////////

RDS with Elastic Beanstalk
////////////////////
RDS tied to Elastic Beanstalk is fine for dev, but not prod
The correct way is to have an external RDS and attach it to the Environment
	This is done through Environment Variables
////////////////////

Elastic Beanstalk Migration: Decouple RDS
////////////////////
Goal decouple the RDS from the Elastic Beanstalk App so it doesn't get lost
Steps:
- Create snapshot to ensure we don't lose the RDS
- Create a new EB Environment
- Connect to the external RDS
- Swap the Environments using Blue/Green
- Delete the old Environment
////////////////////
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 18
  __             ___
 /  \ |  |  /|  /   \   
 |    |__|   |  \___/
 |    |  |   |  /   \
 \__/ |  |. _|_ \___/

AWS CloudFormation
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

AWS CloudFormation - Overview
//////////////////////////////////////////////////////////////////////////

AWS CloudFormation
////////////////////
CloudFormation is a way to write out the structure of a full-stack app
Allows the dev to provision all resources (EC2, ALB, ...) in the correct order
////////////////////

CloudFormation - Template Example
////////////////////
After writing the CloudFormation code, it can be visualized w/ Infrastructure Composer
////////////////////

Benefits of AWS CloudFormation (1/2)
////////////////////
No manually created resources, 
	can version control the code, 
	Infrastructure changes are reviewed through the code
Costs are easy to estimate as resources are price-tagged and totaled
////////////////////

Benefits of AWS CloudFormation (2/2)
////////////////////
Easy to create, see, and delete entire infrastructure
Work off of established projects and online templates
////////////////////

How CloudFormation Works
////////////////////
CloudFormation pulls a Version of a code script from S3, and creates the full app.
	New Versions are always new updates that have to be uploaded
On deletion, it deletes all resources that it created.
////////////////////

Deploying CloudFormation Templates
////////////////////
There are two ways to edit CloudFormation Templates: Manual and Automated.
Manual is just writing it out by hand in a code editor,
	or selecting parameters using Infrastructure Composer
Automated is editing a YAML file, which allows for an automated pipeline,
	and can be used in a CICD pipeline
////////////////////

CloudFormation - Building Blocks
////////////////////
There are lots of components that go into a CloudFormation Template (see slide)
	Most important are AWSTemplateFormatVersion and Resources
There are also Template's Helpers for References and Functions
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Create Stack - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Practice creating Stacks using CloudFormation

Using simple Template, we can edit in YAML or JSON
	And view it in Infrastructure Composer

Using Create Stack, and a very simple YAML file
	Example file has just one Resource, an EC2 Instance
		With a few parameters filled in
Just this is enough to deploy this Stack
//////////////////////////////////////////////////////////////////////////


CloudFormation - Update and Delete Stack - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Practice Updating and Deleting Stacks using CloudFormation

Update:
An Update can only be done by replacing the current Template w/ a new one
	(Upload a new Version)

If the new Version has a call for something like a Description: 
	We get the prompt to fill that in at Creation/Update
Before finalizing an Update, we get a Change Set Preview
	Gives a list of changes from the old Version
	CloudFormation auto-decides if a Resource has changed enough to warrant Replacement or just an Edit

Once Update is confirmed, looking at the Resources during Update
	Resources can be seen deleting/creating

Delete:
Deleting the Stack deletes all Resources of the Stack
These are even deleted in the correct order
//////////////////////////////////////////////////////////////////////////

YML Crash Course
//////////////////////////////////////////////////////////////////////////
Teacher prefers YAML for its string interpolation (rather than JSON)
Main Points:
	Key-Value pairs
	Nested objects
	Supported Arrays
	Multi-line strings
	// Can support comments!

Ex. (Simple list, looks a lot like JSON) // Would need to research differences
Resources:
	- Type: Thing1
	- Type: Thing2
//////////////////////////////////////////////////////////////////////////

CloudFormation - Resources
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Resources are the AWS Resources that a CloudFormation script can allocate and configure.
This is full configuration, including referencing other Resources in the script.
Many (over 700), Resource types, and are of the form:
	service-provider::service-name::data-type-name
////////////////////

Resource Reference Docs
////////////////////
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html

This is a doc with all of the Resource types that can be allocated.
	And the rest can be allocated with custom types!
Each entry has its important features, such as:
	Default // value
	Required
	Type // String, int, etc
	Update requires // If update requires replacement, stopping the Resource, etc.
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Parameters
//////////////////////////////////////////////////////////////////////////

CloudFormation - Parameters, Overview
////////////////////
Parameters are a way to put Resources into a script like variables.
This means that we can define the Resource separately with a name, and use that name in the script.
	Teacher does call these scripts a "template"
Doing this means that these Parameters can be used throughout the script, and variable.
	When the Parameter is defined, its constraints are also defined.
	So we can constrain a Parameter to be an EC2 Instance, with a limited number of sizes.
		Ex: (t2 micro, t3 micro, etc)
////////////////////

When to Use a Parameter
////////////////////
Ex:
Parameters:
	SecurityGroupDescription:
		Description: Security Group Description
		Type: String

When a Resource is likely going to change in future uses of a script.
////////////////////

CloudFormation - Parameter Settings
////////////////////
Parameters have a number of different settings that can be applied:
	Type // Which itself has a large number of settings (see slide)
	Description
	ConstraintDescription
	MORE...
////////////////////

CloudFormation - Parameters Example
////////////////////
// A Parameter for a variable EC2 Instance
// Has constrained options and a Default
Parameters:
	InstanceType:
		Description: Choose an EC2 Instance type
		Type: String
		AllowedValues:
			- t2.micro
			- t2.small
			- t2.medium
		Default: t2.micro

// Here the Parameter is used in the script
Resources:
	MyEC2Instance:
		Type: AWS::EC2Instance
		Properties:
			InstanceType: !Ref InstanceType // The !Ref lets us use the Parameter here
			ImageId: ami-0c09df90sdf
////////////////////

How to Reference a Parameter
////////////////////
We use the Fn::Ref function
But, as seen above, the !Ref (YAML shortcut) lets us use a Parameter in a script

In the script, we don't have to put all Parameters under the Parameters section.
We can name a Resource, filling in all of the settings, and then reference it in other places
////////////////////

CloudFormation - Pseudo Parameters
////////////////////
Pseudo Parameters are Parameters that are variables already set up in AWS
These are things like
	AWS::Region
	AWS::StackName
	etc
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Mappings
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Mappings are fixed variables within a CloudFormation template.
Can be used to differentiate between different environments like dev and prod.
All of the values are hardcoded within the template.

Mappings are variables selected from defined values like Region, AZ, AWS Account, etc
	Ex: Selecting between EC2 AMIs based on Region
////////////////////

Accessing Mapping Values (Fn::FindInMap)
////////////////////
The function Fn::FindInMap is used to find a value in the mapping
!FindInMap [MapName, TopLevelKey, SecondLevelKey]
So if the Mapping is:
Mappings:
	RegionMap:
		us-east-1:
			HVMG2: ami-easteast
		us-west-1:
			HVMG2: ami-westwest
// The mapping can be accessed by AWS::Region and HVMG2
// Works great for AMIs b/c they are region-specific
Resources:
	MyEC2Instance:
		Type:AWS::EC2::Instance
		Properties:
			ImageId: !FindInMap [RegionMap, !Ref "AWS::Region", HVMG2]
                              // !FindInMap [MapName, TopLevelKey, SecondLevelKey]
////////////////////

When to use Mappings vs Parameters
////////////////////
Mappings are preferable when the variables will only change based on things that can be deduced later.
These are things like:
	Region
	AZ
	AWS Account
	Environment
	...

Parameters are used when the variables change based on the specific needs of a User
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Outputs & Exports
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
A Stack can output a Resource from one Template (script) and import it into another Template
	Can be read up by the console
	Or another Stack
		This does require Exporting from one Stack, and Importing by another Stack

In the script, this is done by setting the Export: field
Ex:
	Export:
		Name: SSHSecurityGroup	
////////////////////

CloudFormation - Outputs Cross-Stack Reference
////////////////////
To have the Cross-Stack Referencing, the in-taking Stack must have a !ImportValue set
Important: Every Stack that Imports the Output MUST be deleted first
	So, every Stack that created Resources from an Import must have their Stacks deleted first
Ex:
Resources:
	...
		SecurityGroups:
			- !ImportValue SSHSecurityGroup
// To delete the Stack that created the SSHSecurityGroup Export,
// All Stacks that Imported SSHSecurityGroup need to be deleted first
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Conditions
//////////////////////////////////////////////////////////////////////////

Overview
////////////////////
Controls the creation of Resources based on Conditions
	Ex: dev Environment vs prod Environment
Conditions can be any measurable value: 
	Region
	AZ
	Environment (Dev, Test, Prod, ...)
Conditions can also reference other Conditions
////////////////////

How to Define a Condition
////////////////////
Here, we create a Condition that can then be used elsewhere

Ex:
Conditions:
	CreateProdResources: !Equals [ !Ref EnvType, prod ]
               // Same as Fn::Equals [ !Ref EnvType, prod ]

CreateProdResources // Logical ID
!Equals             // Intrinsic Function

Intrinsic Functions can be:
	And, Equals, If, Not, Or
////////////////////

How to Use a Condition
////////////////////
Here, the above Condition is used to determine if this Resource should be created
	And if this is the prod Environment, it will be created
Ex:
Resources:
	MountPoint:
		Type: AWS::EC2::VolumeAttachment
		Condition: CreateProdResources
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Intrinsic Functions
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
There are MANY of these (see slide)
Most common are:
	Ref, GetAtt, ImportValue
////////////////////

Intrinsic Function - Fn::Ref
////////////////////
References a Parameter or Resource
CANNOT reference a Condition

Ex:
Resources:
	DBSubnet1:
		Type: AWS::EC2::Subnet
		Properties:
			VpcId: !Ref MyVPC

MyVPC - Is a Resource or Parameter defined earlier in the script
Works by getting the ID of a Resource
////////////////////

Intrinsic Function - Fn::Join
////////////////////
Joins a list of values with a delimiter between each one
	An empty string will result in no delimiter between the values

Ex:
Fn::Join ["-", ["Utter", "Bullshit"]]
	Result: "Utter-Bullshit"
////////////////////

Intrinsic Function - Fn::GetAtt
////////////////////
Gets attributes attached to a Resource, like the Resource's AZ
Ex:
Slide shows an EC2 Instance get defined
	Then, an EBSVolume is allocated
		This is placed in the same AZ as the EC2 Instance
		This is done dynamically by using GetAtt to get the AZ from the EC2 Instance:
			*EBSVolume:*
				...
				AvailibilityZone: !GetAtt EC2Instance.AvailabilityZone

EC2Instance is the name of the Resource definition
////////////////////

Intrinsic Function - Fn::FindInMap
////////////////////
Returns a named value from a specific key
Structure is:
	!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]
Slide defines RegionMap, where different Regions use different AMIs
	These are further broken down by HVM64 and HVMG2
Later in the script, when defining an EC2 Instance, it will pull the correct AMI:
	MyEC2Instance:
		...
		ImageId: !FindInMap [RegionMap, !Ref "AWS::Region", HVM64]

So in the Ex:
	It pulls from RegionMap
	It get the Region for where the script is being run
	It get the HVM64 AMI 
////////////////////

Intrinsic Function - Fn::ImportValue
////////////////////
Imports values exported by other definitions
Ex:
MySecureInstance:
	...
	SecurityGroups:
		- !ImportValue SSHSecurityGroup
////////////////////

Intrinsic Function - Fn::Base64
////////////////////
Converts String to Base64 representation

	!Base64 "ValueToEncode"

The most common use is to send the String of User Data to an EC2 Instance

Ex: Putting a String into the UserData of a WebServer (EC2 Instance)
WebServer:
	Type: AWS::EC2::Instance
	...
	UserData:
		!Base64 |
			#!/bin/bash
			dfn update -y
			dfn install -y httpd
////////////////////

Intrinsic Function - Fn::Condition Functions
////////////////////
These are the Condition Functions from the previous section:
	And, Equals, If, Not, Or
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Rollbacks
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Rollbacks have different behavior, based on app status.
Stack Creation Failure - delete everything or troubleshoot
Stack Update Failure - Rollback to previous working state
Rollback Failure - Manual fixes and continue Rollback
////////////////////

Hands On
////////////////////
Goal:
	Trigger a Rollback to test everything deleted vs only failed Resources

Rollback can affect everything, or some Resources
	So if only one Resource failed to create, the rest could be created
After fixing the issue, User MUST delete the failed Stack and re-update
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Service Role
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
CloudFormation uses ServiceRoles as its Security Groups
These are IAM Roles given to CloudFormation for the purpose of creating Resources.
User creating a Stack w/ CloudFormation doesn't need the Permissions needed for those Resources
	User must have iam::PassRole Permissions
////////////////////

Hands On
////////////////////
Created through the IAM Console
	For an AWS Service (CloudFormation)
Default IAM Role is the Permissions of the User creating the PassRole
Optional is to create a separate IAM Role with designated Permissions
	So now this CloudFormation Service will only have the Permissions of the IAM Role
	And will use those Permissions every time it creates its Stack
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Capabilities
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
CAPABILITY_NAMED_IAM and CAPABILITY_IAM
Used when creating or updating IAM Resources (CAPABILITY_IAM) 
	or named IAM Resources (CAPABILITY_NAMED_IAM)

CAPABILITY_AUTO_EXPAND
Needed for Macros or Nested Stacks
Template may change before deploying

InsufficientCapabilitiesException
Excepton thrown during deployment when Capabilities haven't been acknowledged
////////////////////

Hands On
////////////////////
In practice, this just comes down to checking a box when the Stack's script will create IAM Resources.
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - DeletionPolicy
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
This is a top-level attribute of a Resource that controls what happens on deletion of the Stack
Ex:
Resources:
	MyResource:
		...
		DeletionPolicy: //delete, retain, or snapshot 
////////////////////

CloudFormation - DeletionPolicy Delete
////////////////////
Just delete with the rest of the Stack
May need to be modified first (like an S3 Bucket needs to be emptied first)
////////////////////

CloudFormation - DeletionPolicy Retain
////////////////////
Keep this Resource when the Stack is deleted
////////////////////

CloudFormation - DeletionPolicy Snapshot
////////////////////
Keep a copy of this Resource when the Stack is deleted
////////////////////

Hands On
////////////////////
After setting the parameters in the script to retain or snapshot
	We can see these Resources (or a copy) stays after the Stack is deleted
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Stack Policies
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
During creation, a Stack has full Permissions as defined by its User or ServiceRole
However, when updating, we may want to restrict Permissions for Resources
////////////////////

Ex.
////////////////////
Slide shows a script that defines the Update where it has Allow for all Resources
It then has the restriction that the prod DB has a Deny for all
	So, everything in the Stack can be upated, except for the DB
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Termination Protection
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Prevents accidental deletion of a Stack
////////////////////

Hands On
////////////////////
Once a Stack has been created, we can go to Stack Actions in the console, and activate this
	Default is inactive
It is the first item in the Stack Actions dropdown
We can't delete the Stack until this has been turned off
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - Custom Resources
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Resources that aren't already supported by CloudFormation
	Or that are outside of AWS
	Or that are custom scripts being run (Ex: Empty an S3 bucket so it can be deleted)
////////////////////

How to define a Custom Resource
////////////////////
Defined by:
	AWS::CloudFormation::CustomResource, or
	Custom::MyCustomResourceTypeName (recommended)

Ex: (Lambda, b/c it is the most common)
Resources:
	MyCustomResourceUsingLambda:
		Type: MyLambdaResource // Our custom name for the type
		Properties:
			ServiceToken: *Lambda arn* // Must be in same Region
			*Input values, Optional*
			ExampleProperty: "ExampleValue"
////////////////////

Ex: Use Case - Delete content from an S3 bucket
////////////////////
When the Stack has delete called, it will empty the S3 Bucket
	And then the S3 Bucket will be deleted correctly
////////////////////
//////////////////////////////////////////////////////////////////////////

CloudFormation - StackSets
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Organizes Stacks into StackSets for ease of creation, updating, and deletion across Region or accounts
	This can go across AWS Accounts in an AWS Organization
	Can only be created by an Admin Account
////////////////////
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 19
  __            ___
 /  \ |  |  /| |   |   
 |    |__|   | |___|
 |    |  |   |     |
 \__/ |  |. _|_    |

AWS Integration & Messaging: SQS, SNS, and Kinesis
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

Introduction to Messaging
//////////////////////////////////////////////////////////////////////////

Section Intro (1/2)
////////////////////
There are two types of Messaging used by AWS
	Synchronous: Direct communication from one Service to another
	Asynchronous: Messaging goes to a middle layer (like a queue), before the next Service
////////////////////

Section Intro (2/2)
////////////////////
The type used depends on application needs, and traffic load

In the case of high traffic, it's better to use:
	SQS, SNS, or Kinesis
////////////////////
//////////////////////////////////////////////////////////////////////////

Amazon SQS - Standard Queue Overview
//////////////////////////////////////////////////////////////////////////
Amazon SQS - What's a Queue?
////////////////////
Producer puts messages into the Queue
Consumer takes messages from the Queue
////////////////////

Amazon SQS - Standard Queue
////////////////////
SQS is Amazon's oldest AWS offering
Unlimited throughput
Default hold is 4 days to max of 14 days
Low latency with limit of 256KB per message

At least once delivery (duplicates allowed)
May be out of order
////////////////////

SQS - Producing Messages
////////////////////
Produced messages persist in the queue from 4 to 14 days, or until a Consumer deletes it
Messages are sent to the queue by the SDK (SendMessage API)
////////////////////

SQS - Consuming Messages
////////////////////
Consumers can be EC2 Instances, servers, AWS Lambda
Messages can be consumed (up to 10 at a time)
On consumption the messages are deleted (DeleteMessage API)
////////////////////

SQS - Multiple EC2 Instances Consumers
////////////////////
Messages can be consumed in parallel,
which is a great use-case for scaling consumers horizontally.
////////////////////

SQS with Auto Scaling Group (ASG)
////////////////////
Diagram
	Slide shows a CloudWatch Metric checking the Queue Length
	If the queue gets too big, it sets off an alarm
 	The alarm triggers the ASG to increase the Consumer capacity
////////////////////

SQS to Decouple Between App Tiers
////////////////////
Diagram
	Slide shows a diagram of jobs coming in, processing, and storage to an S3 Bucket
	Because the processing can take a while, it is beneficial to decouple it
	The front-end web app takes the jobs
	The Queue holds the jobs
	The Back-end processing takes the jobs, process them, and then stores them in an S3
	
	Both the front-end and back-end have an ASG, so neither gets overwhelmed
		Or wastes resources
////////////////////

SQS - Security
////////////////////
There are three types of encryption:
	Uses in-flight encryption w/ HTTPS API
	At-rest encryption w/ KMS Keys
	Client-side encryption if client wants to handle the encryption/decryption

SQS Access Policies are similar to S3 Bucket Policies:
	Cross-account access
	Allows other Resources to write to the SQS queue
////////////////////
//////////////////////////////////////////////////////////////////////////

SQS - Standard Queue Hands On
//////////////////////////////////////////////////////////////////////////
Goal:
Set up an SQS queue, an be able to receive and process messages

> Amazon SQS > Queues
-> Create Queue

We're creating a Standard Queue
Leave the default Configuration and Encryption
Access Policy defines who can access the queue, and what they can do
	Default is everyone can access, and do everything

Once created, we can now create messages that are sent to the queue
We can then pull those messages, and check its information
//////////////////////////////////////////////////////////////////////////

SQS Queue Access Policy
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
SQS Queue Access Policy is like an S3 Bucket Policy, controlling access
	Controls access by Users and Services
Same parameters as a Bucket Policy
////////////////////

Hands On
////////////////////
In the hands on, an S3 Bucket is set to send messages to the SQS Queue
	The SQS is set so its Access Policy allows messages from the S3
	This happens on the S3 Event of a new object being added
////////////////////
//////////////////////////////////////////////////////////////////////////

SQS - Message Visibility Timeout
//////////////////////////////////////////////////////////////////////////
Overview:
////////////////////
The SQS will have a message taken by a consumer
	That message is then invisible to other consumer for a timeout period
		(If it is not processed and removed)
	That timeout period must be balanced between too long and too short
		(To avoid duplicate processing on the same message)
	Too long: Something went wrong with the consumer, processing takes too long
	Too short: Becomes visible in the queue too soon, and duplicate processing occurs
////////////////////

Hands On
////////////////////
Create message
	Have consumer1 pull the message
	Have consumer2 search messages (does not see the message)
	Something goes wrong w/ consumer1 (processing takes a long time)
	Timeout finishes
	Consumer2 now sees and pulls the message
		This is now duplicate processing
////////////////////
//////////////////////////////////////////////////////////////////////////

SQS - Dead Letter Queue
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Dead Letter Queues are quarantine zones for messages that have been pulled too many times
	(More times than the MaximumReceives setting)
	This means that there is something wrong with the message or consumer
	It is sent to the Dead Letter Queue for manual analysis
Queue type must be the same, so FIFO to a FIFO, or standard to a Standard
Messages in the Dead Letter Queue will have an expiration date
////////////////////

Redrive to Source
////////////////////
Once the errors are resolve, the messages are sent back into the main queue to be processed
////////////////////
//////////////////////////////////////////////////////////////////////////

SQS - Dead Letter Queue Hands On
//////////////////////////////////////////////////////////////////////////
Setup:
	Select a queue
	Enable Dead Letter Queue
	Set MaximumReceives to 3
	Create message w/ poison pill message (can't be processed)
Consumer pulls and puts back the message three times
	On the fourth time, it goes to the Dead Letter Queue
	Resolve the message or consumer
	Enable the DLQ Redrive
	Message goes back to the main queue
//////////////////////////////////////////////////////////////////////////

SQS Delay Queue
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
The DelaySeconds parameter delays messages showing up in the queue
	Message goes in
	Delay
	Message is available for pulling
////////////////////

Hands On
////////////////////
Setup:
	Create queue w/ DelaySeconds set to 30 seconds
	Creating messages for the queue, standard delay is what we set the queue to
Pulling messages show nothing for 30 seconds
////////////////////
//////////////////////////////////////////////////////////////////////////

SQS - Certified Developer Concepts
//////////////////////////////////////////////////////////////////////////
Long Polling
////////////////////
This is where the consumer polls the queue over a long period of time
	This decreases the number of API calls to the queue
////////////////////

SQS Extended Client
////////////////////
Allows consumers to get large files that have to be stored in an S3 Bucket
	The message in the queue tells the consumer to go get the data from S3
////////////////////

Must Know API
////////////////////
There are several API calls to interact with the queues
A few of these can also be used with batch processing
////////////////////

Long Polling Hands On
////////////////////
Done by increasing the value of the Polling Duration variable of the consumer
	This keeps the polling going without needing to do another API call
////////////////////
//////////////////////////////////////////////////////////////////////////

SQS - FIFO Queues
//////////////////////////////////////////////////////////////////////////

Overview
////////////////////
First In, First Out
	Messages only send once, and are processed in order
////////////////////

Hands On
////////////////////
By naming the queue with ".fifo" at the end, we get a FIFO queue
	Make sure to check the box for de-duplication
Now, even though we have four different consumers all grabbing messages, they execute in order
////////////////////
//////////////////////////////////////////////////////////////////////////

SQS - FIFO Queues Advanced
//////////////////////////////////////////////////////////////////////////

FIFO - Deduplication
////////////////////
First In, First Out
De-duplication set to 5 minutes
	Requires a Message De-duplication ID to set explicitly
////////////////////

SQS FIFO - Message Grouping
////////////////////
Same MessageGroupID means that all messages with it will be processed in-order
	If all messages have the same MessageGroupID, then they will all be processed in-order
	Groups of messages with the same MessageGroupID will be in-order for that subset
////////////////////

Hands On
////////////////////
Sending the same message again and again doesn't result in duplicates, nothing happens
	So there is already some de-duplication
Sending a group of messages with the same MessageGroupID makes them all be in order to each other
////////////////////
//////////////////////////////////////////////////////////////////////////

Amazon SNS
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
SNS is an abstraction, allowing Services to send messages out to other Services/Endpoints
	Rather than having each Service get Permissions to send messages
////////////////////

Subs and Pubs
////////////////////
Services are Subscribers to Publishers (Publishers send to SNS)
	There are filtering options
	The communication can handle a LOT of Messages
Lot of AWS Services can be Publishers/Subscribers
	Subs can also be things like emails, mobile notifications, and http endpoints
////////////////////

AWS SNS - How to Publish
////////////////////
Create SNS -> Create sub(s) -> Publish to SNS

For mobile: Create app -> Create endpoint -> Publish to endpoint
////////////////////

AWS SNS - Security
////////////////////
Encryption:
	Same as SQS, so in-flight, KMS, or Client-side

Access Controls:
	IAM Policies

SNS Access Policies:
	Like S3 Bucket Policies, allowing access for accounts and Services
////////////////////
//////////////////////////////////////////////////////////////////////////

SNS and SQS - Fan Out Pattern
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Combines an SNS w/ an SQS to ensure no messages are lost, and it's scalable
////////////////////

Application: S3 Events to Multiple Queues
////////////////////
An S3 can only have one Event Rule (item created, deleted, etc...)
	An SNS can fan out that message to multiple Services
////////////////////

Application: SNS to Amazon S3 Through Kinesis Data Firehose
////////////////////
We can send messages from an SNS to KDF and therefore any KDF-supported Service/endpoint
////////////////////

SNS - FIFO and SQS FIFO Fan Out
////////////////////
Messages are sent out in the same order received
	And has deduplication

Subs can ONLY be SQS Standard or SQS FIFO
	SQS FIFO just extends the features of the SNS FIFO
////////////////////

SNS - Message Filtering
////////////////////
Filter Policies change which subs get which messages

Ex: slide shows how a filter for the message's "state" variable changes which subs get it
	"state" can be purchase placed, cancelled, declined, etc...
////////////////////
//////////////////////////////////////////////////////////////////////////

SNS Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create an SNS that sends messages to an email

SNS > Create SNS
	Select Standard
	Select a sub
	Finish

We now have an SNS that can send messages to an email
	Can use the confirm button to send a confirmation message as well
//////////////////////////////////////////////////////////////////////////

Amazon Kinesis Data Streams
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
IMPORTANT!!!! -> Kinesis Data Streams are for real-time data
	Streams, so created and used right away
Streamers (create real-time data)
	-> Producers (app that receive the data)
		-> Amazon KDS (moves the data elsewhere)
			-> Consumers (use or store that data)

KDS takes and retains data, usually as lots of small real-time data
	Stores up to 1 year, and can replay data
It can be ordered, encrypted, but not deleted (it must expire)
There are two adjacent Services:
	Kinesis Producer Library (KPL) // a Producer
	Kinesis Client Library (KCL)   // a Consumer
////////////////////

Capacity Modes
////////////////////
Provisioned Mode: choose a number of shards of data, with a cost per shard

On-Demand Mode: scaling capacity that charges by stream per hour and data in/out per GB
////////////////////
//////////////////////////////////////////////////////////////////////////

Amazon Kinesis Data Streams - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: simulate a data streaming pipeline to showcase how KDS moves data
	Lots of this uses pre-created commands from the teacher
	Applications tab in KDS console has links to example Producers and Consumers

Create a Streamer
	// We don't have this, rather putting in data, starting at the Producer step

Send data as a Producer
	Open the AWS CloudShell -> enter command to send data as an AWS Kinesis Producer

Create a KDS
	Create Data Stream -> On-Demand // No free tier

Create a Consumer
	Open the AWS CloudShell -> enter command to pull data as an AWS Kinesis Consumer

Doing this simulates the pipeline of:
	Producer creates a data record (shard) // Would be created by a stream, but we spoof this
	KDS takes that data, and stores it
	Consumer pulls that data
//////////////////////////////////////////////////////////////////////////


Amazon Kinesis Firehose
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Receives data from Producers
	apps, SDKs, KDS, more...

When received, data can be:
	transformed (custom Lambda or AWS preset)
	if failed, stored in S3

Stores data before doing a batch write to a Destination
	Destinations can be 3rd Party (mongoDB, ...), AWS (S3, ...), or custom (HTTP Endpoint, ...)
		Lots of options for each of these (see slide)
////////////////////

Overview Details
////////////////////
Fully Managed
Auto-Scaling, pay per usage
Near Real-Time // Is stored until a batch-write
////////////////////

KDS vs Firehose
////////////////////
KDS:
	KDS streams data in real-time, storing data, and can replay

Firehose:
	Auto-scales batching streams, and writes for near real-time, but with no storage or replay
////////////////////
//////////////////////////////////////////////////////////////////////////


Amazon Kinesis Firehose - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: simulate a pipeline of data for Firehose

Amazon Kinesis > Create Delivery Stream // Firehose
	Choose a Source: KDS
	Choose Destination: S3
	Source Settings: Select our existing KDS
	// Note the Data transformation (Lambda) and Reord Formart Conversion (AWS presets) options
	Select existing S3 Bucket
	Set Buffer Interval to 60 seconds
	// Note the auto-created IAM Role for the KDS to write into the Firehose

Use the same CommandShell commands from before to send data into the KDS
	After 60 seconds (Buffer Interval we set) the data appears in the S3 Bucket
//////////////////////////////////////////////////////////////////////////


Amazon Managed Service for Apache Flink
//////////////////////////////////////////////////////////////////////////
Framework for processing data streams

Apps created with Flink can take data from KDS or Apache Kafka // NOT Firehose
	Data can be computed, backed-up, and more
//////////////////////////////////////////////////////////////////////////


SQS vs SNS vs Kinesis
//////////////////////////////////////////////////////////////////////////
Lots of details on the slide for this

SQS
////////////////////
Temporarily stores data for Consumers (pipeline: store, pull, delete)
	Auto-scales to meet capacity of received messages
////////////////////

SNS
////////////////////
Moves data from a Publisher to multiple Subscribers (pipeline: immediate recieve and send)
////////////////////

Kinesis
////////////////////
Streams real-time data to one or more Consumers, while storing it for a period
	pipeline: receive, store and immediate send
////////////////////
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 20
  __          __   __
 /  \ |  |   /  \ /  \
 |    |__|      / | /|
 |    |  |     /  |/ |
 \__/ |  |.   /__ \__/

AWS Monitoring & Audit: Cloudwatch, X-Ray and CloudTrail
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

Monitoring Overview in AWS
//////////////////////////////////////////////////////////////////////////
Why Monitoring is Important
////////////////////
We care about how things were deployed, but the client only cares about the results and impacts
To this, we need to make sure that things work as intended and look ahead for potential problems
Monitoring allows us to both see patterns and troubleshoot
////////////////////

Monitoring in AWS
////////////////////
AWS CloudWatch:
	Metrics, Logs, Events, and Alarms

AWS X-Ray:
	Troubleshooting and Tracing Microservices

AWS Cloudtrail:
	Internal Monitoring and Auditing changes to AWS Resources
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudWatch Metrics
//////////////////////////////////////////////////////////////////////////
AWS CloudWatch Metrics
////////////////////
CloudWatch has Metrics for every AWS Service
These Metrics monitor variables, which are then displayed on the CloudWatch dashboard of metrics
////////////////////

EC2 Detailed Monitoring
////////////////////
EC2 Metrices have metrics every 5 min as a standard, but can get every minute
AWS Free Tier allows for 10 detailed metrics
Note: EC2 memory usage is not standard, and has to be pushed from inside of Instance
////////////////////

Hands On
////////////////////
 > CloudWatch > Metrics > EC2 > Per Instance Metric > Do a search for Credits
	Choose one of our EC2 Instances > Choose a Custom Range of time
	We can now see a graph with the CPU usage recorded every five minutes
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudWatch Custom Metrics
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Custom Metrics can be created for CloudWatch using the API call PutMetricData
	This can be standard (1 minute), or higher resolution (but costs)
		Highest resolution is 1 second
	We are allowed to push this as two weeks in the past, or two hours in the future
		So, this must be checked for accuracy
////////////////////

Hands On
////////////////////
The CloudWatch API can be called from CloudShell
	Slide has 
	The AWS CLI Command Reference has a list of different arguments for PutMetricData
		has example commands as well, which can be put into CloudShell
After entering the command into CloudShell and refreshing the CloudWatch console, we can see the custom namespace listed
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudWatch Logs
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Logs are usually created with a group for an app
	Can have an expiration date
	Can be sent to several different Services (see slide)
////////////////////

Sources
////////////////////
CloudWatch Logs can come from several sources (See slide)
	These sources each have their own data that they report to CloudWatch
	Ex. Route53 logs all DNS Queries made
////////////////////

Insights
////////////////////
Logs are accessed by queries to CloudWatch Logs Insights
	Search and analyze data
	Query language is purpose-built, finding fields from AWS Services, fetching desired fields, more...
	Query multiple log groups, even from other accounts
////////////////////

S3 Export
////////////////////
Batch export to S3, using the API call CreateExportTask
////////////////////

Subscriptions
////////////////////
Real-time stream of Log events from CloudWatch Logs
	Filter can direct logs to different Services, like KDS, Firehose, S3, more...
////////////////////

CloudWatch Logs Aggregation Multi-Account & Multi-Region
////////////////////
CloudWatch Subscription Filters across regions and accounts can be directed to the same KDS
////////////////////

Subscription Destinations
////////////////////
Can be used to send CloudWatch Logs across accounts
	Slide has example of Subscription filter in one account sending data to a KDS in another
		The Subscription Destinations receive the log data, and forward it to the Services in that account
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudWatch Logs - Hands On
//////////////////////////////////////////////////////////////////////////
 > CloudWatch > Log Groups
	Select a Log Group
	Select a Log Stream (stdout)

Create a Filter Pattern
	Filter Pattern: Installing
	Log Data: Select existing log data to test
	Fil out the Metric Details (calling everything DemoMetric-Something)

Select the newly created Custom Namespace
 > CloudWatch > Logs Insights
	Run the query to get timestamps of data
		Records should show up
//////////////////////////////////////////////////////////////////////////


CloudWatch Logs - Live Tail - Hands On
//////////////////////////////////////////////////////////////////////////
 > CloudWatch Logs > Live Tail

Create a new Log Group
Select the new Log Group
	Create a new Log Stream
In Live Tail, Set the Filter to be from the created Log Stream and to the Created Log Group

In the Log Stream, Create a Log Event: "Hello World"
	The event shows up in Live Tail as it tracks Log Events
//////////////////////////////////////////////////////////////////////////


CloudWatch Agent & CloudWatch Logs Agent
//////////////////////////////////////////////////////////////////////////
CloudWatch Logs for EC2
////////////////////
Default is no logs from EC2 Instance to CloudWatch
	Need to: run CloudWatch agent on EC2 Instance and Set IAM Permissions
Slide diagram shows AWS and on-premises EC2 going to CloudWatch Logs
////////////////////

CloudWatch Logs Agent & Unified Agent
////////////////////
Old: CloudWatch Agent, can only do logs
New: CloudWatch Unified Agent, can do logs and more system-level metrics (RAM, processes, ...)
	Easy to set up with SSM Parameter Store
////////////////////

CloudWatch Unified Agent - Metrics
////////////////////
Can collect a lot of Metrics:
	CPU, Disk metrics, RAM, more... (see slide)
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudWatch Logs - Metric Filter
//////////////////////////////////////////////////////////////////////////
CloudWatch Logs can filter based on content found (IP, words), and can trigger alarms
	ONLY works from creation of Filter
//////////////////////////////////////////////////////////////////////////


CloudWatch Logs - Metric Filter Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Set up a simple CloudWatch Log w/ a Metric Filter for 400 error codes

 > CloudWatch > Logs
	-> Create Metric Filter
	// See Metric and Filter Pattern docs for syntax
	Filter Pattern: "400" // Very simple for this demo
	Select the Log to filter
		Test Pattern // Make sure that the filter works on existing data
	-> Next
	Name the:
		Filter, Metric Namespace, Metric, and Metric Value
	Set the default value: 0
The metric is now created, but doesn't show up in the console b/c it hasn't seen anything yet
	This is b/c it doesn't read previous data (except to test syntax)

Once the app the Metric Filter is tracking reloads, it's visible, even if no 400 error have happened
	Until a 400 error happens, it will be empty

Create Alarm:
	Select the Filter
	-> Create Alarm
	Select Static
		Greater than 50
	Set Alarm to existing SNS Topic
	Name the Alarm

We now have a CloudWatch Filter and Alarm set to look for Error 400 codes
	The Alarm triggers if they exceed 50
//////////////////////////////////////////////////////////////////////////


CloudWatch Alarms
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Alarms use a Metric to change their State
	Metrics: data recorded over Period, breaching some established threshold
	State: OK, INSUFFIFICIENT_DATA, ALARM
CloudWatch Alarms on High-Resolution Custom Metrics can be triggered as often as 10 seconds
	Or 30 seconds, or any multiple of 60 seconds
////////////////////

CloudWatch Alarms Targets
////////////////////
Alarm can take several actions:
	EC2: Affect Instance (turn on, off, reboot, etc...)
	EC2 Auto-Scaling: in/out
	SNS: Send message to SNS // If sent to a Lambda, it then do anything we want
////////////////////

CloudWatch Alarms Composite Alarms
////////////////////
Alarms only track one Metric, so we track multiple Metrics w/ Composite Alarms
	Has AND and OR logic in determining if an Alarm is triggered
////////////////////

EC2 Instance Recovery
////////////////////
Alarms can track Instance Status, System Status (hardware), and Attached EBS
	With these, we determine how to recover from a failure
////////////////////

CloudWatch Alarm: Good to Know
////////////////////
Alarms can use CloudWatch Logs Metrics
Alarms can be tested to make sure they will do what we require
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudWatch Alarms - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create and test a CloudWatch Alarm for an EC2 Instance at >= 95% CPU

 > CloudWatch > Create Alarm
	Set the Metric:
		Choose an EC2 Instance
		Select Metric: CPU Utilization
			Set monitoring interval
			Set value threshold
			Set count threshold // how many intervals we detect the threshold being breached
	Select Action: Terminate Instance
	Create Alarm

Test the Alarm:
 > CloudWatch > Alarms > *Alarm Name*
	Get the needed values from AWS CLI Command Ref Docs
	In CloudShell:
		aws cloudwatch set-alarm-state --alarm-name *alarm name*
					       --state-value ALARM
					       --state-reason "Testing"

We now have the Alarm in the ALARM state, and the EC2 Instance gets shut down
//////////////////////////////////////////////////////////////////////////


CloudWatch Synthetics Canary
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Create scripts to simulate user/system behavior // Ex: shoppers using website
	Lots of details on slide
	Integration w/ CloudWatch Alarms, and other Services for full testing
////////////////////

CloudWatch Synthetics Canary Blueprints
////////////////////
Components of Canary Blueprints
	Heartbeat Monitor: ???
	API Canary: read/write functions of REST APIs
	Broken Link Checker: Check all links
	Visual Monitoring: compare screenshot taken by Canary w/ working-state screenshots
	Canary Recorder: record actions on site and build script to repeat them
	GUI Workflow Builder: test workflows on site
////////////////////
//////////////////////////////////////////////////////////////////////////


Amazon EventBridge
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Detect or schedule Events, and then perform Actions
////////////////////

EventBridge Rules
////////////////////
Logic Flow:
	An AWS Service has an Event (EC2 Restarts, S3 uploads object, schedulted Event, ...)
	EventBridge detects the Event based on optional filters
	EventBridge performs an action based on the Event
		Compute - (Ex: Lambda)
		Integration - (Ex: SNS)
		Orchestration - (Ex: Code pipeline)
		Maintenance - (Ex: Restart EC2)
////////////////////

Different EventBridges
////////////////////
There are different Event Bus for AWS and Third-party Services
	AWS:         Default Event Bus
	Third-Party: Partner Event Bus
	Custom Apps: Custom Event Bus
Cross-account access and replay archiving available
////////////////////

EventBridge - Schema Registry
////////////////////
Create code for app that dictates the structure of data in the Event Bus
////////////////////

EventBridge - Resource-based Policy
////////////////////
Permissions Policy for an Event Bus, can grant cross-account Permissions
////////////////////
//////////////////////////////////////////////////////////////////////////


Amazon EventBridge - Hands On
//////////////////////////////////////////////////////////////////////////

// I was pretty dead tired while going through this, so it might need some rewriting

 > Amazon EventBridge > Event Buses

 -> Create Event Bus
	Set Name
	Set Archive Name
	Enable default Schema Discovery
 -> Create Rule
	Set Name
	Select the default Event Bus
	Rule w/ an event pattern
	-> Next
	Event Source: AWS Services
	Set the Sample Event as EC2 Instance State Change Notification
		(Stopped)
	Select: Use Pattern Form
	Event Source: AWS Services
	AWS Service: EC2
	Event Type: EC2 State Change Notification
	-> Next
	Target Types: AWS Service
	Target: SNS Topic
	Topic: Select existing topic
	-> Next -> Next -> Review and Create -> Create

We now have an Event Bridge that detects when EC2 Instances are stopped
	And will send a message into the SNS
//////////////////////////////////////////////////////////////////////////


Amazon EventBridge - Multi-Account Aggregation
//////////////////////////////////////////////////////////////////////////
Multiple accounts all send their Events to a central Event Bus
	THat Event Bus then funnels those Events elsewhere
//////////////////////////////////////////////////////////////////////////


X-Ray Overview
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Visual way to see a trace of your entire app on one screen
This is in contrast to just putting console.logs everywhere and tracing that way
	Especially as there's different logging methods across Services
////////////////////

Visual Analysis
////////////////////
Slide shows a diagram of arrows starting w/ a client 
	going to the app entry point
		and then through the rest of the components of the app
////////////////////

AWS X-Ray Advantages
////////////////////
Lots of advantages, most especially that we can see the path of data and bottlenecks
////////////////////

X-Ra Compatibility
////////////////////
Lots of compatibility w/ Services
	Lambda, Elastic Beanstalk, ECS, on-premise, ...
////////////////////

AWS X-Ray Leverages Tracing
////////////////////
End-to-end tracing the flow of a request through an app
	Tracing is made of segments, and can be given more info by each part of the app
	Can trace actual and simulated requests
		Can be every request or a request every rate per minute
////////////////////

AWS X-Ray - How to Enable
////////////////////
1) Enable by importing the AWS-SDK, so we can capture:
	AWS Services calls, HTTP(S) requests, db calls, SQS calls
2) Install the X-Ray daemon or install the X-Ray AWS Integration
	Lots of Services already run the daemon
X-Ray daemon sends a batch every second to the X-Ray console
Common issue is that dev machine has the daemon, but the prod machine doesn't
////////////////////

The X-Ray Magic
////////////////////
X-Ray creates the magical visual graph by collecting all traces from the Services sending data
////////////////////

AWS X-Ray Troubleshooting
////////////////////
EC2: Proper Permissions and is running the daemon
Lambda: IAM Role w/ proper Policy, imported the code, and enabled Lambda X-Ray Active Tracing
////////////////////
//////////////////////////////////////////////////////////////////////////


X-Ray - Hands On
//////////////////////////////////////////////////////////////////////////
Create the CloudFormation Stack created by teacher
	Creates very simple tic-tac-toe game
	Already conjured with X-Ray
	Link takes you to the visual graph
	
	We can see an error
	Filter the traces by a query
	LOTS of data here like latency, errors, and more
//////////////////////////////////////////////////////////////////////////


X-Ray: Instrumentation and Concepts
//////////////////////////////////////////////////////////////////////////
Instrumentation in code
////////////////////
Instrumentation: measure performance, find errors, trace data
Slide has code snippet of using the X-Ray SDK w/ express to do this
Can modify app code to send extra data to X-Ray using interceptors, filters, more ...
////////////////////

X-Ray Concepts
////////////////////
Segments: batch each app/Service sends
Subsegments: if more detail is needed
Trace: collected end-to-end trace of segments
Sampling: smaller number of calls to X-Ray to reduce costs
Annotations: additional data as key-val pairs, index traces w/ filters
Metadata: additional data as key-val pairs, not indexed, not searching

X-Ray Daemon/Agent: send traces cross-account
	Allows for central account w/ all tracing
////////////////////


X-Ray Sampling Rules
////////////////////
Sampling Rules: control amount of data recorded
	Modify these rules independent of code

Default Sampling is first request each second, 5% of additional requests
Reservoir: The one request per second
Rate: The 5% of additional requests
////////////////////


X-Ray Custom Sampling Rules
////////////////////
Define custom rules for Reservoir and Rate

More data coming in can become very expensive
Changing these doesn't require changing anything with the app components/Services
	It just seemly starts working
////////////////////
//////////////////////////////////////////////////////////////////////////


X-Ray: Sampling Rules
//////////////////////////////////////////////////////////////////////////
Goal: Go over the Sampling Rule parameters in the AWS console

 > CloudWatch > Settings > Traces tab -> Sampling Rules View Settings -> Sampling Rules tab

Editing the Sampling Rule:
	Change the Reservoir and Rate
	Is set to track all URLs and HTTP methods

Create Sampling Rule:
	Set name
	Set Priority // low number is top priority
	Set Reservoir and Rate
	Set the Service(s) to be sampled // * for all
	Set the HTTP Method(s) to be sampled // * for all

No need to restart any apps, they will start being traced upon creation of the Rule
//////////////////////////////////////////////////////////////////////////


X-Ray APIs
//////////////////////////////////////////////////////////////////////////
This is the API for the X-Ray Daemon on the EC2 Instance to write/read to the X-Ray console
For any of these APIs to work, the Instance MUST have the IAM Policy Permissions to the X-Ray Service

Writes
////////////////////
PutTraceSegments: Write segments to X-Ray
PutTelemetryRecords: Writes segments received, rejected, connection errors, ...
GetSamplingRules: Gets all sampling rules
GetSamplingTargets and GetSamplingStatisticsSummaries: Get targets and statistics summaries
////////////////////

Reads
////////////////////
LOTS of different Gets here (see slide)
Basically, they are as their name implies
GetServiceGraph: main graph
BatchGetTraces: List of traces by ID, CANNOT be used w/ X-Ray
GetTraceSummaries: IDs and annotations of tracs within time frame
GetTraceGraph: Get a Service graph by ID
////////////////////
//////////////////////////////////////////////////////////////////////////


X-Ray w/ Beanstalk
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Can set one option in the creation of an Elastic Beanstalk to enable X-Ray
	Or set up the .ebextensions/xray-daemon.config with:
		////////////////////
		option_settings:
			aws:elasticbeanstalk:xray:
				XRayEnabled: true
		////////////////////
This sets up the X-Ray Daemon w/ Permissions as well
	The AWSElasticBeanstalkWebTier Policy has the minimum needed Permissions to read and write

NOTE:
	app code sends data using the X-Ray SDK
	X-Ray Daemon not provided for multi-container Docker
////////////////////

Hands On
////////////////////
Goal: See the important options for enabling X-Ray w/ Elastic Beanstalk

 > Elastic Beanstalk -> Create Application
Set name
Set to Nodejs
*Skip through till security and optional configurations*
Under Platform Software activate the X-Ray daemon
Under Security make sure that the EC2 Instance has the aws-elasticbeanstalk-ec2-role
	This role has the Policy w/ X-Ray Permissions

Now the Stack is fully set up w/ X-Ray
////////////////////
//////////////////////////////////////////////////////////////////////////


X-Ray and ECS
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
There are three options, depending on need
Slide has a great visual of these

ECS Cluster: Each EC2 Instance has one X-Ray Daemon Container, multiple App Containers
ECS Cluster w/ Sidecar: Each App Container has an X-Ray Sidecar (daemon) running on it
Fargate Cluster: Each App MUST have its own X-Ray Sidecar
////////////////////

ECS + X-Ray: Example Task Definition
////////////////////
Three key parts of making the Task Definition work. Slide has full script
Port Mappings: sets containerPort to 2000, protocol to udp // This is for sidecar pattern
Environment: sets AWS_XRAY_DAEMON_ADDRESS to xray-daemon:2000
Links: xray-daemon
////////////////////
//////////////////////////////////////////////////////////////////////////


AWS Distro for OpenTelemetry
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
AWS-supported distribution of the open-source OpenTelemetry
	SIMILAR to X-Ray, but is open-source
Single set of APIs to:
	Collect traces and metrics from Apps
	Metadata from Resources and Services
	Auto-instrumentation Agents collect traces
	Send data to AWS or third-party Services (X-Ray, CloudWatch, ...)
This is to instrument Apps on AWS/on-premises
Benefits of OpenTelemetry: open-source and can send to multiple destinations simultaneously
////////////////////

Diagram
////////////////////
Slide shows that OpenTelemetry can:
	Collect traces, Collect Metrics, Collect AWS Resources/Contextual Data
and can send that to:
	AWS and third-party monitoring solutions (X-Ray, CloudWatch, ...)
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudTrail
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
CloudTrail is mainly used to track dev user activity in the AWS account

CloudTrail is a way to track events and API calls from:
	Console, SDK, CLI, AWS Services
Logs stored in CloudWatch Logs or S3
Trail can be single or all Regions

Ex: resource is deleted mysteriously, check CloudTrail to see who
////////////////////

Diagram Summary
////////////////////
Slide Diagram shows Services going into CLoudTrail Console
	SDK, CLI, Console, IAM User/Roles
And then going to CloudWatch Logs and S3
////////////////////

CloudTrail Events
////////////////////
There are three types of CloudTrail Events

Management Events: Operations on Resources (standard events anyone would do)
	Automatically enabled, and can separate Read/Write Events
Data Events: very frequent Service operations (Lambda exe, S3 GetObject, ...)
	NOT on by default (high volume)
CloudTrail Insight Events (see next section)
////////////////////

CloudTrail Insights
////////////////////
Detects unusual Events, based on normal management events
	Analyzes WRITE Events for unusual Events
Ex: inaccurate resource provisioning, burst of AWS IAM actions, ...
Slide Diagram shows:
Management Events analyzed by CloudTrail Insights and these are sent to:
	CloudTrail Console / S3 / EventBridge
////////////////////

CloudTrail Events Retention
////////////////////
Standard TTL is 90 days
To keep longer, these get put into S3, and then analyzed w/ Athena
Athena: Serverless Service to query data in the S3
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudTrail - Hands On
//////////////////////////////////////////////////////////////////////////
 > CloudTrail > Event History

In this console, we can see the list of different operations performed.
Opening one of the operations, it shows:
	Time of Event
	User who performed operation
	...
//////////////////////////////////////////////////////////////////////////


Amazon CloudTrail - EventBridge Integration
//////////////////////////////////////////////////////////////////////////
API Calls can be filtered and make specific types cause an Alert to happen

Overview
////////////////////
API Events go to CloudTrail --> Amazon EventBridge
There, we can create a filter and send it to SNS, which will send it out
	This is how we can create Alerts for specific API calls
////////////////////

Diagram
////////////////////
Slide Diagram is just two examples:
	User assumes an IAM Role
	User changes the Ingress Rules for a Security Group
These follow the same path as above, ending w/ SNS sending out Notification(s)
////////////////////
//////////////////////////////////////////////////////////////////////////


CloudTrail vs CloudWatch vs X-Ray
//////////////////////////////////////////////////////////////////////////
Summary comparison of these three

CloudTrail: Audit API calls made by Users/Services/AWS Console
CloudWatch: Metrics, App logs, Alarms for unusual Metrics
X-Ray: Visually trace flow through App to see latency/faults
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 21
  __          __   
 /  \ |  |   /  \  /|  
 |    |__|      /   |   
 |    |  |     /    |  
 \__/ |  |.   /__  _|_  

AWS Serverless: Lambda
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


Serverless Introduction
//////////////////////////////////////////////////////////////////////////
What's Serverless?
////////////////////
Any Service that is used without provisioning a server
////////////////////

Serverless in AWS
////////////////////
Slide Diagram shows Users interacting w/ a Static Site on S3, logging w/ Cognito
	and storing data through an API Gateway -> Lambda Fn -> DynamoDB
LOTS of Services are Serverless
	Lambda, DynamoDB, Cognito, ... // (Not yet seen)
	SNS, SQS, Firehose, Fargate, ... // (Seen before)
////////////////////
//////////////////////////////////////////////////////////////////////////


AWS Lambda Overview
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Lambda:
	Virtual (no server management)
	Only limited by a 15 min execution time
	Runs on-demand
	Auto-Scales to need
////////////////////

Benefits of Using Lambda
////////////////////
Easy Pricing (very cheap price w/ generous free tier)
AWS Service and coding languages integration (including CloudWatch monitoring)
Can get up to 10GB RAM, which can improve CPU and network
////////////////////

AWS Lambda Language Support
////////////////////
LOTS of languages supported (see slide)
	Python and Node.js are the big ones
////////////////////

AWS Lambda Integrations - Main Ones
////////////////////
Lots of integrations, but amins ones are:
	API Gateway, Kinesis, DynamoDB, S3, CloudFront, CloudWatch Events EventBridge,
	CloudWatch Logs, SNS, SQS, and Cognito
////////////////////

Example: Serverless Thumbnail Creation
////////////////////
S3 Bucket get an image, triggers a Lambda Fn
Lambda turns it into a thumbnail, and stores it in a different S3
	And it puts metadata into a DynamoDB
////////////////////

Example: Serverless CRON Job
////////////////////
CloudWatch Events EventBridge every hour triggers a Lambda Fn to perform a task
	Best feature is that when not running it uses no resources
////////////////////

AWS Lambda Pricing: Example
////////////////////
Pricing url: https://aws.amazon.com/lambda/pricing

Pricing breaks down into:
	Pay per calls and Pay per duration
	Very cheap and popular
////////////////////
//////////////////////////////////////////////////////////////////////////


AWS Lambda - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create a super simple Lambda Fn w/ just default premade code

 > Lambda > Functions -> Create Function
-> Use a Blueprint -> Hello World (Python) -> Create Function

Create a standard test

Result: We now have a Lambda Fn that will take in eh parameters of the Test
	And return the first one.

Editing basic settings, we cold change the memory
Logs tab shows the activity of the Fn
//////////////////////////////////////////////////////////////////////////


Lambda Synchronous Invocations
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
This is the standard interaction w/ a Lambda Fn
User or Service invokes the Lambda Fn, which then does something, and sends a response
////////////////////

Lambda Synchronous Invocations - Services
////////////////////
Two main types invoke Lambda: User and Service

User Invoked:
	Elastic Load Balancer, API Gateway, CloudFront, ...
Service Invoked:
	Cognito, Step Fns
Other:
	Alexa, Firehose, ...
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Synchronous Invocations - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Test the Lambda Fn w/ CLI (CloudShell)

Go to CloudShell

Put in command to exe the Lambda Fn:
	aws lambda invoke --function-name *Fn name* 
			  --cli-binary-format raw-in-base64-out 
			  --payload *values to send*
			  response.json

Result: We get back the response from the Lambda Fn in json format
//////////////////////////////////////////////////////////////////////////


Lambda & Application Load Balancer
//////////////////////////////////////////////////////////////////////////
Lambda Integration w/ ALB
////////////////////
Invoke Lambda w/ as an HTTP(S) endpoint using ELB or API Gateway
	Lambda must be in Target Group
////////////////////

ALB to Lambda: HTTP to JSON
////////////////////
Slide shows the script that these will send to the Lambda Fn

Important points are:
	ELB Info (Target group ARN)
	HTTP Method & Path (Ex: /lambda)
	Query String Parameters (Key/Value pairs in JSON format)
	Headers (Key/Value pairs)
	Body (if POST or PUT)
////////////////////

ALB to Lambda: JSON to HTTP
////////////////////
Slide shows the script that these will send to the Lambda Fn

Important points are:
	Status Code & Description (Ex: 200, OK)
	Headers (Key/Value pairs)
	Body & isBased64Encoded
////////////////////

ALB Multi-Value Headers
////////////////////
Supports Mutli-Value headers from the url
and converts them into JSON array
Ex:
	url/path?name=foo&name=bar -> {"name":["foo", "bar"]}

Slide shows the url coming from the Client to the ALB
ALB converts the URL headers, and sends them on to the Lambda
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda & Application Load Balancer - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Place a Lambda Fn behind an ALB
	We do this so we have a client-facing interface for the Lambda

Create a new ALB:
 > EC2 > Load Balancers -> Create Load Balancer
	Set name
	Set as Internet-Facing
	Set as IPv4
	Deploy in three AZs
	Create New Security Group
		Set name
		Allow all inbound HTTP IPv4 traffic
	Include the new Security Group
	Create a Target Group
		Select Lambda
		Set name
		Select an existing Lambda
		-> Create Target Group
	Select new Target Group
	-> Create Load Balancer
Set the Lambda to return a good response to the ALB:
	Can find the correct response syntax in the docs
	The Header Content-Type: "text/html"
	This lets the receiving ALB to output the content from Lambda to the browser

More can be seen from the logs in the Lambda
Multi-Headers can also be enabled, though this requires modifying code as well
The ALB can be found under the Configuration tab as a Trigger
Under Permissions, the ALB has the Permissions to invoke the Lambda
//////////////////////////////////////////////////////////////////////////


Lambda Asynchronous Invocations & DLQ
//////////////////////////////////////////////////////////////////////////
DLQ (Dead Letter Queue)
Basically, use DLQ to track and retry failed Lambda executions

Lambda - Asynchronous Invocations
////////////////////
A Lambda that has an error will retry three times w/ continual back-off:
	Immediate, 1 min, 2 min
Processing needs to be idempotent (result should be the same in case of retries)
Use a DLQ if the retries don't work
////////////////////

Lambda - Asynchronous Invocations - Services
////////////////////
There are LOTS of Services that can asynchronously trigger a Lambda:
	S3, SNS, CloudWatch/EventBridge, ... (see slide)
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Asynchronous Invocations - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Invoke the Lambda Asynchronously, let it fail, retry, and then post a message to a DLQ

 > Lambda > *Fn name*

Put in the command to run the Lambda:
	aws lambda invoke \
  --function-name lambda-to-dlq  \
  --invocation-type Event \
  --cli-binary-format raw-in-base64-out \
  --payload '{ "key1": "value" }' response.json

We can't see the result, so we go to CloudWatch Logs

Changing the Lambda code to fail looks the same in the CLI
	(Both times getting the 202 result)
The CloudWatch Logs do show the Error result

Set up the DLQ:
Go to the Configuration tab -> Asynchronous Invocations (left sidebar)
-> Edit
Set the Dead Letter Queue to SQS
Create a new SQS:
	 > SQS -> Create new SQS
	Set name
Assign the SQS as the Lambda's DLQ
// We now get Permissions issues with the Lambda not being able to write to SQS
Change the Permissions of the Lambda's Role:
	 > Lambda > Functions > *Lambda name* > Configuration tab > General Configuration
	-> Edit
	Go to the bottom, and select the Role
	Assign the SQS-All policy
-> Save
Run the Lambda in CloudShell again

Result:
AFTER A MINUTE, there is now a message in the SQS w/ the data that the Lambda received
//////////////////////////////////////////////////////////////////////////


Lambda & CloudWatch Events/EventBridge
//////////////////////////////////////////////////////////////////////////
We can create an EventBridge Rule that will trigger a Lambda Fn every interval
Or, when there is an Event
//////////////////////////////////////////////////////////////////////////


Lambda & CloudWatch Events/EventBridge - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create an interval Event in EventBridge that will trigger a Lambda Fn

Create a basic Lambda Fn:
 > Lambda > Functions -> Create Function
	Set name
	Set to Nodejs
	-> Create
Create a new EventBridge Rule:
 > Amazon EventBridge > Rules -> Create Rule
	Set name
	Select Schedule
	-> Continue to Create Rule
	Select schedule that runs at a regular rate
	Set to Every One Minute
	-> Next
	Select Target: Lambda
	Select the new Lambda Fn
	-> Next -> Next -> Create Rule
// Teacher's slides showed the EventBridge appear as a trigger for the lambda, but this did not happen
// It looked like the Lambda Fn was even being triggered, but I just manually added EventBridge as a trigger
	
See the Event details in the CloudWatch Logs:
 > Lambda > Functions > *Fn name*
Add a print line to the Fn, which prints the event it receives

Result: The Scheduled Event triggers the Lambda Fn, and can be seen in the CloudWatch Logs
	The Event also shows up, with those details
//////////////////////////////////////////////////////////////////////////


Lambda & S3 Event Notifications
//////////////////////////////////////////////////////////////////////////
S3 Events Notifications
////////////////////
S3 can trigger an Event based on any changes to its objects
W/ these, the S3 can trigger:
	SNS, SQS, Lambda, ...

////////////////////

Simple S3 Event Pattern - Metadata Sync
////////////////////
S3 Triggering a Lambda is asynchronous
The Lambda is able to do some work on the item from the S3 bucket, and then store it in a DB
	The DB is usually DynamoDB or RDS
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda & S3 Event Notifications - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: An S3 Bucket will trigger a Lambda Fn when an object is uploaded

Create a Lambda Fn:
 > Lambda > Functions -> Create Function
	Set name
	Set runtime as Python
	Edit code to include "print(event)"

Create an S3 Bucket:
 > S3 -> Create Bucket
	Set name
	-> Create Bucket
Add an Event Notification to the S3:
	Go to the Properties tab > Event Notifications section
	-> Create Event Notification
		Set name
		Event Types > Select the All Object Create Events
		Destination > Lambda Fn
		Select the Lambda Fn
Now the Lambda Fn function Overview shows that the S3 oes invoke the Lambda

Result: Uploading any file to the S3 will trigger the Lambda Fn, and this can be seen in CloudWatch Logs
	Because of the print(event) line, we can see the details as well
//////////////////////////////////////////////////////////////////////////


Lambda Event Source Mapping
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
There are three Services Lambda can Synchronously poll for data:
	Kinesis Data Streams, SQS & SQS FIFO Queues, and DynamoDB Streams
These Services get data, and that triggers them to call the Lambda Fn
////////////////////

Streams & Lambda (Kinesis & DynamoDB)
////////////////////
Event Source Mapping:
	In-order iterator for each Shard
	From beginning or timestamp
	Doesn't delete items
	Can handle multiple (up to 10) batches in parallel
Slide diagram shows how a shard (composed of batches) is sent to a Service,
and then then broken up to be sent across several Lambda Fns for parallel processing
////////////////////

Streams & Lambda - Error Handling
////////////////////
Normally, an error would disrupt Synchronous processing, but we can't allow that
	Reprocess entire batch
	Pause until error resolves
////////////////////

Lambda - Event Source Mapping SQS & SQS FIFO
////////////////////
We can set up SQS and SQS FIFO to send items to a Lambda Fn
This stays efficient by using long polling, batch messages (of 1 to 10),
Ensure completion w/o repeats by setting the queue visibility timeout to 6x the Lambda Fn timeout
To stay synchronous, the Lambda's DLQ must be on the SQS or through the Lambda's Destinations
////////////////////

Queue & Lambda
////////////////////
W/ a standard SQS, Lambda scales up quickly to match workload
Repeats can happen, even w/o an error, though errors are more likely to cause out of order processing
Lambda deletes items from queues after successful processing
////////////////////

Lambda Event Mapper Scaling
////////////////////
Kinesis Data Streams & DynamoDB Streams:
	One Lambda per shard, and w/ parallelization 10 batches per shard
SQS Standard:
	Lambda scales up 60 Instances per minute, and can process 1000 batches of messages simutaneously
SQS FIFO:
	Messages w/ the same GroupID are processed in-order, scales up to number of these groups
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Event Source Mapping - Hands On (SQS)
//////////////////////////////////////////////////////////////////////////
Goal:

 > Lambda -> Create Lambda
Create Lambda:
	Set name
	Set to Python
	-> Create Function
Create SQS:
	Set name
	-> Create Queue
Allow Lambda to retrieve messages:
	 > Lambda > Functions > *Fn name*
	Add trigger
	Select SQS
	Select the queue
	Set batch size to 10
	Enable trigger
	-> Add // We then get an error saying that the Lambda Fn doesn't have Permissions
	Add Permissions:
		 > Lambda > Functions > *Fn name* > Configurations tab > Permissions
		-> Role name
		-> Attach Policies -> AWSLambdaSQSQueueExecutionRole
	-> Add
Change Lambda code:
	Add line: print(event)
	Edit lines for the return to: return "success"
Test by sending message to SQS Queue
	> Amazon SQS > Queues > *queue name* > Send and Receive Messages
	Enter Message Body
	Message attributes: "foo" - - "bar"
	-> Send Message
	 > Lambda > Functions > *Fn name* > Monitor -> CloudWatch Logs > *Log Stream*
Check that the queue is now empty:
	> Amazon SQS > Queues
	The queue now has 0 Messages
Disable SQS:
	 > Lambda > Functions > *Fn name* > Configuration > Triggers
	Select queue -> Edit
	Disable Active

Important Kinesis Trigger Options:
	Kinesis Stream: the Kinesis Stream to use
	Consumer
	Batch Size
	Batch Window
	-> Enable Trigger (Active)	
//////////////////////////////////////////////////////////////////////////


Lambda Event & Context Objects
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
When a Service like EventBridge triggers a Lambda Fn, it sends an Event and Context as parameters
Both of these have a lot of information (see slide)
////////////////////

Event vs Context
////////////////////
Events: Contains data for the Lambda Fn to process
	JSON-formatted data, turned into a dictionary hat has input arguments
Contexts: Contains data about the invocation and the Service that invoked the Lambda Fn
	aws_request_id, function_name, ...
////////////////////

Event & Context Using Python
////////////////////
In Python, the lambda_handler has an event and context arguments:

def lambda_handler(event, context):
	// Fn body
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Destinations
//////////////////////////////////////////////////////////////////////////
A Lambda Fn can send results to another AWS Service:
	SQS, SNS, Lambda, EventBridge
The Destinations are broken into two categories, failed or success
	So we specify where the message goes based on if was a success or failure
//////////////////////////////////////////////////////////////////////////


Lambda Destinations - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create two Queues, one for success, and one for failure.
	Have the lambda do both, and make sure that they get results when they should
The Lambda should have another Service triggering it, either S3 or SQS
Create the Success and Failure Queues:
 > SQS -> Create Queue // Just do this twice to get both
	Set name
	 -> Create
Assign the Lambda the Queues:
 > Lambda > Functions > *lambda function name* -> Destinations
	Assign the failure and success queues
Test the success event from Lambda:
	Place a message/object into the Service
	There is now a message in the success queue
Test the failure event from Lamba:
	Edit the code to replace the response fn with raise Exception("Boom!")
	Place a message/object into the Service
	There is now a message in the failure queue
Result:
	Now, no success and failure, we get the message into the correct AWS Service
//////////////////////////////////////////////////////////////////////////


Lambda Permissions - IAM Roles & Resource Policies
//////////////////////////////////////////////////////////////////////////
Lambda Execution Role (IAM Role)
////////////////////
These grant the Lambda Fn Permissions to access other AWS Services
There are a lot of managed policies already (see slide)
This Role is what the Event Source Mapping uses, and there should be just one per Fn
////////////////////

Lambda Resource Based Policies
////////////////////
These grant other Services the Permissions to invoke Lambda Fns
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Permissions - IAM Roles & Resource Policies - Hands On
//////////////////////////////////////////////////////////////////////////
Going to the IAM Roles we can see the ones made for each Lambda Fn
 > IAM > Roles
Clicking on one of these, the attached Policies indicate what Service can invoke the Fn
	The Policy will have the specific Service in the Conditions section
The Policies will also have the Services that the Lambda Fn can invoke


Going the other way, each Service must have a Policy that allows it to invoke the Lambda Fn
	In the case of an SQS Queue, the Lambda Fn will be the one w/ Permissions to poll and delete
//////////////////////////////////////////////////////////////////////////


Lambda Environment Variables
//////////////////////////////////////////////////////////////////////////
Lambda Fns can have Environment Variables
	These can be encrypted
//////////////////////////////////////////////////////////////////////////


Lambda Environment Variables - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create Environment Variables for a Lambda Fn

If needed, create a Lambda Fn
Go to the Lambda Fn
 > Lambda > Functions > *lambda Fn name*
	Add the lines to the code bring in the Environment Variables and output them
		import os // This for Python
		return os.getenv("ENVIRONMENT_NAME")
	Add the Environment Variables under Configuration tab
		> Environment Variables -> Edit -> Add Environment Variable
		Key: "ENVIRONMENT_NAME"
		Value: "dev"
		-> Save
Test the Fn
	We get a response w/ "dev" returned
Goin back to the Environment Variable -> Edit, we change the Vale to "prod"
	Testing the Fn, we now get "prod" in the response

Result:
	We now have a variable outside of the Lambda Fn that supplies data
	These are great for things we don't want to be accessible to the public		
//////////////////////////////////////////////////////////////////////////



Lambda Monitoring & X-Ray Tracing
//////////////////////////////////////////////////////////////////////////
Lambda Logging & Monitoring
////////////////////
CloudWatch Logs:
	Execution Logs for Lambda Fns
CloudWatch Metrics:
	Detailed Logs of each execution (duration, invocations, ...)
////////////////////

Lambda Tracing with X-Ray
////////////////////
Includes the Lambda Fn in the X-Ray diagram
Must have X-Ray Permissions, and the Fn's code must use the X-Ray SDK
There are three Environment Variables w/ X-Ray:
	_X_AMZN_TRACE_ID:         tracing header
	AWS_XRAY_CONTEXT_MISSING: default is LOG_ERROR
	AWS_XRAY_DAEMON_ADDRESS:  the X-Ray Daemon IP_ADDRESS:PORT
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Monitoring & X-Ray Tracing - Hands On
//////////////////////////////////////////////////////////////////////////
 > Lambda > Functions > Configuration tab -> Monitoring and Operations Tools
	 > Additional Monitoring Tools -> Edit
	Enable the X-Ray traces
	-> Save
Going to Permissions, we now have that set for X-Ray automatically
Test the Lambda Fn being traced
	Make sure that the Lambda Fn will respond w/ a success
	Upload a new file to the S3 bucket that triggers the Lambda Fn
	Go to  > Lambda > Functions > *function name* > Monitoring
		-> View X-Ray Traces
		-> Traces Map now shows the flow of the Lambda Fn being executed
			Clinet -> S3 -> Lambda Fn
//////////////////////////////////////////////////////////////////////////


Lambda@Edge & CloudFront Functions
//////////////////////////////////////////////////////////////////////////
Customization At the Edge
////////////////////
Lambda Fns can be deployed at CloudFront Edge Locations for super-fast responses
There are two types
	CloudFront Functions, Lambda@Edge
They are serverless, pay per use, and deployed globally
////////////////////

CloudFront Functions & Lambda@Edge Use Cases
////////////////////
There are a LOT of use cases for these (see slide)
Some are security, SEO, and bot mitigation at the Edge
////////////////////

The next two sections both show a diagram where:
	Client makes request to CloudFront
	CloudFront makes request to Origin
	Origin sends response to CloudFront
	CloudFront sends response to Client

CloudFront Functions
////////////////////
Super lightweight Fn that lives at the Edge Location
	Used to change the requests and responses of the viewer to/from CloudFront
	Used for high-scale, latency-sensitive apps
	Written in javascript
////////////////////

Lambda@Edge
////////////////////
Lightweight Fn that can change any of the requests/responses in the diagram:
	Client request/response to/from CloudFront
	CloudFront request/response to/from Origin
Not as fast as CloudFront Fns
Create in Region, auto-created in all Regions
Written in NodeJS or Python
////////////////////

CloudFront Functions vs Lambda@Edge
////////////////////
See slide for details
Big differences are:
	Speed: CloudFront Fns < 1ms, Lambda@Edge 5-10s
	Memory: CloudFront 2MB, Lambda@Edge up to 10GB
	Access to Origin files/Request Body: CloudFront Fn no, Lambda@Edge yes
////////////////////

CloudFront Functions vs Lambda@Edge Use Cases
////////////////////
Lots of uses for both of these (see slide)
CloudFront Fns:
	Caching data, Header manipulation, URL rewrite/redirect, auth
Lambda@Edge:
	Longer execution time, more memory, access to Origin files, 3rd party libraries
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda in VPC
//////////////////////////////////////////////////////////////////////////
Lambda by Default
////////////////////
By default:
	Lambda is deployed in AWS Cloud, outside of the VPC
	It can access other Services in the AWS Cloud, but not ones in a VPC
////////////////////

Lambda n VPC
////////////////////
Lambda can be deployed in a VPC:
	Define VPC ID, Subnets and Security Groups
	The Lambda creates an ENI(Elastic Network Interface), which accesses the Services in the VPC
	Use AWSLambdaVPCAccessExecutionRole
////////////////////

Lambda in VPC - Internet Access
////////////////////
Just as we've seen before with EC2 Instances in private Subnets, 
Lambdas, even deployed in public Subnets require:
	A NAT in the public Subnet, to connect to the Gateway
	Communicating w/ other Services as URLs, or creating a VPC Endpoint
CloudWatch Logs will still work w/o any of these
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda in VPC - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create a Lambda Fn inside of the VPC, and give it Internet access
 > Lambda > Functions -> Create Function
	Set name
	-> Create Function
Create Security Group
	 >EC2 > Security Groups -> Create Security Group
	Set name
	Set Description
Edit Lambda Fn to have VPC
 > Lambda > Function > Configuration tab -> VPC
Select default VPC
Choose a few Subnets
Select the Security Group
-> Save
// We get an error that the Role does not have needed Permissions (CreateNetworkInterface)
Give the Lambda Fn Permissions
Lambda > Functions > "VPC Lambda name" -> Configuration tab -> Permissions
	-> *Lambda Role*
	-> Add Permissions
	Search for "eni"
	Select the AWSLambdaENIManagementAccess
	// This has all of the Permissions needed to create an Internet connection
-> Save
Run the test // Might take a few minutes b/c things are being updated
Verify that a network interface has been created for each AZ the Lambda is deployed in
 > EC2 > Network Interfaces on sidebar

Result:
	We now have a Lambda Fn deployed within the public/private VPC, and still access the Internet
//////////////////////////////////////////////////////////////////////////


Lambda Function Performance
//////////////////////////////////////////////////////////////////////////
Lambda Fn Configuration
////////////////////
Lambda can scale up to 10GB of RAM, and run for up to 15 minutes
The RAM is allocated in vCPUs of 1,792MB
	For each of these, multi-threading will be needed to benefit from the RAM
	This uses the /tmp directory
////////////////////

Lambda Execution Context
////////////////////
The Execution Context is a re-used context that is executed outside of the handler function
	This is how we can establish a DB connection, and save it for future calls!
////////////////////

Initialize Outside the Handler
////////////////////
Ex in slide shows the DB connection inside of the handler Fn - this is bad!
Fix: Put the DB connection outside of the handler, and if the connection exists, return it
////////////////////

Lambda Fns /tmp Space
////////////////////
We can use the /tmp space to write temporary files, up to 10GB
	For any bigger or more persistent data, use S3
	For Encryption in /tmp, use KMS
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Function Performance - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: See the execution of a Fn outside of the handler Fn be sow the first time, and fast after

Edit the Lambda Fn to have a delay Fn outside of the handler Fn
 > Lambda > Configuration tab
Check out the Memory section
	The more memory, the more expense
	This is how to get more cores too
Edit the code to simulate some work
	Import the time library (Python)
		import time
		Run the test
	Add the line
		time.sleep(2)
	// We see the Duration result be about 2 seconds
	Change the time to 5
		time.sleep(5)
		// The Lambda Fn failed out, b/c it exceeded the timeout
	Chang the timeout to 6
	 > Lambda > Configuration tab > General Configuration -> Edit
		Change the timeout to 6 seconds
		-> Save
	Run the test again
		Succeeds after about 5 seconds
Move work outside of the handler Fn6
	Go back to code
	Move the sleep line above the handler Fn // must be above
		Put it into a Fn, and place a call to that Fn below it
		///////
			def connection-work():
				sleep(5)
			connection-work()
		///////
The first time takes the duration, but the following ones don't
//////////////////////////////////////////////////////////////////////////



Lambda Layers
//////////////////////////////////////////////////////////////////////////
A Lambda Layer is a place where we store dependencies that our Lambda Fn(s) rely on
	Reuse across versions of the Lambda Fn, and across different Lambda Fns
Lambda Layers can create custom runtimes (other coding languages)
//////////////////////////////////////////////////////////////////////////



Lambda Layers
//////////////////////////////////////////////////////////////////////////
Goal: Add and use a Lambda Layer to a Lambda Fn

 > Lambda > Functions
Create a Lambda Fn w/ Python
-> *Fn name*
Add the following code to the Lambda Fn:
///////
import pandas as pd

def lambda_handler(event, context):
	data = {
		"Name":["Alice", "Bob"],
		"Age": [20, 21]
	}
	df = pd.DataFrame(data)
	filtered_df = df[df["Age"] > 20]
	result = filtered_df.to_dict(orient="records")
	return result
///////
Run the test on the Lambda Fn
	// We get an error saying it is unable to import a module named pandas
Add the pandas Layer to the Lambda Fn:
	Click on the Layers box beneath the Lambda in the Function Overview window
	-> Add a Layer
	Select the Pandas from the dropdown
	-> Add
Run the test again
	It now succeeds, bringing back the requested data
Result: By adding the pandas Layer to the Lambda Fn, we imported that library
//////////////////////////////////////////////////////////////////////////



Lambda Files Systems Mounting
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Lambda Fns can connect to an EFS
	Just make sure that the EFS isn't having its connection limits hit
////////////////////

Lambda - Storage Options
////////////////////
/tmp:
	Ephemeral storage for Lambda Fn while it runs, fastest retrieval
Lambda Layers:
	Additional runtimes or libraries
S3:
	Persistent storage, fast
EFS:
	Persistent storage, faster
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Concurrency
//////////////////////////////////////////////////////////////////////////
Lambda Concurrency and Throttling
////////////////////
Lambda Fns can scale up to 1000 concurrent executions, but we should limit this
	A Throttle is any execution over the limit
	Throttle behavior:
		synchronous: return ThrottleError - 429
		asynchronous: auto-retry and then DLQ
We can actually open a support ticket w/ Amazon for a higher limit
////////////////////

Lambda Concurrency Issue
////////////////////
B/c concurrency is counted against by all functions in the Account, one Service can have them all
	This means the at the other Services downstream in the workflow can't reserve any
	But as those fail, the upstream Services keep taking up the limit...
By limiting the concurrency of each Service, we solve this
////////////////////

Concurrency and Asynchronous Invocations
////////////////////
Throttling events just mean that they are auto-retried until they work
	Or till the time limit is reached
	Attempts are made, up until 6 hours
	Retry interval increases from 1 second to 5 minutes
////////////////////

Cold Starts & Provisioned Concurrency
////////////////////
Cold Start: the Lambda Fn has to run initializations on first run
Provisioned Concurrency: Allocate instances of the Lambda Fn before its invoked
Note:
	See slide for AWS blog post about reducing Cold Start latency
////////////////////

Reserved and Provisioned Concurrency
////////////////////
See slide for graphs comparing Concurrency with and without Reserved Concurrency
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Concurrency - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Explore the different Concurrency settings for Lambda Fns

 > Lambda > Functions > *Fn name* > Configuration tab -> Concurrency sidebar
-> Edit
Set Reserved Concurrency: 0
-> Save
Try running a Test on the Fn
	This fails, saying that Rate Exceeded
Set the Concurrency back to use the default Concurrency

Add Provisioned Concurrency
In the Provisioned Concurrency Configurations section
	-> Add
	Set the Qualifier type
	Set the number of Concurrencies to provision // This is not free
//////////////////////////////////////////////////////////////////////////


Lambda External Dependencies
//////////////////////////////////////////////////////////////////////////
Lambda Dependencies (external libraries) can be zipped together w/ the Fn, and then uploaded
	Files larger than 50MB need to be uploaded to S3 first
//////////////////////////////////////////////////////////////////////////


Lambda External Dependencies - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create a Lambda F from CloudShell, zip it, and upload it to the Lambda Fn of the Account

Open up the CloudShell console
 > CloudShell
create a folder called lambda
	mkdir lambda
Navigate inside
	cd lambda
Install nano
	sudo yum install-y nano
Paste in the code:
///////
const AWSXRay = require('aws-xray-sdk-core')
const AWS  = AWSXRay.captureAWS(require('aws-sdk'))
const s3 = new AWS.S3()

exports.handler = async function(event){
        return s3.listBuckets().promise()
}
///////
Verify that index.js has the code
	cat index.js
Install dependencies (x-ray, s3, aws)
	npm install aws-xray-sdk
	npm install aws-s3-sdk
	npm install aws-sdk
Change the Permissions of these files
	chmod a+r *
Zip the files
	zip -r function.zip .
Create a Role for the Lambda Fn
 > IAM > Roles -> Create Role
	Select AWS Service
	Select Lambda from the Dropdown
	-> Next
	Search with "lambdabasic" and select the AWSLambdaBasicExecutionRole
	-> Next
	Set a name
	-> Create Role
Back in CloudShell, create the Lambda Fn
	aws lambda create-function 
	--zip-file fileb://function.zip 
	--function-name lambda-x-ray-with-dependencies
	--runtime nodejs16.x // This may be out of date, just look up what the current version is
	--handler index.handler
	--role *arn of the new Role*

Ex:
aws lambda create-function --zip-file fileb://function.zip --function-name lambda-x-ray-with-dependencies --runtime nodejs18.x --handler index.handler --role arn:aws:iam::160579864932:role/AquinDemoWithDependencies

We now have a new Fn
 > Lambda > Functions -> lambda-x-ray-with-dependencies

Running a basic test will result in an access-denied error due to not having the S3 and X-Ray Permissions
Give the Lambda Role X-Ray Permissions:
 > Lambda > Functions > *Fn Name* > Configuration -> Monitoring and Operations Tools sidebar
	> Additional Tools section -> Edit
	 > CloudWatch Application Signals and AWS X-Ray section
		Enable: Lambda Service Traces
Add the needed Permissions to the Lambda Role
 > Lambda > Functions > *Fn Name* > Configuration -> Permissions sidebar
-> *Role Name*
	// We see the X-Ray Permissions are there b/c enabling X-Ray brought those in
	-> Add Policies
		Search for "S3Read" and select AmazonS3ReadOnlyAccess
		-> Add Permissions
Run the Lambda test again
It succeeds!

Check the X-Ray:
 > X-Ray > Traces sidebar
	Select the Lambda Fn and run a query
 > X-Ray > Trace Map sidebar
	// We should now see the diagram of the Lambda Fn Service

Result:
	We now have a Lambda Fn created from the CloudShell CLI
	It can read from S3, and it has traces in X-Ray
//////////////////////////////////////////////////////////////////////////


Lambda and CloudFormation
//////////////////////////////////////////////////////////////////////////
Lambda and CloudFormation - Inline
////////////////////
We can create simple Lambda Fns w/o Dependencies using zipped code files
////////////////////

Lambda and CloudFormation  through S3
////////////////////
We can store the Lambda zip in S3, using three attributes:
	S3Bucket - Where the Lambda is stored
	S3Key - Full path to zip
	S3ObjectVersion - If Bucket has versioning (highly recommended)
To update the Lambda, we must update all three attributes or CloudFormation won't update
////////////////////

Lambda and CloudFormation through S3 Multiple Accounts
////////////////////
To deploy a Lambda Fn from an S3 Bucket in Account1 using a Bucket Policy
	These Accounts do need an Execution Role in place that the Bucket Policy grants access
	This allows other Accounts to access the Lambda Fn and deploy it to their CloudFormation
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda and CloudFormation - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create a Lambda Fn for CloudFormation

Download the lambda-xray-template.yaml file
	Go to the code for the course, and download the code folder
Create the S3 Bucket to store the Lambda Fn
 > S3 -> Create Bucket
	Set name
	Enable Bucket Versioning
Download the function.zip from the tutorial creating a Lambda through CloudShell
 > CloudShell
Get the full path to function.zip
	pwd
	Copy the result
	-> Actions
	-> Download file
	Paste in the path and add "/function.zip"
	-> Download
Upload function.zip to the S3
Create a Stack on CloudFormation
 > CloudFormation -> Create Stack (with new resources)
	Select: Upload Template
	-> Choose File
	Upload the lambda-cloudformation.yaml file from the course code files
	Set name
	-> Next
	S3: the Bucket name
	Key: "function.zip"
	Version: > S3 > Buckets > *bucket name* > Objects > *function zip file* > Versions tab
		 Copy the version
	Acknowledge that CloudFormation will create things
	-> Next
	-> Submit
// CloudFormation needs a few minutes to create itself
Check the Lambda Fn made by CloudFormation
Run a basic test
Check the Trace in X-Ray

Result:
	We now have a Lambda Fn created by CloudFormation
//////////////////////////////////////////////////////////////////////////


Lambda Container Images
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
We can deploy a Lambda Fn as a Docker Container
	Can be big, up to 10GB, with dependencies, lots of languages
	Test Containers locally w/ Lambda Runtime Interface Emulator
Slide has example that specifies an:
	image, app code, dependencies, and Lambda handler Fn to run
////////////////////

Lambda Container Images - Best Practices
////////////////////
Best Practices:
	Use AWS-provided Base Images
	Use Multi-Stage Builds
	Build from Stable to Frequently Changing
	Use a single Repo for Fns w/ Large Layers
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Versions and Aliases
//////////////////////////////////////////////////////////////////////////
AWS Lambda Versions
////////////////////
Versions are  way to update the Lambda Fn
	Lambda Fns can be change, but published Versions can't, they can only be replaced
	Immutable: Can't be changed
	Each Version has its own ARN
////////////////////

AWS Lambda Aliases
////////////////////
Aliases are pointers to Lambda Fn Versions
	Aliases are of course mutable
	To test a new Version, we can direct % of traffic to the new Version (Canary Deployment)
	Aliases can't reference other Aliases
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Versions and Aliases - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create a Lambda Fn w/ multiple Versions, and weight them to simulate Canary Testing

Create the Lambda Fn:
 > Lambda > Functions -> Create Function
	Select: Author from Scratch
	Set name
	Set Runtime to Python
	-> Create Function
Change the Lambda code to return "This is Version 1"
	Deploy and test
Author the Fn as Version 1
	-> Actions dropdown -> Publish New Version
		-> Publish
Edit the code to return "This is Version 2"
 > Lambda > Functions > *Fn name* // After publishing we are one level too deep, at the Version 1
Edit the code
Deploy and Test
Author the Fn as Version 2
	-> Actions dropdown -> Publish
Change the weight of traffic using Aliases
Go back to the Lambda Fn
 > Lambda > Functions > *Fn name*
	Under the Versions tab, we can see our Versions
Create an Alias for each Version
-> Aliases tab -> Create Alias
	Set name: prod/dev
	Set the Version
	-> Create
Test out the code
Add weights to the Aliases in a 50/50 split
 > Lambda > Functions > *Fn name* > Aliases tab -> *Alias name* -> Edit
Select the other Version
Set the weight to 50%
-> Save
Go to the Alias
 > Lambda > Functions > *Fn name* > Aliases tab -> *Alias name* -> Test
Running the test will eventually show the other Version of the Lambda Fn

Result:
	The Alias will direct traffic to each of the Versions
//////////////////////////////////////////////////////////////////////////


Lambda - CodeGuru Integration
//////////////////////////////////////////////////////////////////////////
Lambda and CodeGuru Profiling
////////////////////
We can set up Lambda w/ CodeGuru profiling
	Profiles runtime performance
	Supports Java, Python
	Must have Permissions: AmazonCodeGuruProfilerAgntAccess Policy
////////////////////
//////////////////////////////////////////////////////////////////////////


Lambda Limits
//////////////////////////////////////////////////////////////////////////
Lambda is limited by:
	Execution: Memory, Execution Time, Concurrent Fns, ...
	Deployment: Deployment size, Env Variables size, ...
Anything beyond these limits is not a use case for Lambda Fns
See slide for details
//////////////////////////////////////////////////////////////////////////


Lambda Best Practices
//////////////////////////////////////////////////////////////////////////
(See slide for details)
Perform one-time work outside of the Handler Fn
	DB connections, initializations, ...
Env Variables
	Usernames/passwords, connection URLs, ...
Minimize deployment packages to runtime necessities
	Use layers, Keep within limits, ...
NEVER use code that calls itself
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 22
  __          __   __
 /  \ |  |   /  \ /  \
 |    |__|      /    /
 |    |  |     /    /
 \__/ |  |.   /__  /__

AWS Serverless: DynamoDB
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Overview
//////////////////////////////////////////////////////////////////////////

Traditional Architecture
////////////////////
Rigidly predefined tables in an RDBMS DB that can only scale vertically
////////////////////

Not Only SQL Databases (NoSQL DB) // Terrible shortened name...
////////////////////
DynamoDB is a NoSQL DB
NoSQL:
	Better for horizontal scaling (SQL DBs need vertical scaling)
	Unstructured data (More like JSON, SQL DBs require pre-defined tables)
	Limited support for query JOINs and no aggregations like SUM
////////////////////


Amazon DynamoDB
////////////////////
NoSQL DB with auto-scaling, able to scale to 1000s of TB of storage
Highly available, Fast performance, and able to handle millions of request per second
Storage Tiers: Standard and Infrequent Access
////////////////////

DynamoDB - Basics
////////////////////
DB is made of Tables
	Each Table needs a Primary Key (decided at runtime)
	Unlimited number of rows
	Max Item size is 400KB
////////////////////

DynamoDB - Primary Keys
////////////////////
Key that is unique for each item
	Just use something like *item description*_ID
	Why use anything else?
////////////////////

DynamoDB - Partition Keys
////////////////////
(This is the second option)
Partition Key + Sort Key (HASH + RANGE)
We can use multiple keys to get more specific data:
	User_ID + Game_ID - Get that player's score from that specific game
////////////////////

DynamoDB - Partition Keys (Exercise)
////////////////////
Oh my, which of these is the best, most specific key (sarcasm)
	movie_id, producer_name, lead_actor_name, movie_language
The answer is of course the movie_id
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Basics - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Create a table and fill it with data

> DynamoDB > Tables -> Create Table
	Set name: Users
	Set Partition Key: User_ID
	-> Customize Settings // For learning purposes
	Select Provisioned
	Disable Auto-scaling for Read/Write
	Set Read/Write Provisioned Capacity Units: 2
	// The cost per month only happens if we exceed the free tier
	-> Create Table
> DynamoDB > Tables -> *Table name*
Lots of info here:
	Partition Key, Sort Key, ...
Create an item:
> DynamoDB > Tables > *Table name* -> Explore Table Items -> Create Item
	Set UserID // I dislike that we can set this...
	Create Attribute (String): first_name
		Set first_name
	Create Attribute (String): last_name
		Set last_name
	-> Create Item
Create a second item:
	// Setting User_ID to the same value fails
	Set User_ID (unique)
	Create Attribute (String): first_name
		Set first_name
	-> Create Item
	// The action completes, even w/o a last_name
Create Table:
	Set name: UsersPosts
	Partition Key: User_ID
	Sort Key: Post_TS
	-> Customize Settings
	Select Provisioned
	Disable Auto-scaling for Read/Write
	-> Create Table
Put an Item into UsersPosts:
> DynamoDB > Tables > *Table name* -> Explore Table Items -> Create Item
	Set User_ID to an id from the Users Table
	Sort Key: some timestamp (2021-10-09T12:23:34Z)
	Add Attribute: "Content"
		Set Content: Hello World!
	-> Create Item
Put another Item into UsersPosts:
	Set User_ID to the same id from the Users Table
	Sort Key: some timestamp (2021-11-09T04:04:04Z)
	Add Attribute: "Content"
		Set Content: Second post!
	-> Create Item
// This works b/c there's a difference b/t the User_ID and/or Post_TS

Result: We now have two tables, one storing Users and one Posts
	These aren't linked...yet
//////////////////////////////////////////////////////////////////////////


DynamoDB WCU & RCU - Throughput
//////////////////////////////////////////////////////////////////////////
DynamoDB - Read/Write Capacity Modes
////////////////////
We can provision or use on-demand
Provisioned Mode:
	Provision capacity before use, pay for that capacity
On-Demand Mode:
	Only pay for what gets used
	More expensive for the same capacity provisioned
Can switch b/t these every 24 hours
////////////////////

R/W Capacity Modes - Provisioned
////////////////////
Preset the capacity:
	Read Capacity Units (RCU)
	Write Capacity Units (WCU)
Theses can be exceeded w/ Burst Capacity
	Gives error if Burst Capacity exceeded
		Exponential backoff kicks in
////////////////////

DynamoDB - Write Capacity Units (WCU)
////////////////////
Formula to determine WCU:
	(items per second) * (item size) = (# of WCUs)
Ex 1:
	10 items per second w/ item size of 2KB
	10 * (2KB/1KB) = 20 WCUs
Ex 2:
	6 items per second w/ item size of 4.5KB
	6 * (5KB/1KB) = 30 WCUs // KB always rounded up
Ex 3:
	120 items per minute w/ item size of 2KB
	2 * (2KB/1KB) = 4 WCUs
////////////////////

Strongly Consistent Read vs Eventually Consistent Read
////////////////////
Much like before with replication across Regions
Eventually Consistent Read:
	Default mode
	May get stale data if too fast
Strongly Consistent Read
	More expensive
	Possible latency
	Will be correct w/ data
////////////////////

DynamoDB - Write Capacity Units (WCU)
////////////////////
There are two formulas, eventually and strongly consistent
Round up to multiple of 4, and then divide by 4
Eventually Consistent:
	(1 read per second / 2) * (item size rounded up to 4, divided by 4) = RCUs
Strongly Consistent:
	(1 read per second) * (item size rounded up to 4, divided by 4) = RCUs
Ex 1:
	10 strongly items per second w/ item size of 4KB
	10 * (4KB/4KB) = 10 WCUs
Ex 2:
	16 eventually items per second w/ item size of 12KB
	(16/2) * (12KB/4KB) = 24 WCUs
Ex 3:
	10 strongly items per second w/ item size of 6KB
	10 * (8KB/4KB) = 20 WCUs
////////////////////

DynamoDB - Partitions Internal
////////////////////
Data is broken up by Partition Key
Determined by formulas we don't need to know (see slide)
Important:
	WCUs and RCUs are spread evenly across partitions
////////////////////

DynamoDB - Throttling
////////////////////
Overloading provisioned RCUs or WCUs can get error:
	ProvisionedThroughputExceededException
Reasons:
	Can be one Partition Key read too may times (called a Hot Key)
	Items are large
Solutions:
	Exponential Backoff
	Distribute keys better
	For RCU, use DynamoDB Accelerator (DAX)
////////////////////

R/W Capacity Modes - On-Demand
////////////////////
Scales w/ workload
Charged per read/write
About 2.5x more expensive than same provisioned units

Best for unknown/unpredictable workloads
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB WCU & RCU - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: Check out and edit the RCUs and WCUs of a table

> DynamoDB > Tables > *Table name* > Additional Settings tab (farthest right)
Read/Write Capacity section -> Edit
	On-Demand bills per use, but is more expensive
	Provisioned is predefined, but cheaper
		RCUs are precomputed based on capacities
		Estimated costs are shown as well
		Can set up capacity auto-scaling b/t limits
//////////////////////////////////////////////////////////////////////////


DynamoDB - Basic Operations
//////////////////////////////////////////////////////////////////////////
DynamoDB - Writing Data
////////////////////
PutItem - new item
UpdateItem - update or create
Conditional Writes - Puts conditions on writes, puts, and delete
////////////////////

DynamoDB - Reading Data
////////////////////
GetItem
	Based on Primary Key
	Eventually or Consistent
	... (see slide)
////////////////////

DynamoDB - Reading Data (Query)
////////////////////
KeyConditionExpression
	Partition Key and Sort Key
FilterExpression
	Filters items based on non-key attributes
Returns
	list of Items up to limit or 1MB
Can paginate results
////////////////////

DynamoDB - Reading Data (Scan)
////////////////////
Brings back all Items in Table
Up to 1MB, can use pagination
Parallel Scan - faster performance
////////////////////

DynamoDB - Deleting Data
////////////////////
DeleteItem
	Deletes one Item
DeleteTable
	Deletes entire Table
////////////////////

DynamoDB - Batch Operations
////////////////////
Saves latency by reducing API calls
Done in parallel
Retries failed parts
	Exponential Backoff on failures
BatchWriteItem
	Up to 25 PutItem, and or DeleteItem
	Up to 16MB write, 100KB per Item
	No updates
BatchGetItem
	Up to 100 Items, 16MB of data, one or more Tables
////////////////////

DynamoDB - PartiQL
////////////////////
Can use SQL-like language for doing all API calls
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Basic APIs - Hands On
//////////////////////////////////////////////////////////////////////////
Goal: See how actions in the AWS console translate to the APIs

Create Item is PutItem
Editing an Item is UpdateItem
Selecting an Item is GetItem
Selecting one Item and deleting is DeleteItem
	Selecting multiple Items and deleting is a batch DeleteItem
Selecting the Table and deleting that is DeleteTable

Can filter based on Primary Key and then optionally Sort Key and other Attributes

Result: Each action in the console maps to some API call
//////////////////////////////////////////////////////////////////////////


DynamoDB - Conditional Writes
//////////////////////////////////////////////////////////////////////////
DynamoDB - Conditional Writes
////////////////////
Specify which Items should be affected, using Conditions
	Write APIs (PutItem, UpdateItem, DeleteItem, BatchWriteItem)
	Conditions (attribute_exists, attribute_type, contains, ...) // See slide for more
////////////////////

Conditional Writes - Example on Update Item
////////////////////
Ex: UpdateItem on Items to give DISCOUNT if price greater than LIMIT
	DISCOUNT and LIMIT specified in separate file

Slide has code
////////////////////

Conditional Writes - Example on Delete Item
////////////////////
Ex: DeleteItem if attribute_not_exists is true (price)
Ex: DeleteItem if attribute_exists (ProductReviews.OneStar) // Delete one-star reviews

Slide has code
////////////////////

Conditional Writes - Do Not Overwrite Elements
////////////////////
By using attribute_not_exists, we can check if an Attribute exists w/o creating it

Slide has code
////////////////////

Conditional Writes - Example Complex Condition
////////////////////
Conditions can specify ranges and target multiple values in an Attribute
	Attribute is between 500 and 600
	Attribute can be "Sports" or "Comics"

Slide has code
////////////////////

Conditional Writes - Example of String Comparisons
////////////////////
Strings have special comparisons:
	begins_with // check the start of a String
	contains // String contains a value
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Indexes (GSI + LSI)
//////////////////////////////////////////////////////////////////////////
DynamoDB - Local Secondary Index (LSI)
////////////////////
LSIs are alternative Sort Keys
So we can have our Primary and Sort Keys, and then we can predefine an LSI
	Using this, we can query based on the LSI instead of the Sort Key
	We can have up to 5 LSIs (predefined at Table creation)
////////////////////

DynamoDB - Global Secondary Index (GSI)
////////////////////
Can use a GSI to query a non-key Attribute like a Primary and/or Sort Key
	Can be created/modified after Table creation
	Uses its own RCUs and WCUs
////////////////////

DynamoDB - Indexes and Throttling
////////////////////
GSI: Throttling on GSI throttles the Table
LSI: Uses the RCUs and WCUs of the Table
	No special throttling behavior
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB PartiQL
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
SQL-like query language that allows for
	INSERT, UDATE, SELECT, DELETE
////////////////////

Hands On
////////////////////
> DynamoDB > PartiQL editor
We can choose the actions on the left-hand side to create SQL statements,
	and then modify them from there
Scan Table:
	SELECT * FROM "demo_indexes"
Scan Table w/ Conditions:
	SELECT * FROM "demo_indexes" WHERE "user_id" = '123' AND "game_ts" = 'sortKeyValue'
Lots more we can do
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Optimistic Locking
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
This is where the Item is locked against changes that are requested on an old version

So, if the version is 1, and two Users try to make a change at the same time,
	Both send a change request, directed at the Item, Version 1
	One of them has their change go through, changing the version
	The other change fails b/c the version is now 2
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB DAX
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
This is a cache that saves the most recent queries and returns them when requested again
	Major speed up and protects against throttling
TTL is 5 minutes
10 nodes in Cluster
////////////////////

DynamoDB Accelerator (DAX) vs ElastiCache
////////////////////
DAX:
	Used when querying directly against DynamoDB
ElastiCache:
	Used when querying data that has been modified (summed, aggregated, ...)
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Streams
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Streams of Create/Update/Delete
	Sent to Kinesis Data Streams
	Read by AWS Lambda, Kinesis Client Library apps
Retain data up to 24 hours
Used for:
	Real-time changes
	Inserts
	Analytics
	Cross-Region replication
////////////////////

Architecture Diagram
////////////////////
Slide diagram shows that changes are made to DynamoDB Table
	Triggers Stream output to KDS -> Firehose
	Goes to another AWS Service depending on intention (analytics, indexing, ...)
////////////////////

Data Sent
////////////////////
Data sent can be specified by:
	KEYS_ONLY - Items key Attributes
	NEW_IMAGE - full Item post-modification
	OLD_IMAGE - full Item, pre-modification
	NEW_AND_OLD_IMAGES - full Item, pre and post modification
Shards:
	Streams use Shards
	Created by AWS, and do NOT get created retroactively
////////////////////

DynamoDB Streams & Lambda
////////////////////
To make Lambda read from DynamoDB Streams:
	Event Source Mapping
	Permissions
	Synchronously invoke Lambda
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB TTL
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
We can set a TTL for Items in DynamoDB
	Doesn't consume WCUs to Delete
	Items Deleted from LSIs and GCIs w/i 24 hours
	Item pending Deletion ae sill visible
////////////////////

Hands On
////////////////////
After creating a Table, add an Item w/ a numeric Attribute
	This Attribute will a number set to a time in seconds, since the epoch
		(epoch being time in seconds since Linux was created)
	So the number will be (time since epoch) + (seconds till Item expires)
	We can use an epoch converter to find this
We also have to enable TTL in the Table settings
	This is where we specify the Attribute name that will be checked against the expiration time
In the TTL section of the Table, we can also see how many Items have been deleted from expiration
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB CLI
//////////////////////////////////////////////////////////////////////////
DynamoDB CLI - Good to Know
////////////////////
--projection-expression: Attribute(s) to retrieve
--filter-expression: filter returned Items

General AWS CLI Pagination Options
	--page-size, --max-items, --starting-token (details on slide)
////////////////////

Hands On
////////////////////
Code can be found in the cli-examples.sh file

we can perform the API commands from the CLI, just like from the console
So, we can call the scan API and --projection-expression (from above) w/ variables to retrieve
	Scan w/ --filter-expression and data structured under --expression-attribute-values
	Scan an entire Table, and specify the page to return w/ --page-size
		Or return a specific number of Items w/ --max-items
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Transactions
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
A series of all-or-nothing read/write operations // Either they all work, or they all fail
	ACID: Atomicity, Consistency, Isolation, and Durability
	Consumes 2x the WCUs and RCUs // This adds a prepare to executing the action
Use Cases:
	Online games, Financial software, and other real-time pivotal information....
////////////////////

DynamoDB Transactions - Example
////////////////////
Given the case of a bank account system
	We can see how the bank record Item and the transaction history MUST match
	We cannot have a modified account w/o the history of the latest transaction
	And we cannot have a full history w/o the correct amount in the bank
	This is an all-or-nothing scenario
////////////////////

DynamoDB Transactions - Capacity Computations
////////////////////
We use the same RCU and WCU formulas, but just double the cost

So, 3 Transactional writes per second, size of 5KB:
	(3 per second) * (5KB / 1KB) * (2, transaction cost) = 30 WCUs
5 Transactional reads per second, size of 5KB:
	(5 per second) * (8KB / 4KB) * (2, transaction cost) = 20 RCUs
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Session State
//////////////////////////////////////////////////////////////////////////
DynamoDB as Session State Cache
////////////////////
DynamoDB can utilize being Serverless and highly available to store session state
This makes it one of the best choices, along w/ ElastiCache, and  maybe EFS
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Partitioning Strategies
//////////////////////////////////////////////////////////////////////////
DynamoDB Write Sharding
////////////////////
To prevent a Hot Partition by repeated reads/writes using that key,
	We can instead use a suffix that attaches to the end of the key's value
So for a key of Candidate_ID, we could have candidateA-11, candidateB-13, candidateA-77, ...
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Conditional Writes, Concurrent Writes & Atomic Writes
//////////////////////////////////////////////////////////////////////////
DynamoDB - Write Types
////////////////////
Concurrent Writes:
	Two Users both writing to the same Item can overwrite each other's changes
Conditional Writes:
	The two Users will only do a write operation if the value of the Item is still the same
	So the first User's write will execute successfully, but the second User's won't
	This is also called Optimistic Locking (we already covered this though...)

Atomic Writes:
	The writes happen quickly and so can't overwrite each other
	They also have an additive effect rather than a direct edit
		So, it's adding 5 to the Item rather than rewriting it to 5

Batch Writes:
	Write/Update many Items at once
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Patterns w/ S3
//////////////////////////////////////////////////////////////////////////
DynamoDB - Large Objects Pattern
////////////////////
This pattern is utilizing DynamoDB and S3 for what they're best at:
	DynamoDB: Lots of small objects w/ easy search functions
	S3: Lots of large objects
Here, DynamoDB just has a link to the S3 Bucket where the Item's value is stored
////////////////////

DynamoDB - Indexing S3 Objects Metadata
////////////////////
To create this pattern:
	S3 Bucket sends invokes a Lambda Fn when an object gets uploaded
	Lambda Fn creates an entry into the DynamoDB Table
	Outside apps access the DynamoDB Table to access that Item
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Operations
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Table Cleanup:
	Scan and Delete Item by Item // Slow, WCU expensive
	Drop the Table: Fast, cheap
Copying a DynamoDB Table:
	AWS Data Pipeline: Specialized, only mention in this course
	Backup and restore into new Table: Takes time
	Scan + (PutItem or BatchWriteItem): Have to write own code, not recommended
////////////////////
//////////////////////////////////////////////////////////////////////////


DynamoDB Security & Other
//////////////////////////////////////////////////////////////////////////
DynamoDB - Security & Other Features
////////////////////
Security:
	VPC Endpoints w/o going across Internet
	IAM access
	Encryption at Rest w/ KMS, In-Transit SSL/TLS
Backup and Restore feature available:
	Point-in-time Recovery
	No performance impact
Global Tables:
	Highly available and performant Tables w/ Streams enabled
DynamoDB Local:
	Develop and test locally
Other:
	Migrate w/ AWS DMS from other DBs (DynamoDB and others)
////////////////////

DynamoDB - Users Interact w/ DynamoDB Directly
////////////////////
Have Users access an auth service, and get TEMPORARY IAM credentials
	These credentials allow access directly to the DynamoDB Table
////////////////////

DynamoDB - Fine-Grained Access Control
////////////////////
When Users get assigned the IAM Role, a Condition is put in place to limit access to DynamoDB
The IAM Role's Permissions has a Condition line that only allows access to that User's data
	But the User can do anything else that we allow (GetItem, PutItem, ...)
This line uses the dynamodb:LeadingKeys list to limit row-level access for Users on the Primary Key
Attributes can be limited, so Users can't operate on these
////////////////////
//////////////////////////////////////////////////////////////////////////


//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 23
  __          __    __
 /  \ |  |   /  \  /  \
 |    |__|      /    _/
 |    |  |     /      \
 \__/ |  |.   /__  \__/

AWS Serverless: API Gateway
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway Overview
//////////////////////////////////////////////////////////////////////////
Example: Building a Serverless API
////////////////////
By placing an API Gateway as the entry-point to our app, we get a lot of features:
	Auth, Usage Plans, Dev stages, more...
////////////////////

AWS API Gateway
////////////////////
API Gateway and Lambda means fully serverless

Lots of features for API, auth, and cache
See slide for details
////////////////////

API Gateway - Integrations High Level
////////////////////
Place an API Gateway in front of other Services to use rate limiting, caching, etc
	Lambda Fn, HTTP, Other AWS Service
////////////////////

API Gateway - AWS Service Integration Kinesis Data Streams Example
////////////////////
Place an API Gateway in front of a KDS to provide auth, and not expose the KDS
////////////////////

API Gateway - Endpoint Types
////////////////////
Edge-Optimized (default):
	API Gateway in one Region has requests routed through CloudFront Edge Locations
Regional:
	Clients in same Region
Private:
	Accessed from VPC sing VPC Endpoint (ENI)
////////////////////

API Gateway - Security
////////////////////
User Authentication through:
	IAM Roles, Cognito, Custom Auth
Custom Domain Name HTTPS
	Uses the AWS Certificate Manager (ACM)
	Edge-Optimized, certificate must be in us-east-1
	Regional, certificate must be in API Gateway Region
	MUST have a CNAME of A-alias in Route 53
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateways Stages and Deployment
//////////////////////////////////////////////////////////////////////////
API Gateway - Deployment Stages
////////////////////
Deploy the API Gateway in stages for:
	Making sure changes are pushed
	Rollback is easy
	Configuration at each stage
	Keep version history
////////////////////

API Gateway - Stages v1 and v2 API Breaking Change
////////////////////
In case of deploying a breaking change, like changing the invoked Lambda Fn,
	A new version is put up w/ a new URL, which we gradually migrate Users to
	Once no Users are going to the old URL, it is shut down
////////////////////

API Gateway - Stage Variables
////////////////////
Format: ${stageVariables.variableName}
Like Environment Variables for the Stages of AI Gateway deployment
Passed to the Context object in AWS Lambda

These are used for:
	Lambda Fn ARN, HTTP Endpoint, ...
As the different Stages of the deployment execute, these can change

////////////////////

API Gateway Stage Variables & Lambda Aliases
////////////////////
Each Stage of the deployment can be pointing to a different Lambda Alias
	An Alias can point to a different Lambda Fn, or distribute across multiple Lambda Fns
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway Canary Deployments
//////////////////////////////////////////////////////////////////////////
Basic Canary Deployment, but w/AWS Lambda and API Gateway

We use the different Stages to be our versions of the app
	Test by sending some amount of traffic to the new version
	When confident the new version is good, move over all traffic
//////////////////////////////////////////////////////////////////////////


API Gateway Integration Types & Mappings
//////////////////////////////////////////////////////////////////////////
API Gateway - Integration Types
////////////////////
MOCK:
	API Gateway send response w/o sending request to backend
HTTP/AWS (Lambda & AWS Services):
	Set up Mapping Templates for request and response
	Allows for changing data before it goes to the backend
AWS_PROXY (Lambda Proxy):
	Is a proxy, so requests are sent to Lambda as-is
HTTP_PROXY:
	Sent to HTTP Endpoint, and can add HTTP Header API Key
////////////////////

Mapping Templates (AWS & HTTP Integration)
////////////////////
Allows for editing a request to conform to what the Lambda expects
	Or add to the request
	Uses VTL (Velocity Template Language) for scripts
	Content-Type MUST be application/json OR application/xml
////////////////////

Mapping Example: JSON to XML with SOAP
////////////////////
Use Mapping Template to:
	Convert JSON request into SOAP message for XML
	Handle request as XML
	Convert XML response to JSON 
////////////////////

Mapping Example: Query String Parameters
////////////////////
Use Mapping Template to rename variable in query string to be what Lambda Fn needs
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway Open API
//////////////////////////////////////////////////////////////////////////
API Gateway - Open API Spec
////////////////////
A spec that doubles as the actual code
Can import OpenAPI to API Gateway
////////////////////

REST API - Request Validation
////////////////////
Validates that the request is in the correct format for eh backend
	Checks the request against the configured JSON Schema
	Failures result in a 400-Error response
////////////////////

REST API - Request Validation - OpenAPI
////////////////////
Validations are checked against a JSON OpenAPI file
	Can check parameters only or all validators
Slide has code
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway Caching
//////////////////////////////////////////////////////////////////////////
Caching API Responses
////////////////////
Default TTL is 300 seconds
	Min is 0, Max is 3600 seconds
Cache settings can be overridden per stage
Cache size range: .5GB to 237GB
Cache is expensive, so not for Dev, only Prod
////////////////////

API Gateway Cache Invalidation
////////////////////
Cache can be flushed immediately
Can invalidate cache w/ header CacheControl: max-age:0
Must impose an invalidation policy or anyone can invalidate
////////////////////

Hands On
////////////////////
Go to the Prod Stage and Edit the Stage Details section
We can also choose specific method (Get, Push, etc) and specify the Cache for that

Remember to disable the Cache to stop incurring costs
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway Usage Plans & API Keys
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Lets our customers access and use the API Gateway

Usage Plan:
	Access one or more Stage of the API Gateway
	Settings for customers speeds, throttling limits, and more
	API Keys identify customers
API Keys:
	Large strings that identify customers
////////////////////

API Gateway - Correct Order for API Keys
////////////////////
Steps to configure a usage plan:
1.
	Create API, methods, Stages, and require API Keys
2.
	Generate or import API Keys
3.
	Create usage plan w/ throttle and quota limits
4.
	Associate API Stages and API Keys

Customers supply requests w/ API Key in x-api-key header
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway Monitoring, Logging, and Tracing
//////////////////////////////////////////////////////////////////////////
API Gateway - Logging & Tracing
////////////////////
CloudWatch:
	CloudWatch stores the request going through the API Gateway
	And then it stores the response from the backend
X-Ray:
	Can enable traces so API Gateway and Lambda gives full picture
////////////////////

API Gateway - CloudWatch Metrics
////////////////////
Metrics: by Stage
CacheHitCount & CacheMissCount: efficiency of Cache
Count: Total API requests
IntegrationLatency: Time of request to backend and response
Latency: Time of request to API Gateway and response back to Client, max is 29 seconds
4XXError: Client-side, 5XXError: Server-side
////////////////////

API Gateway Throttling
////////////////////
We need to keep usage in check b/c API Gateways throttle requests at 10,000 rps across ALL APIs
	Error is 429 Too Many Requests
	Must have limits to prevent bad actors from attacking the API Gateway
		At Stage and Method limits
		OR
		In Usage Plans per customer
////////////////////

API Gateway - Errors
////////////////////
Common errors for Client and Server sides

4XX, Client-side:
	400 (Bad request), 403 (Access Denied), 429 (Quota exceeded)
5XX, Server-side:
	502 (Incompatible response), 503 (Service Unavailable), 504 (API Gateway Timeout)
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway CORS
//////////////////////////////////////////////////////////////////////////
AWS API Gateway - CORS
////////////////////
CORS MUST be enabled for API calls from another domain
Request MUST have Access-Control-Allow-...
	Methods, Headers, and Origin
////////////////////

CORS - Enabled on the API Gateway
////////////////////
Example:
	S3 Bucket website makes URL calls to API Gateway
1.
	S3 Site sends Preflight Request, checked by the API Gateway CORS
2.
	API Gateway sends Preflight Response back to Client
3.
	On success, API Gateway and S3 Site communicate
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway Authentication and Authorization
//////////////////////////////////////////////////////////////////////////
API Gateway - Security, IAM Permissions
////////////////////
Auth through AIM:
	Authentication w/ IAM, Authorization w/ IAM Policy
	Best for access w/i AWS (EC2, Lambda, ...)

////////////////////

API Gateway - Resource Policies
////////////////////
Just like Lambda Resource Policies
	Cross-account access w/ IAM Security
	Specific IP
	VPC Endpoint
Silde shows a script that looks like all other Resource Policies
////////////////////

API Gateway - Security, Cognito User Pools
////////////////////
Cognito: a DB of Users

Auth through Cognito:
	Authentication w/ Cognito User Pools, Authorization w/ API Gateway Methods
	Client authenticates w/ Cognito, Cognito sends back Token, Client sends Token to API Gateway
////////////////////

API Gateway - Security, Lambda Authorizer (formerly Custom Authorizers)
////////////////////
Client authorizes w/ a Lambda Authorizer
	Lambda returns an IAM Policy for the User
	Authentication w/ External, Authorization w/ Lambda Fn
	Client authenticates w/ 3rd Party auth sys, gets Token, sends Token to API Gateway,
		API Gateway uses Lambda Authorizer to verify Token w/ 3rd Party auth sys,
		Lambda Authorizer gives API Gateway an IAM Policy for the Client,
		API Gateway puts Policy into Policy Cache
////////////////////

API Gateway - Security - Summary
////////////////////
IAM: Best for access w/i AWS
Custom Authorizer: Best for 3rd Party Authorizers, returns IAM Policy
Cognito Pool: Auth through Cognito
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway REST API vs HTTP API
//////////////////////////////////////////////////////////////////////////
High Level:
	HTTP APIs are cheaper, but have less features

HTTP APIs:
	Fast, cheap, Lambda proxy, OIDC and OAuth 2.0, and built-in CORS
	No usage plans or API Keys
REST APIs:
	All features, except Native OpenID Connect / OAuth 2.0
//////////////////////////////////////////////////////////////////////////


API Gateway Websocket API
//////////////////////////////////////////////////////////////////////////
API Gateway - WebSocket API - Overview
////////////////////
WebSocket:
	A connectiong b/t CLient and Server, where Server can push messages to the Client w/o a request
WebSocket APIs are used in real-time apps, such as multiplayer games
Works w/ AWS Services (Lambda, DynamoDB) or HTTP Endpoints
////////////////////

Connecting to the API
////////////////////
The Client connects to the API Gateway over a wss connection (rather than https)
The API Gateway invokes a Lambda Fn w/ a connectionID that persists while the connection does
////////////////////

Client to Server Messaging, connectionID is re-used
////////////////////
The Client continues using the wss URL to send frames to the API Gateway
	Messages from the Client are called Frames here
The Client uses the sme URL to invoke a new Lambda Fn
	The Server uses the connectionID to identify the Client and return the correct data
////////////////////

Server to Client Messaging
////////////////////
The Lambda Fn will use a Connection URL Callback Fn to send Messages back to the Client
////////////////////

Connection URL Operations
////////////////////
The POST, GET, and DELETE operations have specific purposes here:
POST: Send Message
GET: Get connection status
DELETE: End the connection
////////////////////

API Gateway - WebSocket API - Routing
////////////////////
When a Client connects to the API Gateway, the Connection can be routed to part of the backend
	The Client will have some parameter in the Frame that determines the routing
////////////////////
//////////////////////////////////////////////////////////////////////////


API Gateway - Architecture
//////////////////////////////////////////////////////////////////////////
The slide shows how an API Gateway can be placed as the entry-point for an entire company
The API Gateway just routes the Clients to the correct Endpoint
//////////////////////////////////////////////////////////////////////////


//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 24
  __          __    
 /  \ |  |   /  \  |  |
 |    |__|      /  |__|
 |    |  |     /      |
 \__/ |  |.   /__     |

AWS CICD: Code Commit, Code Pipeline, CodeBuild, CodeDeploy
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


Introduction to CICD in AWS
//////////////////////////////////////////////////////////////////////////
CICD - Introduction
////////////////////
We can deploy/update Services manually, but this can be clunky and error-prone
	CICD allows us to do CICD straight from Git, w/ testing and rollback
Tools we'll use for this:
	GitHub       - Versioning
	CodePipeline - Deploy to Elastic Beanstalk
	CodeBuild    - Build and test
	CodeDeploy    - Deploy to EC2 Instances
	CodeStar     - Manage software development
	CodeArtifact - Store and publish software
	CodeGuru     - AI Code Reviews
////////////////////

Continuous Integration (CI)
////////////////////
Pushing code to Git
	Automatic testing, build, and feedback to the developer
////////////////////

Continuous Delivery (CD)
////////////////////
Code that passes the tests:
	Code gets a build
	Code is sent to deployment server
	Code is deployed into production
	Lots of small frequent updates, rather than large infrequent ones
////////////////////

Tech Stack for CICD (Pipeline)
////////////////////
Git           - Versioning
Jenkins CI    - Build and Test
CodeDeploy    - Deployment
AWS Endpoints - The backend where the code is in production
////////////////////
//////////////////////////////////////////////////////////////////////////


CodeCommit Overview
//////////////////////////////////////////////////////////////////////////
CodeCommit has since been discontinued
	Use Git
//////////////////////////////////////////////////////////////////////////


AWS CodePipeline Overview
//////////////////////////////////////////////////////////////////////////
AWS CodePipeline
////////////////////
Pipeline:
	Visual Workflow of CICD
	Source - Git
	Build  - CodeBuild
	Test   - CodeBuild
	Deploy - CodeDeploy
	Invoke - Lambda
Stages
	Each Stage has sequential or parallel actions
	Build -> Test -> Deploy -> Load Testing -> ...
	Manual approval at any Stage
////////////////////

Tech Stack for CICD (Pipeline) // Repeated from CICD Intro
////////////////////
Git           - Versioning
Jenkins CI    - Build and Test
CodeDeploy    - Deployment
AWS Endpoints - The backend where the code is in production
////////////////////

CodePipeline - Artifacts
////////////////////
Artifacts are the results of each step that is then passed on to the next step in the Pipeline
	Artifacts are stored in S3, and then passed to the next step
Code Push -> Git -> CodeBuild -> CodeDeploy -> backend
////////////////////

CodePipeline - Troubleshooting
////////////////////
CloudWatch Events: Events for failed/cancelled Stages
Console: will have info from errors/failures
Permissions: Each step MUST have the correct Permissions
CloudTrail: audit AWS PI calls
////////////////////
//////////////////////////////////////////////////////////////////////////


CodeBuild Overview
//////////////////////////////////////////////////////////////////////////
AWS CodeBuild
////////////////////
Source - GitHub
Build Instructions - buildspec.yml
Output Logs - S3 and CloudWatch
	CloudWatch Metrics for builds
	EventBridge for failures
	CloudWatch Alarms for thresholds
Build Projects definition - CodePipeline or CodeBuild
////////////////////

CodeBuild - Supported Environments
////////////////////
CodeBuild can test:
	Java, Ruby, Python, Go, Node.js, Android, .Net Core, PHP
	Anything else, using Docker containers
////////////////////

CodeBuild - How it Works
////////////////////
Diagram shows:
	Code repo stores versions
	Code Repo outputs to CodeBuild
		And pulls in Docker and S3 (Cache)
	CodeBuild outputs to S3 and sends logs to S3/CloudWatch
////////////////////

CodeBuild - buildspec.yml
////////////////////
Slide has example buildspec.yml file

buildspec.yml - MUST be at root
env - Environment Variables listed under "variables", "parameter-store", and "secrets-manager"
phases - Actions that need to be taken at each phase of the build
	Phases: install, pre_build, Build, post_build
artifacts - Output stored in S3
cache - files saved to S3 to speed up future builds
////////////////////
//////////////////////////////////////////////////////////////////////////


CodeBuild Hands On Part 1
//////////////////////////////////////////////////////////////////////////
Goal: Test that we can set up CodeBuild to deploy our Elastic Beanstalk project

Create a new CodeBuild project and select GitHub as the source
	Authorize GitHub access and select the repo
	Make sure to enable rebuilds on PUSH to GitHub
//////////////////////////////////////////////////////////////////////////



CodeBuild Hands On Part 2
//////////////////////////////////////////////////////////////////////////
Goal: build CodeBuild based on CodePipeline changes

Create the buildspec.yml file and put it at the root of the project on GitHub
	The code for this is at Code > codebuild > buildspec.yml
	Push to GitHub and it builds successfully

Change the CodeBuild source from GitHub to CodePipeline
//////////////////////////////////////////////////////////////////////////


CodeDeploy Overview
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Deployment Services that automates deployment
	Across EC2 Instances, On-premises Servers, Lambda Fns, and ECS Services
Automates rollback on failure or CloudWatch Alarms
Variable deployment speeds
Controlled w/ file: appspec.yml
////////////////////

CodeDeploy - EC2/On-premises Platform
////////////////////
Deployments Types: In-place, blue/green
Instances MUST run CodeDeploy Agent
Update Speeds: AllAtOnce, HalfAtATime, OneAtATime, Custom
////////////////////

CodeDeploy - In-Place Deployment
////////////////////
Shut some instances down, update them, restart them, repeat with the rest
////////////////////

CodeDeploy - Blue-Green Deployment
////////////////////
Copy all instances in Auto Scaling Group, Update copy, Move traffic, Shut down originals
////////////////////

CodeDeploy Agent
////////////////////
CodeDeploy is put on the Instances, and MUST have Permissions to access S3 to get deployment bundles
////////////////////

CodeDeploy - Lambda Platform
////////////////////
This is integrated w/ SAM (Serverless Application Model) framework
Three options:
	Linear: Grow traffic every N minutes
	Canary: Small amount of traffic as a test, and then all traffic
	AllAtOnce: Immediate change
////////////////////

CodeDeploy - ECS Platform
////////////////////
Can only have Blue/Green deployments
Same three options:
	Linear: Grow traffic every N minutes
	Canary: Small amount of traffic as a test, and then all traffic
	AllAtOnce: Immediate change
////////////////////
//////////////////////////////////////////////////////////////////////////


CodeDeploy Hands On
//////////////////////////////////////////////////////////////////////////
Goal:

Create Role for CodeDeploy and an EC2 Instance
	Add S3 read Permissions

Create S3 Bucket to store the application
	Create folder w/ zipped file at CODE > codedeploy > SampleApp_Linux > SampleApp_Linux.zip

Create the EC2 Instance and add apply the Role
Use CloudShell to install CodeDeploy on the Instance
	Follow instructions at CODE > codedeploy > commands.sh
Create the Deployment Group in the CodeDeploy console
	Use tags to differ b/t "Dev" and "Prod"
	Select the S3 Bucket w/ the application under Revision Location
	Apply the Role
//////////////////////////////////////////////////////////////////////////


CodeDeploy for EC2 and ASG
//////////////////////////////////////////////////////////////////////////
CodeDeploy - Deployment to EC2
////////////////////
Standard In-place or blue/green deployment of previous sections:
	Configure w/ appspec.yml, set deployment strategy
Can configure hooks to verify success
////////////////////

CodeDeploy - Deploy to an ASG (Auto-Scaling Group)
////////////////////
In-place Deployment:
	Update current or create new EC2 Instances
Blue/Green Deployment:
	MUST be using an ELB
////////////////////

CodeDeploy - Redeploy & Rollbacks
////////////////////
Rollback:
	Can be automatic or manual, or set to not rollback at all
	Rollbacks are handled as a new deployment (not a restored version)
////////////////////
//////////////////////////////////////////////////////////////////////////


CodeArtifact - Overview
//////////////////////////////////////////////////////////////////////////
AWS CodeArtifact
////////////////////
Storing and retrieving Code Dependencies (package dependencies) is called Artifact Management
Works w/ common dependency tools such as npm, pip, ... (see slide for more)
Developers and CodeBuid can work directly w/ CodeArtifact
////////////////////

CodeArtifact - Diagram
////////////////////
Diagram shows:
	CodeArtifact inside the VPC
	Domain inside of CodeArtifact
	Repos inside of the Domain
	Requests to CodeArtifact are proxied to the Public Artifact Repos, like npm
		These packages are then cached in CodeArtifact's repos
		Admins determine which repo the packages sould be stored in
			Different package managers can store packages in the same repo
////////////////////

CodeArtifact - EventBridge Integration
////////////////////
CodeArtifact can trigger an Event in EventBridge whenever a package changes
EventBridge can use that Event to invoke a Lambda Fn, send a message to SQS, ....
	OR, can even have CodePipeline rebuild and redeploy the project
////////////////////

CodeArtifact - Resource Policy
////////////////////
When a User is given access to a CodeArtifact repo, they need an IAM Policy
When another account is given access to a CodeArtifact repo, they need a Resource Policy

Both of these are an all-or-nothing Policy (we can't specify specific packages in the repo)
////////////////////
//////////////////////////////////////////////////////////////////////////


CodeArtifact - Hands On
//////////////////////////////////////////////////////////////////////////
Create the CodeArtifact repo and specify the Public Artifact Repo (Ex: pypi-store for Python)
	Create the Domain and select it for the CodeArtifact repo
In CloudShell get a temporary (12 hours) authorization token from the Public Artifact Repo
	// W/ pypi-store, it needs to use pip3 (NOT pip)
	Execute the authorization command
We can now use pip3 to install packages
	The packages can be seen in the Console
//////////////////////////////////////////////////////////////////////////


CodeGuru - Overview
//////////////////////////////////////////////////////////////////////////
Amazon CodeGuru
////////////////////
CodeGuru is an ML-powered code reviewer that can look for errors and judge performance
CodeGuru Reviewer: Looks for errors and leaks in code
CodeGuru Profiler: Judges performance of code before and after deployment
////////////////////

Amazon CodeGuru Reviewer
////////////////////
Supports Java and Python
Looks for errors, security gaps, input validations, and other common problems
Integrates w/ GitHub and others
////////////////////

Amazon CodeGuru Profiler
////////////////////
Supports apps on AWS or on-premises
Examines runtime performance problems, looking for:
	code inefficiencies, CPU efficiencies, anomaly detection, and can provide a Heap summary
////////////////////
//////////////////////////////////////////////////////////////////////////


CodeGuru - Agent Configuration
//////////////////////////////////////////////////////////////////////////
Amazon CodeGuru - Agent Configuration
////////////////////
There are several parameters that control how much information CodeGuru takes in: 
MaxStackDepth - number of stacks (Ex: method calls) that is profiled
MemoryUsageLimitPercent - memory percent used
MinimumTimeForReportingInMilliseconds - minimum time b/t reports
ReportingIntervalInMilliseconds - profile report intervals
SamplingIntervalInMilliseconds - profile sampling interval
////////////////////
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 25
  __          __    ___
 /  \ |  |   /  \  |
 |    |__|      /  |__
 |    |  |     /      |
 \__/ |  |.   /__  ___|

AWS Serverless: SAM - Serverless Application Model
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


SAM Overview
//////////////////////////////////////////////////////////////////////////
AWS SAM
////////////////////
SAM is a Framework for quicly creating and deploying serverless apps
	Configured w/ a YAML file
	Supports anything from CloudFormation
	Can run Lambda, API Gateways, and DynamoDB locally
////////////////////

AWS SAM - Recipe
////////////////////
SAM Template Header is: Transform: 'AWS::Serverless-2016-10-31'
	// Just guessing that the date can vary
The constructing file consists of lines w/ "AWS::Serverless::..."
	Function, Api, and SimpleTable
Package and deploy w/ "sam deploy"
Sync changes w/ "sam sync --watch"
////////////////////

Deep Dive into SAM Deployment
////////////////////
Slide diagram shows:
	User creates a SAM Template in YAML
		This is transformed into a CloudFormation Template in YAML
	File is uploaded as a CloudFormation Template to S3
		As a zip file
	The S3 Bucket sends the file onto CloudFormation, which creates the Services for it
////////////////////

SAM Accelerate (sam sync)
////////////////////
SAM Accerlerate is features that allows for making a SAM app deploy or change very fast
sam sync specifically allows for specifying what to update
////////////////////

SAM Accelerate (sam sync) - Examples
////////////////////
sam sync
	Syncs code and infrastructure
sam sync --code
	Syncs code olny, no infrastructure change
sam sync --code -- resource AWS::Serverless::Function
	Syncs only Lambda Fns and their dependencies
sam sync --code --resource-id HelloWorldLmbdaFn
	Sync only the specified resource, by id
sam sync --watch
	Monitors for file changes and automatically chooses b/t
		sam sync --code for code changes only
		sam syc for code and infrastructure
////////////////////
//////////////////////////////////////////////////////////////////////////


Installing the SAM CLI - Hands On
//////////////////////////////////////////////////////////////////////////
Get instructions by Googling the SAM CLI, and bring up the AWS docs

Just use whatever it tells and confirm the install w/ sam --version
//////////////////////////////////////////////////////////////////////////


Creating First SAM Project (Hands On)
//////////////////////////////////////////////////////////////////////////
Start off w/ sam init on the CLI

Create the YAML template.yaml file and commands.sh
Under the src folder, put app.py (Ex is in Python)
The template file is a list of keys and values, in the yaml format
Keys are:
	AWSTemplateFormatVersion // Tells that this is a CloudFormation template
	Transform // Tells that this is a SAM template
	Description // app description
	Resources // Resources to be created, such as a Python Lambda Fn
		Type: of the Resource, like Serverless Fn
		Properties: // of the Resource
			Handler: // Resource name and entry method
			Runtime // Language and version
			CodeUri: // Location of the Fn
			Description // Fn description
			MemorySize: // memory
			TimeOut: time to give the Fn
//////////////////////////////////////////////////////////////////////////


Deploying SAM Project (Hands On)
//////////////////////////////////////////////////////////////////////////
Step-by-step specifics are in video 382

Create the Bucket:
	aws mb s3://*bucket name*
Package the app
Deploy the app

There is now an S3 Bucket and Lambda Fn on the AWS Console, and we can test the Fn
//////////////////////////////////////////////////////////////////////////


SAM API Gateway (Hands On)
//////////////////////////////////////////////////////////////////////////
Find the template for an API Gateway on the awslabs GitHub

Paste the template into the app.py
	Modify it to just return "Hello World", and leave the respond method as is
Add an event to template.yaml for the Gateway API

Run the sam package command again
Run the sam deploy command again

In the Console, we can see the Stack has been updated to include:
	Permissions, ApiGateway::RestApi, ApiGateway::Deployment, ApiGateway::Stage
We can also see that in Amazon API Gateway:
	There is a GET created for the Serverless app as well
	Test the GET to see it returns correctly
//////////////////////////////////////////////////////////////////////////


SAM DynamoDB (Hands On)
//////////////////////////////////////////////////////////////////////////
To get DynamoDB into the app, add the creation lines into app.py
	import boto3
	import os
	region_name = os.environ['REGION_NAME']
	dynamo = boto3.client('dynamodb', region_name=region_nme)
	table_name = os.environ['TABLE_NAME']

Add a Table to the list of Resources to create in he template.yaml
	Create environment variables for the Table and Region
Limiting the provisioned throughputs makes it cheaper

Build and Deploy the application
//////////////////////////////////////////////////////////////////////////


SAM Policy Templates
//////////////////////////////////////////////////////////////////////////
LOTS of templates for Policies at link on slide

Ex Templates: (Pretty self-explanatory)
	S3ReadPolicy - Read Permissions to S3
	SQSPollerPolicy - Poll Permisssions to SQS
	DynamoDBCrudPolicy - CRUD Permissions for DynamoDB

In a SAM Template, under a Lambda Fn creation, we have a Policies entry where we can list these
	See Ex in Slide
//////////////////////////////////////////////////////////////////////////


SAM with CodeDeploy
//////////////////////////////////////////////////////////////////////////
SAM and CodeDeploy
////////////////////
SAM framework uses CodeDeploy to update Lambda Fns

This features:
	Traffic shifting
	Pre and Post Hooks // Optional
	Rollback and CloudWatch Alarms

Diagram shows CodeDeploy in a CICD pipeline
	The app uses V1 of a Lambda Fn
	New update creates V2 of Lambda Fn
	Lambda Pre-Traffic Hook tests the V2
	Traffic is shifted to the V2
	Post-Traffic Hook checks the V2
	CloudWatch Alarm monitors the CodeDeploy during this
	Alias for the Fn is fully moved to V2
	Final step is to remove the V1
////////////////////

SAM Template and CodeDeploy Ex
////////////////////
Slide has the code

SAM Template has variables for the process of updating the Lambda Fn
AutoPublishAlias - Updates the Lambda Fn: change detected -> publish Fn -> redirect Alias
DeploymentPreference - Canary, Linear, AllAtOnce
Alarms - Trigger a rollback if needed
Hooks - Tests for Pre and post traffic shifts
////////////////////

Hands On
////////////////////
Init a new SAM application in a directory
Run: sam build
To start, the Fn just puts out "Hello World"
Add to the SAM Template Properties field of the Lambda Fn:
	AutoPublishAlias: live

	DeploymentPreference:
		Type: Canary10Percent10Minutes
		// This will move 10 percent of the traffic to the new version for 10 minutes
		// After that, it will move all traffic to the new version
Run: sam build
Run: sam deploy --guided // This makes the terminal ask questions about the deployment

Update Fn to output "Hello World V2"
Run the build and deploy commands

CodeDeploy in the AWS Console shows the deployment happening
////////////////////
//////////////////////////////////////////////////////////////////////////


SAM - Local Capabilities
//////////////////////////////////////////////////////////////////////////
Locally Start AWS Lambda:
	sam local start-lambda
	Starts local endpoint that emulates AWS Lambda w/ optional tests
Locally Invoke Lambda Fn:
	sam local invoke
	Invoke Lambda Fn w/payload to generate tests
	For API calls to AWS, use --profile
Locally Start an API Gateway Endpoint:
	sam local start-api
	Start local HTTP server for Fns w/ auto-reloaded changes to the Fns
Generate AWS Events for Lambda Fns:
	sam local generate-event
	Generate sample payloads for S3, API Gateway, SNS, ...
//////////////////////////////////////////////////////////////////////////


SAM - Multiple Environments
//////////////////////////////////////////////////////////////////////////
We can define multiple Environments by creating a samconfig.toml file

This file has parameters like:
	s3_bucket, s3_prefix, region, ...

There are four sections
	dev.deploy.parameters: Defines the Stack for dev
	prod.deploy.parameters: Defines the Stack for prod
	dev.sync.parameters: Defines the sync for the Stack, defined same as deploy
	prod.sync.parameters:

With this file, we can then run "sam deploy --config-env dev"
	This will go to the samconfig.toml file and run the deploy parameters for dev
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 26
  __          __    ___
 /  \ |  |   /  \  |
 |    |__|      /  |___
 |    |  |     /   |   |
 \__/ |  |.   /__  |___|

Slide 757
Cloud Development Kit
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


CDK Overview
//////////////////////////////////////////////////////////////////////////
AWS Cloud Development Kit (CDK)
////////////////////
CDK is a way to write scripts that provision and define resources, like a CloudFormation Template
	The CDK is available in multiple programming languages: js/ts, Python, .NET, ...

Components are called Constructs
Code is compiled into a CloudFormation Template (JSON or YAML)
Great for Lambda Fns and Docker Containers
////////////////////

CDK in Diagram
////////////////////
Diagram just shows how an AWS Service is written as a Construct,
	in a programming language,
	turned into a CloudFormation Template,
	and finally sent to CloudFormation for Stack creation
////////////////////

CDK vs SAM
////////////////////
SAM:
	Serverless focus
	Only JSON or YAML
	Great for Lambda
CDK:
	All AWS Services
	Many languages
Both:
	Leverages CloudFormation
////////////////////

CDK + SAM
////////////////////
We can use these together by having CDK create the CloudFormation Template that SAM uses
This also means we can use the SAM CLI to run the CDK results locally

This is done using the command: cdk synth
////////////////////

CDK Hands-On
////////////////////
Slide has a diagram showing what we will accomplish in the Hands-On section:
	Send image to S3 Bucket, which triggers a Lambda Fn
	Lambda Fn analyzes image w/ Rekognition
	Lambda Fn saves the results to DynamoDB
	
All of this will be written into a CDK script, and then deployed
////////////////////
//////////////////////////////////////////////////////////////////////////


CDK - Hands On
//////////////////////////////////////////////////////////////////////////
Go to CloudShell
Install the cdk onto the command line: sudo npm install -g aws-cdk-lib

Create and enter a directory, then init the cdk:
	cdk init app --language javascript

Replace the file content of lib/cdk-app-stack.js with CODE > cdk > lib > cdk-app-sack.js

In the file, we are:
	Creating an S3 Bucket
	Create a Role for AWS Lambda
		Adding a Policy to the Role for Rekognition
	Create a DynamoDB Table
	Create a Lambda Fn
		Has the script to put items into DynamoDB when an item is added to S3
		Gets Permissions for S3 Read access and DynamoDB Full access

Create and enter a directory for the Lambda Fn, and create index.py
	Copy into it the contents of CODE > cdk > lambda > index.py

In the file, we are:
	Taking a file from S3
	Sending that to Rekognition
	Putting the results into DynamoDB

Bootstrap the project:
	Needed to be done once per Region per Account
	In the directory w/ cdk.json
	cdk bootstrap

We can see the Stack in the CloudFormation Console
	CloudFormation > Stacks > CDKToolkit

(Optional) Synthesize the app: cdk synth
	This creates a CloudFormation Template of the app

Deploy the app: cdk deploy

Test the app by uploading an image to the S3 Bucket
	The Lambda will send the image to Rekognition, and store the result into DynamoDB

Cleanup:
	Manually empty the S3 Bucket
	Destroy the app: cdk destroy
//////////////////////////////////////////////////////////////////////////


CDK - Constructs
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
A Construct is an AWS Service that will be created for the Stack
	// If multiple AWS Services are in one Construct, they should be related
The CDK has a library of every AWS Services, and comes in three levels (L1, l2, L3)
There is also an open-source community that makes custom Constructs for 3rd Party Services
////////////////////

CDK Constructs - Layer 1 Constructs (L1)
////////////////////
These Constructs are direct imports from CloudFormation, and start with "Cfn"
Ex: new s3.CfnBucket...
////////////////////

CDK Constructs - Layer 2 Constructs (L2)
////////////////////
AWS Resources with intent-based API
Has defaults and boilerplate, so not needing to define as many properties
Methods can be called on the Construct after creation
	Ex: const bucket = new s3.Bucket...
	    const objectUrl = bucket.urlForObject('MyBucket/MyObject');
////////////////////

CDK Constructs - Layer 3 Constructs (L3)
////////////////////
These Constructs are also called Patterns, as they are multiple related Services
	Ex: A Lambda Fn w/ an ALB and an API Gateway
The slide shows the code for a Lambda Fn that w/ an ALB and API Gateway
	We can easily define the GET, POST, and DELETE methods
	These are automatically configured as needed w/ Permissions
////////////////////
//////////////////////////////////////////////////////////////////////////


CDK - Commands & Bootstrapping
//////////////////////////////////////////////////////////////////////////
CDK - Important Commands to Know
////////////////////
npm install -g aws-cdk-lib: Install CDK
cdk init app: Create CDK project
cdk synth: Synthesize and print CloudFormation Template
cdk bootstrap: Creates S3 an IAM Role for Stack
cdk deploy: Deploy Stack
cdk diff: View local/deployed Stack differences
cdk destroy: Destroy Stack(s)
////////////////////

CDK - Bootstrapping
////////////////////
Creates a Stack in an S3 w/ an IAM Role
	After bootstrapping, the Stack is ready for deploy
	Bootstrapping MUST be done for each new Environment (Account, Region)
////////////////////
//////////////////////////////////////////////////////////////////////////


CDK - Unit Testing
//////////////////////////////////////////////////////////////////////////
Test the syntax of a cdk script w/ that language's test framework
	javascript: Jest, python: Pytest
There are two types of tests:
	Fine-grained Assertions: CloudFormation Template syntax is correct
	Snapshot Tests: Test synthesized result against stored baseline
Templates can be:
	From CDK: Template.fromStack(MyStack)
	From outside CDK: Template.fromString(MyString)
Slide has code Exs
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 27
  __          __    ___
 /  \ |  |   /  \      |
 |    |__|      /      |
 |    |  |     /       |
 \__/ |  |.   /__      |

Slide 
Cognito: Cognito User Pools, Cognito Identity & Cognito Sync
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


Cognito Overview
//////////////////////////////////////////////////////////////////////////
Gives Users outside of AWS an identity to use our apps
Cognito User Pools:
	Integrates w/ API Gateway & ALB
Cognito Identity Pools (Federated Identity): 
	Integrates w/ Cognito User Pools
//////////////////////////////////////////////////////////////////////////


Cognito User Pools
//////////////////////////////////////////////////////////////////////////
Cognito User Pools (CUP) - User Features
////////////////////
Serverless DB for Users
Simple login w/ Username/password
	Can validate Email, Phone Number, and have MFA
	Can use Federated Identities (Facebook, Google, ...)
Login sends back a JSON Web Token (JWT)
////////////////////

Cognito User Pools (CUP) - Diagram
////////////////////
Diagram shows choice of logging in w/ mobile/web apps OR Federated Identities
////////////////////

Cognito User Pools (CUP) - Integrations
////////////////////
There are two paths:
	Get Token and use it at API Gateway
	Auth through ALB, and be forwarded after auth
////////////////////
//////////////////////////////////////////////////////////////////////////


Cognito User Pools Hands On
//////////////////////////////////////////////////////////////////////////
Create the User Pool

Cognito Console has lots of window w/ info on User, Auth methods, ...

Go to Managed Login and set up the login page

Create a temp email w/ mailinator.com
	Verify the email address w/ the code
//////////////////////////////////////////////////////////////////////////


Cognito User Pools - Others
//////////////////////////////////////////////////////////////////////////
Cognito User Pools - Lambda Triggers
////////////////////
LOTS of information on slide
Broken up into:
	Auth Events - pre/post auth, pre token generation
	Sign-Up - pre sign-up, post confirm, migrate User
	Messages - custom Lambda trigger
	Token Creation - pre token generation
////////////////////

Cognito User Pools - Hosted Authentication UI
////////////////////
Cognito has hosted authentication UI which can be customized to match our site
////////////////////

CUP - Hosted UI Custom Domain
////////////////////
Custom domain MUST have an ACM cert in us-east-1, no other choice
	And be in App Configuration section
////////////////////

CUP - Adaptive Authentication
////////////////////
Login attempts are given a risk score, and Cognito determines what action (if any) to take
Risk Score -  (low, med, high) familiar device, location, compromised account, ... 
Integrated w/ CloudWatch Logs
////////////////////

Decoding a ID Token; JWT - JSON Web Token
////////////////////
CUP issues JWT Tokens, which have
	Header, Payload, Signature
JWT Tokens are verified by a signature, and deliver a payload of user info
////////////////////
//////////////////////////////////////////////////////////////////////////


Application Load Balancer - User Authentication
//////////////////////////////////////////////////////////////////////////
Application Load Balancer - Authenticate Users
////////////////////
ALB can auth Users by:
	Identity Provider (IdP)
	Cognito User Pools
////////////////////

Application Load Balancer - Cognito Auth
////////////////////
Diagram shows:
1. GET /api/data User GET to ALB's api
2. authenticate w/ Cognito
3. GET /api/data to ECS
////////////////////

ALB - Auth through Cognito User Pools
////////////////////
ALB has a User Pool that authenticates the User, and a callback sends back a Token
	That tells the ALB to send the User to the application
////////////////////

Application Load Balancer - OIDC Auth
////////////////////
Instead of using Cognito, we can any other auth provider

Diagram showing:
1. HTTPS Request
2. ALB redirects User for auth, to 3rd Party's auth endpoint
3. Auth endpoint sends User a Code
4. ALB checks code against 3rd Party Token endpoint
5. ALB verifies Code, and gets an Access Token for the User
6. ALB sends Access Token to 3rd Party's User Info Endpoint
7. 3rd Party sends back a User Claims, which is user info for this session
8. ALB sends User's request to the AWS Services

9. AWS Service(s) send response to ALB
10. ALB sends response to User
////////////////////

ALB - Auth Through an Identity Provider (IDP)
That is OpenID Connect (OIDC) Compliant
////////////////////
To make the above work, we need to set up:
	Identity Provider
	Issuer
	Auth Endpoint
	Token Endpoint
	User Info Endpoint
	Client ID
	Client Secret

Although, I imagine that any 3rd party auth provider has much as this set and laid out already
	We do have to configure our system to use it though
////////////////////
//////////////////////////////////////////////////////////////////////////


Cognito Identity Pools // Not the same as Cognito User Pools
//////////////////////////////////////////////////////////////////////////
Cognito Identity Pools (Federated Identities)
////////////////////
We can't give every User their own IAM Role

We use an Identity Pools instead:
	Public Providers (Google, Facebook, ...)
	Users in Cognito Pool
	OpenID Connect and SAML Identity (Providers)
	Devs w/ authorization
	Guest Access
Access to the AWS Services/API Gateway is controlled though IAM Policies defined in Cognito
	They can be further customized based on user_id
////////////////////

Cognito Identity Pools - Diagram
////////////////////
Users want access to AWS Services like S3 and DynamoDB
They go to a login and get a token from a provider, (AWS Cognito User Pools OR 3rd Party)
Users send the Token to Cognito Identity Pools, and get back temp credentials (if validated)
	These temp credentials have an IAM Policy that defines what they can do
////////////////////

Cognito Identity Pools - Diagram w/ CUP
////////////////////
If exclusively using Cognito User Pools, we have an Internal DB of Users
There area few changes:
Users want access to AWS Services like S3 and DynamoDB
They go to a login and get a token from Cognito User Pools, can auth through 3rd Party
	All Users have an entry in Cognito User Pools. Token comes from CUP
Users send the Token to Cognito Identity Pools, and get back temp credentials (if validated)
	These temp credentials have an IAM Policy that defines what they can do
////////////////////

Cognito Identity Pools - IAM Roles
////////////////////
- Best to have an IAM Role for Guests, and another for authenticated Users
- IAM Credentials come from Cognito Identity Pools through STS (Security Token Service)
- These Roles must have a "trust" Policy of Cognito Identity Pools
////////////////////

Cognito Identity Pools - Guest User Example
////////////////////
Policy that gives Guest the Guest profile image
////////////////////

Cognito Identity Pools - Policy Variable on S3 (Authenticated Users)
////////////////////
Policy that only gives User access to data that has the User's id as a prefix
////////////////////

Cognito Identity Pools - DynamoDB (Authenticated Users)
////////////////////
Policy that only gives User access to values in DynamoDB that has the User's id as a prefix
////////////////////
//////////////////////////////////////////////////////////////////////////


Cognito Identity Pools Hands On
//////////////////////////////////////////////////////////////////////////
Choose Authenticated And/or Guest Users
	For Authenticated, we must choose an auth provider as well
Create an IAM Role Policy for Authenticated and Guest
Set the specifics of the chosen Authentication Service
	Set the specifics of what Users can access
To demo this, we need to follow the workflow as seen in the diagrams above
Two Cognito Roles can be found in the IAM Console (Authentication and Guest)
	And thus, we can edit them as well
//////////////////////////////////////////////////////////////////////////


Cognito User Pools vs Cognito Identity Pools
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
Cognito User Pools just have a DB of Users and the login to authenticate
	Customize login page
	Pre/Post login Lambda triggers
	Can pull from User lists in 3rd Parties like Google, Facebook, ...

Cognito Identity Pools
	Validates Users by exchanging authentication Tokens for temp credentials
	Can be used w/ Cognito User Pools and/or 3rd party User lists
	Users get mapped to IAM Roles to define their Persmissions
////////////////////

Cognito Identity Pools - Diagram w/ CUP
////////////////////
Same as above diagram:

If exclusively using Cognito User Pools, we have an Internal DB of Users
There area few changes:
Users want access to AWS Services like S3 and DynamoDB
They go to a login and get a token from Cognito User Pools, can auth through 3rd Party
	All Users have an entry in Cognito User Pools. Token comes from CUP
Users send the Token to Cognito Identity Pools, and get back temp credentials (if validated)
	These temp credentials have an IAM Policy that defines what they can do
////////////////////
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 28
  __         __    ___
 /  \ |  |  /  \  /   \   
 |    |__|     /  \___/
 |    |  |    /   /   \
 \__/ |  |.  /__  \___/

Other Serverless: Step Functions & AppSync
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

Step Functions Overview
//////////////////////////////////////////////////////////////////////////
AWS Step Functions
////////////////////
Visualization of app workflows as state machines(nodes, paths)
	Written in JSON format
////////////////////

Step Functions - Task States
////////////////////
Task States are some work done by a step in the workflow
These have about as many possibilities as there are Services
	Invoke Lambda, run AWS Batch job, run ECS task, insert into DynamoDB, ...
////////////////////

Example - Invoke Lambda Function
////////////////////
Writing the JSON script, we can invoke a Lambda Fn w/ parameters
	Slide has code
////////////////////

Step Functions - States
////////////////////
States determine the path of the app through the workflow

Choice State: Test for condition
Fail or Succeed State: Stop execution after fail/success
Pass State: Pass input data to output w/ no work, optional manipulation
Wait State: Delay a time
Map State: Dynamically iterate steps ???
Parallel State: Begin parallel branches of execution
////////////////////

Visual Workflow in Step Functions
////////////////////
The visual is dynamic, so as the app executes the workflow, the steps change color
Green: Success
Red: Failed
White: Cancelled/Skipped
Blue: In Progress
////////////////////
//////////////////////////////////////////////////////////////////////////


Step Functions - Hands On
//////////////////////////////////////////////////////////////////////////
Design:
	Actions: Actions done by Services
	Flow: Path from Node to Node
	Patterns: Premade branches (Ex: Process CSV file in S3)
Code:
	Write JSON style code to create Workflows

After creation, we can run the Workflow
For these runs we can:
	Change Input variables
	See output
//////////////////////////////////////////////////////////////////////////


Step Functions - Invoke Lambda - Hands On
//////////////////////////////////////////////////////////////////////////
Workflow structure:
	Invoke Lambda as entry-point
	Choice State - go to left or right branch depending on Lambda result
	Action Node for each branch
	End Node

We create the Lambda to be invoked
Set the Node Actions
Test the Workflow
//////////////////////////////////////////////////////////////////////////


Step Functions - Error Handling
//////////////////////////////////////////////////////////////////////////
Error Handling in Step Functions
////////////////////
Any State can return an Error, for a variety of reasons
	Definition issue, Task failure, Transient issue, ...
Resolve with:
	Retry and Catch
Predefined Error codes:
	States.ALL - (any error)
	States.TimeOut - (Task took too long)
	States.TaskFailed  (task failed)
	State.Permissions - (Insufficient Permissions)
State itself can also return Errors
////////////////////

Step Functions - Retry (Task or Parallel State)
////////////////////
What happens and how many times to retry

Syntax is much like a try-catch block w/ a few parameters:
	ErrorEquals - Each Catch looks for one of the "States." error codes
	IntervalSeconds - intial delay b/f retry
	MaxAttempts - times to retry
	BackoffRate - delay to multiply to each retry
If still failing, Catch kicks in
////////////////////

Step Functions - Catch (Task or Parallel State)
////////////////////
Syntax is much like a try-catch block w/ a few parameters:
	ErrorEquals - Each Catch looks for one of the "States." error codes
	Next - State or Node to go to
	ResultPath - Adds data in an "error" variable for next State/Node
////////////////////

Step Functions - ResultPath
////////////////////
ResultPath adds a variable to the data that will be sent to the next State/Node
Ex:
{
	"error": "error found"
}
////////////////////
//////////////////////////////////////////////////////////////////////////


Step Functions - Error Handling Hands On
//////////////////////////////////////////////////////////////////////////
Create new Step Function
	Choose Blueprint and get the error step-function
Create a Lambda Fn
	Code written to throw an error
Set the Retry Nodes (All, TaskFailed, ...)
Checking the output, we can track the retries and eventual sending of Error to endpoint
Changing the error a bit gets caught by a different State. handler
//////////////////////////////////////////////////////////////////////////


Step Functions - Wait for Task Token
//////////////////////////////////////////////////////////////////////////
Step Functions - Wait for Task Token
////////////////////
.waitForTaskToken is how we pause the execution of a function. This can be for:
	User sign-off
	AWS Service execution
This needs to get back a Task Token:
	SendTaskSuccess
	SendTaskFailure
////////////////////

Diagram
////////////////////
Diagram shows how the Step Function Workflow send a call to an AWS Service w/ a Task Token to sign
The Service does its work, and then sends back the Task Token with a succeed or fail result
////////////////////
//////////////////////////////////////////////////////////////////////////


Step Functions - Activity Tasks
//////////////////////////////////////////////////////////////////////////
Activity Tasks pull from a Task queue, looking for work
	This is in contrast to what we've seen b/f, which had Tasks pushed to it
	Can be on EC2 , Lambda, ...
	Send success/failure w/ SendTaskSuccess/SendTaskFailure
	Keep track of Task w/ SendTaskHeartBeat and HeartBeatSeconds
//////////////////////////////////////////////////////////////////////////


Step Functions - Standard vs Express
//////////////////////////////////////////////////////////////////////////
The Standard and Express Step Fns have a number of differences (see Slide)
	Slide also has helpful diagram
	CloudWatch can create/store logs

Standard:
	Standard (Exactly-Once-Execution)
	Can see results in Console
	Useful for Workflows that need a result
	
Express:
	Synchronous (At-Least-Once)
	Asynchronous (At-Most-Once)
	Can't see results in Console
	Useful for Workflows that don't need to return a result or wait for the main program
//////////////////////////////////////////////////////////////////////////


AppSync Overview
//////////////////////////////////////////////////////////////////////////
Overview
////////////////////
AppSync: Service that uses GraphQL
GraphQL: A Service that is able to make graph with many different sources, like DynamoDB and an API, at the same time
	Integrated w/ AWS Services, and can use Lambda Fns
Resolvers: Tells how to fetch the wanted data
Real-time w/ WebSocket
Just needs a  GraphQL Schema
////////////////////

GraphQL Example
////////////////////
Slide has details about the GraphQL flow and example code
	Client makes request to GraphQL
	GraphQL uses its template to get the data from its different sources
////////////////////

AppSync Diagram
////////////////////
Slide has diagram showing:
	Sources like Web/Mobile apps, real-time dashboards, and offline syncs query the GraphQL, which then gets data from other Services
		Can be used w/ CloudWatch Logs and Metrics
////////////////////

AppSync - Security
////////////////////
Four ways to authorize Users to use GraphQL:
	API_KEY: Generate keys like for an API Gateway
	AWS_IAM: IAM Roles
	OPENID_CONNECT: OpenID provider / JSON Web Token
	AMAZON_COGNITO_USER_POOLS: same

For HTTPS: Use CloudFront in front of AppSync
////////////////////
//////////////////////////////////////////////////////////////////////////


AppSync Hands On
//////////////////////////////////////////////////////////////////////////
Choose GraphQL, as the Merged is a combo of multiple GraphQL
Choose backed by DynamoDB
Create a Table and some Fields for it
We now have an AppSync and a DynamoDB Table we can look at
In AppSync:
	We can run queries to the GraphQL (create or list)
	We can create API keys, and choose how access is done
//////////////////////////////////////////////////////////////////////////


AWS Amplify
//////////////////////////////////////////////////////////////////////////
AWS Amplify - Create Mobile and Web Apps
////////////////////
Amplify is a set of tools to quickly make mobile and web apps
Amplify Studio: Visual app builder
Amplify CLI: Create backend w/ guided CLI workflow
Amplify Libraries: Connect to AWS Services
Amplify Hosting: Host apps on AWS content delivery network
////////////////////

AWS Amplify
////////////////////
Features like storage, auth, machine-learning (AWS Services)
Front-ends: React, Vue, ...
Build and deploy w/ Studio or CLI
////////////////////

AWS Amplify - Important Features
////////////////////
Auth:
	Leverages Cognito
	Has User registration, account recovery, ...
	Pre-built UI components

Datastore:
	APIs to DynamoDB, AppSync
	Powered by GraphQL
	Offline and real-time
	Visual data modeling w/ Amplify Studio
////////////////////

AWS Amplify Hosting
////////////////////
Hosting w/ lots of features (see slide)
Overview: Online hosting for a CICD app
Deploys front-end to CloudFront, and back-end to Amplify
////////////////////

AWS Amplify - End-to-End (E2E) Testing
////////////////////
Use a amplify.yml file to test for regression errors in app at testing phase of CICD Pipeline
Diagram:
	Build -----> Test (E2E) -----> Deploy
////////////////////
//////////////////////////////////////////////////////////////////////////


AWS Amplify - Hands On
//////////////////////////////////////////////////////////////////////////
Docs show how to get started quickly, and gives code on GitHub to copy (To-Do app)
Create new repo and paste the code
Amplify Console -> Create new app
App URL is now working, and we can check it out
Amplify > *app name* > main > Deployments
	Data Manager (DynamoDB back-end)
	Create auth w/ Cognito
	Create Fns
	Create UI Library
//////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
Ch 29
  __         __    ___
 /  \ |  |  /  \  |   |   
 |    |__|     /  |___|
 |    |  |    /       |
 \__/ |  |.  /__      |

Advanced Identity
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////

STS Overview
//////////////////////////////////////////////////////////////////////////
AWS STS - Security Token Service
////////////////////
Gives temp auth access (1 hour max) w/ a Security Token

Most Important:
////
AssumeRole: Assume IAM Role
GetSessionToken: MFA from User or Account
GetCallerIdentity: Returns details about IAM User or Role from API call
DecodeAuthorizationMessage: Decode error message when AWS API is denied
////

Others:
////
AssumeRoleWithSAML: Get credentials for SAML Users
AssumeRoleWithWebIdentity:
	old: gets credentials for 3rd party Service (Google, Facebook, ...)
	new: Cognito Identity Pools
GetFederationToken: Get temp creds for federated User
////
////////////////////

Using STS to Assume a Role
////////////////////
Steps to Assume a Role:
	Define IAM Role
	Define accessible Services for Role
	User goes to STS to get credentials of IAM Role
	Temp credentials last from 15min to 1 hour
////////////////////

Cross-Account Access with STS
////////////////////
Diagram shows steps of assuming Role
1. Admin creates Role
2. Admin gives Role to group
3. User requests Role
4. STS gives temp creds
5. User uses temp Role
////////////////////

STS with MFA
////////////////////
In the case of an IAM Policy that requires MFA, we can use GetSessionToken

Use GetSessionToken from STS
Requires IAM Policy w/ variable set:
	aws:MultiFactorAuthPresent:true
The GetSessionToken returns:
	Access ID, Secret Key, Secret Token, Expiration Date
////////////////////
//////////////////////////////////////////////////////////////////////////


Advanced IAM
//////////////////////////////////////////////////////////////////////////
Advanced IAM - Authorization Model - Evaluation of Policies, simplified
////////////////////
Slide has diagram of the evaluation process to check if a User can access a Service

Starts w/ default DENY
	Checks Policies
		If explicit deny -> DENY
		else evaluates if it should allow or deny
////////////////////

IAM Policies & S3 Bucket Policies
////////////////////
IAM - Attached to Users, Roles, Groups
S3 - Attached to S3 Buckets

Evaluation considers the UNION of these two Policies
////////////////////

Ex 1
////////////////////
IAM Role w/ EC2 Instance, Policy allows RW to S3
S3 w/ no Policy

Can the EC2 Instance Read/Write to Bucket?
	YES, b/c the UNION allows for it
////////////////////

Ex 2
////////////////////
IAM Role w/ EC2 Instance, Policy allows RW to S3
S3 w/ explicit DENY to IAM Role

Can the EC2 Instance Read/Write to Bucket?
	NO, b/c the explicit DENY is evaluated first
////////////////////

Ex 3
////////////////////
IAM Role w/ EC2 Instance, no Policy for S3
S3 w/ Policy allows RW to IAM Role

Can the EC2 Instance Read/Write to Bucket?
	YES, b/c the UNION allows for it
////////////////////

Ex 4
////////////////////
IAM Role w/ EC2 Instance, Policy has explicit DENY
S3 w/ explicit RW Allow to IAM Role

Can the EC2 Instance Read/Write to Bucket?
	NO, b/c the explicit DENY is evaluated first
////////////////////

Dynamic Policies with IAM
////////////////////
Assign each User their own folder structure in an S3 Bucket
Options:
	Create IAM Policy allowing *user name* to access /home/*user name* in Bucket
		Requires IAM Policy per User! Not ideal!
	Dynamic Policy, allows using a variable
		${aws:username}
////////////////////

Dynamic Policy Example
////////////////////
The Policy code will have the Resource specifying the username variable"
	"Resource": ["*arn prefix*.../home/${aws:username}/*"]
////////////////////

Inline vs Managed Policies
////////////////////
Managed:
	Default AWS Policies for admins, devs
Customer Managed:
	Custom Policies for our custom Services
Inline:
	Just for one-to-one b/t Policy and Principle
	Is deleted if the IAM Principle is deleted
////////////////////

Hands On
////////////////////
In the IAM Console, we can:
	Search for the default AWS Managed Policies
	Search for custom Policies, and what uses it
	Quicly create an Inline Policy, but not be able to duplicate it beyond its assigned Principle
////////////////////
//////////////////////////////////////////////////////////////////////////


Granting a User Permissions to Pass a Role to an AWS Service
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////


AWS Directory Services
//////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////













































